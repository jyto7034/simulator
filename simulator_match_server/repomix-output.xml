This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/main.yml
.gitignore
Cargo.toml
config/development.toml
config/production.toml
review.md
src/auth.rs
src/env.rs
src/lib.rs
src/main.rs
src/matchmaker/actor.rs
src/matchmaker/handlers.rs
src/matchmaker/lock.rs
src/matchmaker/messages.rs
src/matchmaker/mod.rs
src/matchmaker/scripts.rs
src/protocol.rs
src/provider.rs
src/pubsub.rs
src/util/mod.rs
src/ws_session.rs
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/main.yml">
name: Code Coverage

on:
  pull_request:
    branches: [ main, master ]

jobs:
  codecov:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
          components: llvm-tools-preview
      
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      
      - name: Generate code coverage
        run: cargo llvm-cov --all-features --workspace --lcov --output-path lcov.info
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: lcov.info
</file>

<file path=".gitignore">
/target
</file>

<file path="Cargo.toml">
[package]
name = "match_server"
version = "0.1.0"
edition = "2021"
description = "card backend"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html



[dependencies]
backoff = "0.4"
tokio = { version = "1.15", features = ["full", "tracing"] }
tracing = "0.1.41"
tracing-appender = "0.2.3"
tracing-subscriber = { version = "0.3.19", features = ["env-filter"] }
actix-web = "4.9.0"
actix = "=0.13.5"
actix-ws = "0.2.5"
actix-web-actors = "4.3.0"
async-tungstenite = {version="0.28.2", features = ["tokio-runtime"]}
reqwest = { version = "0.12.12", features = ["json"] }
url = "2.5.4"
uuid = { version = "1.14.0", features = ["v4", "serde"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
rand_core = "0.6"
rand = "0.8.5"
futures-util = {version = "0.3", features = ["sink"]}
futures = "0.3.31"
ctor = "0.4.1"
redis = { version = "0.22.3", features = ["tokio-comp", "connection-manager"] }
thiserror-core = "1.0.50"
dotenv = "0.15.0"
thiserror = "2.0.12"
config = {version = "0.15.11", features = ["toml"]}
jsonwebtoken = "9.3.1"
anyhow = "1.0"
actix-web-prom = "0.10.0"
simulator_metrics = { path = "../simulator_metrics" }
chrono = { version = "0.4.41", features = ["serde"] }
hex = "0.4.3"
</file>

<file path="config/development.toml">
# 서버 설정
[server]
bind_address = "0.0.0.0"
port = 8080
log_level = "INFO"

# Redis 설정
[redis]
url = "redis://127.0.0.1:6379"

[jwt]
secret = "your-super-secret-and-long-key-that-no-one-knows"

# 매치메이킹 설정
[matchmaking]
tick_interval_seconds = 5
queue_key_prefix = "queue"

# 게임 모드 목록 (테이블 배열)
# 각 [[matchmaking.game_modes]] 항목이 하나의 게임 모드를 정의합니다.
[[matchmaking.game_modes]]
id = "Normal_1v1"          # 게임 모드의 고유 ID
required_players = 2       # 매칭에 필요한 인원
use_mmr_matching = false   # MMR 기반 매칭 사용 여부 (일반 모드)

# [[matchmaking.game_modes]]
# id = "Ranked_1v1"
# required_players = 2
# use_mmr_matching = true    # 랭크 모드에서는 이 값을 true로 설정
</file>

<file path="config/production.toml">
redis_url = "redis://your-production-redis-endpoint:6379"
log_level = "WARN"
password = ""
</file>

<file path="review.md">
네, 정확합니다. 원래 의도하신 "쿠버네티스가 비정상 종료를 감지하고 복구하도록 한다"는 전략은 클라우드 네이티브 환경에서 매우 올바른 방향입니다.

문제는 **어떻게** 비정상 종료를 하느냐에 있습니다. `std::process::exit(1)`는 가장 간단하지만 가장 무책임한 방법입니다. 마치 퇴근 시간이 되었다고 컴퓨터 전원을 그냥 뽑아버리는 것과 같습니다.

사용자께서 제안하신 **"Terminate 메시지를 만들어 리소스 정리 후 비정상 종료"** 하는 방식이 바로 이 문제를 해결하는 **가장 정석적이고 우아한 해법**입니다. 이를 "Graceful Shutdown Triggering" 패턴이라고 부를 수 있습니다.

### 왜 `std::process::exit(1)`가 위험한가?

`RedisSubscriber`가 `std::process::exit(1)`을 호출하는 순간, 다른 모든 액터들은 자신의 상태를 정리할 기회를 완전히 잃어버립니다.

- **`Matchmaker` 액터:** 만약 `TryMatch` 핸들러가 실행 중이었다면, 플레이어들을 큐에서 꺼냈지만(`SPOP`) 아직 로딩 세션을 생성하지 않은 상태에서 서버가 꺼질 수 있습니다. 이 플레이어들은 허공에 뜨게 됩니다.
- **`DedicatedServerProvider` 액터:** 진행 중인 작업이 있었다면 중단됩니다.
- **수많은 `MatchmakingSession` 액터들:** `stopping` 메서드가 호출될 기회조차 없이 사라집니다. 이는 수많은 플레이어가 큐나 로딩 세션에서 제거되지 않는 결과를 낳습니다.
- **로그 및 메트릭 유실:** 버퍼에 남아있던 마지막 로그나 메트릭 데이터가 파일이나 네트워크로 전송되지 않고 유실될 수 있습니다.

결론적으로, 서버가 재시작되더라도 이전 상태의 "쓰레기 데이터(stale data)"가 Redis에 그대로 남아있어 시작부터 문제를 안고 가게 됩니다.

---

### Graceful Shutdown Triggering (권장 해결책)

이 패턴은 시스템의 최상위 관리자(이 경우 `main` 함수 또는 감독(Supervisor) 액터)에게 "이제 우리 모두 정중하게 문을 닫아야 할 시간입니다"라고 알리는 방식입니다.

**구현 단계:**

#### 1. `main` 함수에서 시스템 종료를 위한 채널(Channel) 설정

`main` 함수는 모든 액터의 "부모"와 같으므로, 종료 명령을 내리기에 가장 적합한 위치입니다. `tokio::sync::mpsc` 같은 비동기 채널을 사용하여 종료 신호를 전달합니다.

```rust
// in src/main.rs
use tokio::sync::mpsc;

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    // ... 기존 설정 ...

    // 1. 시스템 종료를 위한 MPSC(Multi-Producer, Single-Consumer) 채널 생성
    // tx는 여러 곳에서 복제해서 사용할 수 있고, rx는 한 곳(main)에서만 받습니다.
    let (shutdown_tx, mut shutdown_rx) = mpsc::channel::<()>(1);

    // ... RedisSubscriber 생성 시 shutdown_tx의 복제본을 전달 ...
    RedisSubscriber::new(
        redis_client.clone(),
        sub_manager_addr.clone(),
        10,
        1000,
        60000,
        shutdown_tx.clone(), // <-- 종료 채널의 송신기를 전달
    ).start();

    // ... HttpServer 설정 ...
    let server = HttpServer::new(move || {
        App::new()
            // ...
    })
    .bind(&bind_address)?
    .run();

    let server_handle = server.handle();

    // 2. 종료 신호 대기 및 처리 로직
    tokio::select! {
        // 서버가 정상적으로 완료된 경우
        res = server => {
            info!("Actix-Web server has shut down.");
            res
        },
        // 어딘가에서 종료 신호를 보낸 경우
        _ = shutdown_rx.recv() => {
            error!("Shutdown signal received. Initiating graceful shutdown of Actix-Web server...");
            // 서버를 정중하게 중지시킴 (기존 연결 처리가 끝날 때까지 기다림)
            server_handle.stop(true).await;

            info!("Server stopped. Exiting with error code to trigger K8s restart.");
            // 모든 정리가 끝난 후, 쿠버네티스가 감지하도록 비정상 종료
            std::process::exit(1);
        }
    }
}
```

#### 2. `RedisSubscriber`가 종료 신호 보내기

`RedisSubscriber`는 이제 직접 프로세스를 죽이는 대신, `main` 함수에게 "더 이상 가망이 없으니 종료 절차를 시작해 주세요"라고 요청합니다.

```rust
// in src/pubsub.rs
use tokio::sync::mpsc;

pub struct RedisSubscriber {
    // ... 기존 필드
    shutdown_tx: mpsc::Sender<()>, // <-- 종료 채널 송신기
}

impl RedisSubscriber {
    pub fn new(
        // ... 기존 인자
        shutdown_tx: mpsc::Sender<()>,
    ) -> Self {
        Self {
            // ...
            shutdown_tx,
        }
    }

    fn connect_and_subscribe(&mut self, ctx: &mut Context<Self>) {
        // ...
        let shutdown_tx = self.shutdown_tx.clone(); // 복제해서 async 블록으로 이동

        async move {
            if current_reconnect_attempts >= max_reconnect_attempts {
                error!("Max Redis reconnect attempts reached. Sending shutdown signal.");
                // 직접 종료하는 대신, main 함수에 종료 요청 메시지를 보냄
                if shutdown_tx.send(()).await.is_err() {
                    error!("Failed to send shutdown signal. Forcing exit.");
                    std::process::exit(1); // 만약 채널마저 닫혔다면 최후의 수단 사용
                }
                return;
            }
            // ...
        }.into_actor(self).wait(ctx);
    }
}
```

#### 3. (선택적) 다른 액터들의 Graceful Shutdown

`actix-web`의 `server_handle.stop(true)`는 새로운 연결을 받지 않고 기존 HTTP 요청이 완료될 때까지 기다려주는 역할을 합니다. 하지만 이와 별개로 백그라운드에서 동작하는 `Matchmaker` 같은 액터들은 별도의 정리 로직이 필요할 수 있습니다. `main` 함수에서 `server_handle.stop()` 호출 전, 이들 액터에게도 `System::current().stop()` 등을 통해 종료 신호를 보내고 잠시 기다려주는 로직을 추가할 수 있습니다.

### 이 방식의 장점

1.  **질서 있는 종료:** `main` 함수가 중앙 통제소 역할을 하여 모든 시스템이 질서정연하게 종료될 기회를 갖습니다. `actix-web` 서버는 처리 중인 요청을 마저 처리하고, 다른 액터들도 `stopping` 라이프사이클 훅을 통해 자신의 상태를 정리할 수 있습니다.
2.  **데이터 정합성 유지:** "유령 플레이어"나 "허공에 뜬 매치" 같은 쓰레기 데이터가 남을 가능성을 최소화합니다.
3.  **의도 명확화:** "쿠버네티스에 의한 복구"라는 원래의 의도는 그대로 유지됩니다. 모든 정리 작업이 끝난 후 마지막에 `std::process::exit(1)`을 호출하여 쿠버네티스에게 "의도된 비정상 종료"임을 명확히 알립니다.
4.  **중앙화된 제어:** 종료 로직이 `main` 함수 한 곳에 모여있어 관리하기 쉽고, 나중에 다른 종료 조건(예: 특정 관리자 API 호출)을 추가하기도 용이합니다.

결론적으로, `std::process::exit(1)`을 직접 호출하는 대신 **메시지 패싱(Message Passing)**을 통해 중앙 관리자에게 종료를 위임하는 방식은 시스템의 안정성과 예측 가능성을 극적으로 향상시키는 매우 성숙한 엔지니어링 패턴입니다.
</file>

<file path="src/auth.rs">
use actix_web::{dev::Payload, web, FromRequest, HttpRequest};
use jsonwebtoken::{decode, DecodingKey, Validation};
use serde::{Deserialize, Serialize};
use std::future::{ready, Ready};

use crate::AppState;

// JWT Claims 구조체 (auth_server의 것과 동일해야 함)
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Claims {
    pub sub: String, // Subject (user's steam_id)
    pub iat: usize,
    pub exp: usize,
}

// 핸들러에서 인증된 사용자 정보를 담을 구조체
#[derive(Debug)]
pub struct AuthenticatedUser {
    pub steam_id: i64,
}

// Actix-Web 추출기(Extractor) 구현
impl FromRequest for AuthenticatedUser {
    type Error = actix_web::Error;
    type Future = Ready<Result<Self, Self::Error>>;

    fn from_request(req: &HttpRequest, _: &mut Payload) -> Self::Future {
        let app_state = req.app_data::<web::Data<AppState>>().expect(
            "AppState not configured in Actix-Web application. This is a critical server configuration error.",
        );

        // 1. Authorization 헤더에서 토큰 추출
        let auth_header = match req.headers().get("Authorization") {
            Some(header) => match header.to_str() {
                Ok(s) => s,
                Err(_) => {
                    return ready(Err(actix_web::error::ErrorUnauthorized(
                        "Invalid Authorization header encoding",
                    )));
                }
            },
            None => {
                return ready(Err(actix_web::error::ErrorUnauthorized(
                    "Missing Authorization header",
                )));
            }
        };

        if !auth_header.starts_with("Bearer ") {
            return ready(Err(actix_web::error::ErrorUnauthorized(
                "Invalid token format",
            )));
        }

        let token = &auth_header["Bearer ".len()..];

        // 2. JWT 디코딩 및 검증
        let token_data = match decode::<Claims>(
            token,
            &DecodingKey::from_secret(app_state.jwt_secret.as_ref()),
            &Validation::default(),
        ) {
            Ok(data) => data,
            Err(e) => {
                // 로그에 에러 기록
                tracing::warn!("JWT validation failed: {}", e);
                return ready(Err(actix_web::error::ErrorUnauthorized("Invalid token")));
            }
        };

        // 3. Claims에서 steam_id 파싱
        let steam_id = match token_data.claims.sub.parse::<i64>() {
            Ok(id) => id,
            Err(_) => {
                return ready(Err(actix_web::error::ErrorBadRequest(
                    "Invalid steam_id in token",
                )));
            }
        };

        // 4. 성공 시 AuthenticatedUser 반환
        ready(Ok(AuthenticatedUser { steam_id }))
    }
}
</file>

<file path="src/env.rs">
use config::{Config, ConfigError, File};
use serde::Deserialize;

#[derive(Debug, Deserialize, Clone)]
pub struct ServerSettings {
    pub bind_address: String,
    pub port: u16,
    pub log_level: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct RedisSettings {
    pub url: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct JwtSettings {
    pub secret: String,
}

/// TOML 설정 파일의 [[matchmaking.game_modes]] 테이블에 대응하는 구조체입니다.
#[derive(Debug, Deserialize, Clone)]
pub struct GameModeSettings {
    pub id: String,
    pub required_players: u32,
    pub use_mmr_matching: bool,
}

#[derive(Debug, Deserialize, Clone)]
pub struct MatchmakingSettings {
    pub tick_interval_seconds: u64,
    pub queue_key_prefix: String,
    pub game_modes: Vec<GameModeSettings>,
    pub heartbeat_interval_seconds: u64,
    pub client_timeout_seconds: u64,
    pub loading_session_timeout_seconds: u64,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Settings {
    pub server: ServerSettings,
    pub redis: RedisSettings,
    pub jwt: JwtSettings,
    pub matchmaking: MatchmakingSettings,
}

impl Settings {
    pub fn new() -> Result<Self, ConfigError> {
        let run_mode = std::env::var("RUN_MODE").unwrap_or_else(|_| "development".into());

        let s = Config::builder()
            .add_source(File::with_name(&format!("config/{}", run_mode)).required(true))
            .build()?;

        s.try_deserialize()
    }
}
</file>

<file path="src/lib.rs">
use crate::{matchmaker::Matchmaker, pubsub::SubscriptionManager};
use actix::Addr;
use redis::aio::ConnectionManager;

pub mod auth;
pub mod env;
pub mod matchmaker;
pub mod protocol;
pub mod provider;
pub mod pubsub;
pub mod util;
pub mod ws_session;

// 서버 전체에서 공유될 상태
#[derive(Clone)]
pub struct AppState {
    pub jwt_secret: String,
    // redis_client는 이제 pubsub 액터에서만 사용되므로 AppState에서 제거
    // pub redis_client: RedisClient,
    pub matchmaker_addr: Addr<Matchmaker>,
    // provider_addr는 Matchmaker가 내부적으로 소유하므로 AppState에서 제거
    // pub provider_addr: Addr<DedicatedServerProvider>,
    pub sub_manager_addr: Addr<SubscriptionManager>,
    pub matchmaking_settings: crate::env::MatchmakingSettings,
    pub redis_conn_manager: ConnectionManager,
}

use std::{io, sync::Once};
use tracing_appender::rolling::{RollingFileAppender, Rotation};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt, EnvFilter};
static INIT: Once = Once::new();
static mut GUARD: Option<tracing_appender::non_blocking::WorkerGuard> = None;
pub fn setup_logger() {
    INIT.call_once(|| {
        // 1. 파일 로거 설정
        let file_appender = RollingFileAppender::new(Rotation::DAILY, "logs", "app.log");
        let (non_blocking_file_writer, _guard) = tracing_appender::non_blocking(file_appender);

        // 2. 로그 레벨 필터 설정 (환경 변수 또는 기본값 INFO)
        let filter = EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("info")); // 기본 INFO 레벨

        // 3. 콘솔 출력 레이어 설정
        let console_layer = fmt::layer()
            .with_writer(io::stdout) // 표준 출력으로 설정
            .with_ansi(true) // ANSI 색상 코드 사용 (터미널 지원 시)
            .with_thread_ids(true) // 스레드 ID 포함
            .with_thread_names(true) // 스레드 이름 포함
            .with_file(true) // 파일 경로 포함
            .with_line_number(true) // 라인 번호 포함
            .with_target(false) // target 정보 제외 (선택 사항)
            .pretty(); // 사람이 읽기 좋은 포맷

        // 4. 파일 출력 레이어 설정
        let file_layer = fmt::layer()
            .with_writer(non_blocking_file_writer) // Non-blocking 파일 로거 사용
            .with_ansi(false) // 파일에는 ANSI 코드 제외
            .with_thread_ids(true)
            .with_thread_names(true)
            .with_file(true)
            .with_line_number(true)
            .with_target(false)
            .pretty();

        // 5. 레지스트리(Registry)에 필터와 레이어 결합
        tracing_subscriber::registry()
            .with(filter) // 필터를 먼저 적용
            .with(console_layer) // 콘솔 레이어 추가
            .with(file_layer) // 파일 레이어 추가
            .init(); // 전역 Subscriber로 설정

        unsafe {
            GUARD = Some(_guard);
        }

        tracing::info!("로거 초기화 완료: 콘솔 및 파일(logs/app.log) 출력 활성화.");
    });
}
</file>

<file path="src/main.rs">
use actix::Actor;
use actix_web::{get, web, App, Error, HttpRequest, HttpResponse, HttpServer};
use actix_web_actors::ws;
use actix_web_prom::PrometheusMetricsBuilder;
use match_server::{
    env::Settings,
    matchmaker::Matchmaker,
    provider::DedicatedServerProvider,
    pubsub::{RedisSubscriber, SubscriptionManager},
    setup_logger,
    ws_session::MatchmakingSession,
    AppState,
};
use simulator_metrics::register_custom_metrics;
use std::time::Duration;
use tokio::sync::mpsc;
use tracing::{error, info};

#[get("/ws/")]
async fn matchmaking_ws_route(
    req: HttpRequest,
    stream: web::Payload,
    state: web::Data<AppState>,
) -> Result<HttpResponse, Error> {
    let session = MatchmakingSession::new(
        state.matchmaker_addr.clone(),
        state.sub_manager_addr.clone(),
        Duration::from_secs(state.matchmaking_settings.heartbeat_interval_seconds),
        Duration::from_secs(state.matchmaking_settings.client_timeout_seconds),
    );
    ws::start(session, &req, stream)
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    dotenv::dotenv().ok();
    let settings = Settings::new().expect("Failed to load settings.");
    setup_logger();

    let prometheus = PrometheusMetricsBuilder::new("match_server")
        .endpoint("/metrics")
        .build()
        .expect("Failed to build Prometheus metrics.");

    register_custom_metrics(&prometheus.registry).expect("Failed to register custom metrics");

    let redis_client =
        redis::Client::open(settings.redis.url.clone()).expect("Failed to create Redis client");
    let redis_conn_manager = redis::aio::ConnectionManager::new(redis_client.clone())
        .await
        .expect("Failed to create Redis connection manager");
    info!("Redis connection manager created.");

    // --- Graceful Shutdown Channel ---
    let (shutdown_tx, mut shutdown_rx) = mpsc::channel::<()>(1);

    // --- Start New Pub/Sub Actors ---
    let sub_manager_addr = SubscriptionManager::new().start();
    info!("SubscriptionManager actor started.");

    RedisSubscriber::new(
        redis_client.clone(),
        sub_manager_addr.clone(),
        10,    // max_reconnect_attempts
        1000,  // initial_reconnect_delay_ms (1 second)
        60000, // max_reconnect_delay_ms (60 seconds)
        shutdown_tx.clone(),
    )
    .start();
    info!("RedisSubscriber actor started.");
    // --- End of Start New Actors ---

    let provider_addr = DedicatedServerProvider::new(redis_conn_manager.clone()).start();
    info!("DedicatedServerProvider actor started.");

    let matchmaker_addr = Matchmaker::new(
        redis_conn_manager.clone(),
        settings.matchmaking.clone(),
        provider_addr.clone(),
    )
    .start();
    info!("Matchmaker actor started.");

    let app_state = AppState {
        jwt_secret: settings.jwt.secret.clone(),
        matchmaker_addr: matchmaker_addr.clone(),
        sub_manager_addr,
        matchmaking_settings: settings.matchmaking.clone(),
        redis_conn_manager: redis_conn_manager.clone(),
    };

    let bind_address = format!("{}:{}", settings.server.bind_address, settings.server.port);
    info!("Starting Actix-Web server on {}", bind_address);

    let server = HttpServer::new(move || {
        App::new()
            .wrap(prometheus.clone())
            .app_data(web::Data::new(app_state.clone()))
            .service(matchmaking_ws_route)
    })
    .bind(&bind_address)?
    .run();

    let server_handle = server.handle();

    tokio::select! {
        res = server => {
            info!("Actix-Web server has shut down.");
            return res;
        },
        _ = shutdown_rx.recv() => {
            error!("Critical error signal received. Initiating graceful shutdown...");
            server_handle.stop(true).await;
            info!("Server stopped. Exiting with error code to trigger K8s restart.");
            std::process::exit(1);
        },
        _ = tokio::signal::ctrl_c() => {
            info!("Ctrl+C received. Initiating graceful shutdown...");
            server_handle.stop(true).await;
            info!("Server stopped gracefully.");
        }
    }

    Ok(())
}
</file>

<file path="src/matchmaker/actor.rs">
use crate::provider::DedicatedServerProvider;
use actix::{Actor, Addr, AsyncContext, Context};
use redis::aio::ConnectionManager;
use std::time::Duration;
use tracing::info;

use super::messages::{CheckStaleLoadingSessions, TryMatch};

pub struct Matchmaker {
    pub(super) redis: ConnectionManager,
    pub(super) http_client: reqwest::Client,
    pub(super) settings: crate::env::MatchmakingSettings,
    pub(super) provider_addr: Addr<DedicatedServerProvider>,
}

impl Matchmaker {
    pub fn new(
        redis: ConnectionManager,
        settings: crate::env::MatchmakingSettings,
        provider_addr: Addr<DedicatedServerProvider>,
    ) -> Self {
        Self {
            redis,
            http_client: reqwest::Client::new(),
            settings,
            provider_addr,
        }
    }
}

impl Actor for Matchmaker {
    type Context = Context<Self>;
    fn started(&mut self, ctx: &mut Self::Context) {
        info!("Matchmaker actor started.");
        // 매칭 시도 타이머
        ctx.run_interval(
            Duration::from_secs(self.settings.tick_interval_seconds),
            |act, ctx| {
                for mode_settings in act.settings.game_modes.clone() {
                    ctx.address().do_send(TryMatch {
                        game_mode: mode_settings,
                    });
                }
            },
        );
        // 오래된 로딩 세션 정리 타이머
        ctx.run_interval(
            Duration::from_secs(self.settings.loading_session_timeout_seconds),
            |_act, ctx| {
                ctx.address().do_send(CheckStaleLoadingSessions);
            },
        );
    }
}
</file>

<file path="src/matchmaker/handlers.rs">
use crate::{protocol::ServerMessage, provider::FindAvailableServer};
use actix::{AsyncContext, Handler, ResponseFuture};
use futures_util::stream::StreamExt;
use redis::aio::ConnectionManager;
use redis::{AsyncCommands, Script};
use serde::{Deserialize, Serialize};
use simulator_metrics::{MATCHES_CREATED_TOTAL, PLAYERS_IN_QUEUE, HTTP_TIMEOUT_ERRORS_TOTAL, MATCHMAKING_ERRORS_TOTAL};
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use tracing::{error, info, warn};
use uuid::Uuid;

use super::{
    actor::Matchmaker,
    lock::DistributedLock, // DistributedLock 임포트
    messages::*,
    scripts::{
        ATOMIC_CANCEL_SESSION_SCRIPT, ATOMIC_LOADING_COMPLETE_SCRIPT, ATOMIC_MATCH_SCRIPT,
        CLEANUP_STALE_SESSION_SCRIPT,
    },
};

const LOCK_DURATION_MS: usize = 30_000; // 30초

// --- Helper Functions ---
async fn publish_message(redis: &mut ConnectionManager, player_id: Uuid, message: ServerMessage) {
    let channel = format!("notifications:{}", player_id);
    let payload = match serde_json::to_string(&message) {
        Ok(p) => p,
        Err(e) => {
            warn!(
                "Failed to serialize ServerMessage for player {}: {}",
                player_id, e
            );
            return;
        }
    };
    if let Err(e) = redis.publish::<_, _, ()>(&channel, &payload).await {
        warn!("Failed to publish message to channel {}: {}", channel, e);
    }
}

async fn requeue_players(redis: &mut ConnectionManager, queue_key: &str, player_ids: &[String]) {
    warn!("Re-queuing players due to an error: {:?}", player_ids);
    if player_ids.is_empty() {
        return;
    }
    PLAYERS_IN_QUEUE.add(player_ids.len() as i64);
    let result: Result<i32, _> = redis.sadd(queue_key, player_ids).await;
    if let Err(e) = result {
        error!(
            "CRITICAL: Failed to re-queue players {:?} into {}: {}",
            player_ids, queue_key, e
        );
    }
}

// --- Message Handlers ---

/// EnqueuePlayer: 플레이어를 큐에 추가하는 메시지
impl Handler<EnqueuePlayer> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: EnqueuePlayer, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        // 게임 모드 유효성 검사
        let is_valid_game_mode = self
            .settings
            .game_modes
            .iter()
            .any(|m| m.id == msg.game_mode);
        if !is_valid_game_mode {
            let player_id = msg.player_id;
            return Box::pin(async move {
                warn!(
                    "Player {} tried to enqueue for invalid game mode: {}",
                    player_id, msg.game_mode
                );
                publish_message(
                    &mut redis,
                    player_id,
                    ServerMessage::Error {
                        message: format!("Invalid game mode: {}", msg.game_mode),
                    },
                )
                .await;
            });
        }

        Box::pin(async move {
            let player_id_str = msg.player_id.to_string();
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);

            // Redis SADD는 원자적이므로 락 불필요
            let result: Result<i32, _> = redis.sadd(&queue_key, &player_id_str).await;
            match result {
                Ok(count) if count > 0 => {
                    info!("Player {} added to queue {}", player_id_str, queue_key);
                    PLAYERS_IN_QUEUE.inc();
                    publish_message(&mut redis, msg.player_id, ServerMessage::Queued).await;
                }
                Ok(_) => {
                    warn!("Player {} already in queue {}", player_id_str, queue_key);
                    publish_message(
                        &mut redis,
                        msg.player_id,
                        ServerMessage::Error {
                            message: "Already in queue".to_string(),
                        },
                    )
                    .await;
                }
                Err(e) => {
                    error!("Failed to add player to queue: {}", e);
                    publish_message(
                        &mut redis,
                        msg.player_id,
                        ServerMessage::Error {
                            message: "Internal server error".to_string(),
                        },
                    )
                    .await;
                }
            }
        })
    }
}

impl Handler<DequeuePlayer> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: DequeuePlayer, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();
        Box::pin(async move {
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);
            let player_id_str = msg.player_id.to_string();

            // Redis SREM은 원자적이므로 락 불필요
            let result: Result<i32, _> = redis.srem(&queue_key, &player_id_str).await;
            match result {
                Ok(count) if count > 0 => {
                    info!(
                        "Player {} (disconnected) removed from queue {}",
                        player_id_str, queue_key
                    );
                    PLAYERS_IN_QUEUE.dec();
                }
                Ok(_) => {
                    tracing::debug!(
                        "Player {} was not in queue {}, likely already matched.",
                        player_id_str,
                        queue_key
                    );
                }
                Err(e) => {
                    error!(
                        "Failed to remove player {} from queue {}: {}",
                        player_id_str, queue_key, e
                    );
                }
            }
        })
    }
}

impl Handler<TryMatch> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: TryMatch, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let game_mode_settings = msg.game_mode.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();
        let loading_session_timeout_seconds = self.settings.loading_session_timeout_seconds;

        Box::pin(async move {
            let queue_key = format!("{}:{}", queue_key_prefix, game_mode_settings.id);
            // 게임에 필요한 플레이어 수 입니다.
            let required_players = game_mode_settings.required_players;
            let lock_key = format!("lock:match:{}", game_mode_settings.id);

            if game_mode_settings.use_mmr_matching {
                warn!(
                    "MMR-based matching for '{}' is not yet implemented. Falling back to simple matching.",
                    game_mode_settings.id
                );
            }

            // --- 분산락 획득 ---
            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => {
                    // 다른 서버가 이미 매칭 처리 중이므로 건너뛰기
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to acquire lock for matching in {}: {}",
                        game_mode_settings.id, e
                    );
                    return;
                }
            };

            // 필요한 플레이어 수 만큼 redis 에서 player id 를 가져옵니다.
            let loading_session_id = Uuid::new_v4();
            let current_timestamp = SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .expect("System time is set before UNIX EPOCH. This is a critical system error.")
                .as_secs()
                .to_string();

            let script = Script::new(ATOMIC_MATCH_SCRIPT);
            let script_result: Vec<String> = match script
                .key(&queue_key)
                .arg(required_players)
                .arg(loading_session_id.to_string())
                .arg(current_timestamp)
                .arg(loading_session_timeout_seconds)
                .invoke_async(&mut redis)
                .await
            {
                Ok(p) => p,
                Err(e) => {
                    error!("Matchmaking script failed for queue {}: {}", queue_key, e);
                    // 락 해제
                    if let Err(lock_err) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for matching in {}: {}",
                            game_mode_settings.id, lock_err
                        );
                    }
                    return;
                }
            };

            if script_result.len() as u32 == (required_players + 2) {
                // game_mode, loading_session_id, player_ids
                let game_mode = script_result[0].clone();
                let returned_loading_session_id = Uuid::parse_str(&script_result[1]).unwrap();
                let player_ids: Vec<String> = script_result[2..].to_vec();

                PLAYERS_IN_QUEUE.sub(required_players as i64);
                info!(
                    "[{}] Found a potential match with players: {:?} for session {}",
                    game_mode, player_ids, returned_loading_session_id
                );

                info!(
                    "[{}] Notifying players to start loading for session {}",
                    game_mode, returned_loading_session_id
                );
                let message = ServerMessage::StartLoading {
                    loading_session_id: returned_loading_session_id,
                };
                for player_id_str in &player_ids {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        publish_message(&mut redis, player_id, message.clone()).await;
                    }
                }
            } else {
                // 스크립트가 예상된 결과를 반환하지 않았을 경우 (예: 플레이어 부족)
                info!(
                    "[{}] Not enough players in queue {} for game mode {}. Current: {}. Required: {}",
                    game_mode_settings.id, queue_key, game_mode_settings.id,
                    // Note: SCARD is not directly available here, but the script handles it.
                    // We can log the actual number of players returned by the script if needed.
                    script_result.len() as u32 - 2, // Assuming game_mode and loading_session_id are always returned if any players are.
                    required_players
                );
                // 이 경우 플레이어는 큐에 남아있으므로 별도의 requeue_players 호출은 필요 없음.
            }

            // --- 분산락 해제 ---
            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for matching in {}: {}",
                    game_mode_settings.id, e
                );
            }
        })
    }
}

impl Handler<HandleLoadingComplete> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: HandleLoadingComplete, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let http_client = self.http_client.clone();
        let provider_addr = self.provider_addr.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let loading_key = format!("loading:{}", msg.loading_session_id);
            let player_id_str = msg.player_id.to_string();
            let lock_key = format!("lock:{}", loading_key);

            // --- 분산락 획득 ---
            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => {
                    info!(
                        "Could not acquire lock for session {}, another process is handling it.",
                        msg.loading_session_id
                    );
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to acquire lock for session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            let script = Script::new(ATOMIC_LOADING_COMPLETE_SCRIPT);
            let result: Result<Vec<String>, _> = script
                .key(&loading_key)
                .arg(&player_id_str)
                .invoke_async(&mut redis)
                .await;

            let mut script_result: Vec<String> = match result {
                Ok(ids) if !ids.is_empty() => ids,
                Ok(_) => {
                    info!(
                        "Player {} is ready, but waiting for others in session {}.",
                        player_id_str, msg.loading_session_id
                    );
                    // 아직 모든 플레이어가 준비되지 않았으므로 락을 해제하고 종료
                    if let Err(e) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for session {}: {}",
                            msg.loading_session_id, e
                        );
                    }
                    return;
                }
                Err(e) => {
                    error!(
                        "Atomic loading script failed for session {}: {}",
                        msg.loading_session_id, e
                    );
                    // 스크립트 실패 시에도 락 해제
                    if let Err(e) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for session {}: {}",
                            msg.loading_session_id, e
                        );
                    }
                    return;
                }
            };

            // 모든 플레이어가 준비되었으므로, 여기서부터는 락 안에서 모든 작업을 처리합니다.
            let game_mode = script_result.remove(0);
            let player_ids = script_result;

            info!(
                "All players {:?} are ready for session {}. Finding a dedicated server...",
                player_ids, msg.loading_session_id
            );

            let find_server_result = provider_addr.send(FindAvailableServer).await;

            match find_server_result {
                Ok(Ok(server_info)) => {
                    let create_session_url =
                        format!("http://{}/session/create", server_info.address);
                    #[derive(Serialize)]
                    struct CreateSessionReq {
                        players: Vec<Uuid>,
                    }
                    let req_body = CreateSessionReq {
                        players: player_ids
                            .iter()
                            .filter_map(|id| Uuid::parse_str(id).ok())
                            .collect(),
                    };

                    match http_client
                        .post(&create_session_url)
                        .json(&req_body)
                        .timeout(Duration::from_secs(5)) // 5초 타임아웃 추가
                        .send()
                        .await
                    {
                        Ok(resp) if resp.status().is_success() => {
                            #[derive(Deserialize, Debug)]
                            struct CreateSessionResp {
                                server_address: String,
                                session_id: Uuid,
                            }

                            match resp.json::<CreateSessionResp>().await {
                                Ok(session_info) => {
                                    info!(
                                        "[{}] Successfully created session: {:?}",
                                        game_mode, session_info
                                    );
                                    MATCHES_CREATED_TOTAL.inc();
                                    let message = ServerMessage::MatchFound {
                                        session_id: session_info.session_id,
                                        server_address: session_info.server_address.clone(),
                                    };
                                    for player_id_str in &player_ids {
                                        if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                                            publish_message(&mut redis, player_id, message.clone())
                                                .await;
                                        }
                                    }
                                    // 성공적으로 매치를 생성했으므로 loading 키는 삭제해도 안전합니다.
                                    // ATOMIC_LOADING_COMPLETE_SCRIPT 에서 이미 삭제되었으므로 별도 처리 필요 없음.
                                }
                                Err(e) => {
                                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                                    MATCHMAKING_ERRORS_TOTAL.with_label_values(&["session_response_parse_failed"]).inc();
                                    error!("[{}] Failed to parse session creation response: {}. Re-queuing players.", game_mode, e);
                                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                                }
                            }
                        }
                        Ok(resp) => {
                            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                            MATCHMAKING_ERRORS_TOTAL.with_label_values(&["dedicated_server_error_response"]).inc();
                            error!(
                                "[{}] Dedicated server returned error: {}. Re-queuing players.",
                                game_mode,
                                resp.status()
                            );
                            requeue_players(&mut redis, &queue_key, &player_ids).await;
                        }
                        Err(e) => {
                            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                            
                            // HTTP 타임아웃 에러인지 확인하고 메트릭 기록
                            if e.is_timeout() {
                                HTTP_TIMEOUT_ERRORS_TOTAL.with_label_values(&["dedicated_server"]).inc();
                            }
                            MATCHMAKING_ERRORS_TOTAL.with_label_values(&["dedicated_server_request_failed"]).inc();
                            
                            error!(
                                "[{}] Failed to contact dedicated server (timeout or network error): {}. Re-queuing players.",
                                game_mode, e
                            );
                            requeue_players(&mut redis, &queue_key, &player_ids).await;
                        }
                    }
                }
                Ok(Err(e)) => {
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                    MATCHMAKING_ERRORS_TOTAL.with_label_values(&["server_provider_failed"]).inc();
                    error!(
                        "[{}] Failed to find available server: {}. Re-queuing players.",
                        game_mode, e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                }
                Err(e) => {
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                    MATCHMAKING_ERRORS_TOTAL.with_label_values(&["server_provider_mailbox_error"]).inc();
                    error!(
                        "[{}] Mailbox error when contacting provider: {}. Re-queuing players.",
                        game_mode, e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                }
            }

            // --- 모든 작업 완료 후 분산락 해제 ---
            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for session {}: {}",
                    msg.loading_session_id, e
                );
            }
        })
    }
}

impl Handler<CancelLoadingSession> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: CancelLoadingSession, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let loading_key = format!("loading:{}", msg.loading_session_id);
            let disconnected_player_id_str = msg.player_id.to_string();

            info!(
                "Attempting to cancel loading session {} due to player {} disconnection.",
                msg.loading_session_id, msg.player_id
            );

            let script = Script::new(ATOMIC_CANCEL_SESSION_SCRIPT);
            let result: Result<Vec<String>, _> = script
                .key(&loading_key)
                .arg(&disconnected_player_id_str)
                .invoke_async(&mut redis)
                .await;

            let mut script_result = match result {
                Ok(val) if !val.is_empty() => val,
                Ok(_) => {
                    // 스크립트가 빈 값을 반환하면, 세션이 이미 처리되었음을 의미합니다.
                    warn!(
                        "Loading session {} already handled or cleaned up before cancellation.",
                        msg.loading_session_id
                    );
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to run cancellation script for session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            // 스크립트는 [game_mode, player1_id, player2_id, ...] 형식으로 반환합니다.
            let game_mode = script_result.remove(0);
            let players_to_requeue = script_result;

            if !players_to_requeue.is_empty() {
                info!(
                    "Notifying remaining players {:?} and re-queuing them for game mode '{}'.",
                    players_to_requeue, game_mode
                );
                let queue_key = format!("{}:{}", queue_key_prefix, game_mode);

                let message = ServerMessage::Error {
                    message:
                        "A player disconnected during loading. You have been returned to the queue."
                            .to_string(),
                };

                for player_id_str in &players_to_requeue {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        publish_message(&mut redis, player_id, message.clone()).await;
                    }
                }
                requeue_players(&mut redis, &queue_key, &players_to_requeue).await;
            }
        })
    }
}
impl Handler<CheckStaleLoadingSessions> for Matchmaker {
    type Result = ResponseFuture<()>;

    fn handle(
        &mut self,
        _msg: CheckStaleLoadingSessions,
        _ctx: &mut Self::Context,
    ) -> Self::Result {
        let mut redis = self.redis.clone();
        let matchmaker_addr = _ctx.address();
        let loading_session_timeout_seconds = self.settings.loading_session_timeout_seconds;

        Box::pin(async move {
            info!("Checking for stale loading sessions...");

            // SCAN 사용으로 Redis 블로킹 방지
            let mut keys: Vec<String> = Vec::new();
            match redis.scan_match::<_, String>("loading:*").await {
                Ok(mut iter) => {
                    while let Some(key) = iter.next().await {
                        keys.push(key);
                    }
                }
                Err(e) => {
                    error!("Failed to scan loading sessions: {}", e);
                    return;
                }
            };

            for key in keys {
                let lock_key = format!("lock:{}", key);

                // --- 각 세션에 대한 분산락 획득 시도 ---
                let lock =
                    match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await {
                        Ok(Some(lock)) => lock,
                        _ => continue, // 락 획득 실패 시 (다른 프로세스가 처리 중이거나 에러), 다음 키로 넘어감
                    };

                let now = SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .expect(
                        "System time is set before UNIX EPOCH. This is a critical system error.",
                    )
                    .as_secs();

                let script = Script::new(CLEANUP_STALE_SESSION_SCRIPT);
                let result: Result<Vec<String>, _> = script
                    .key(&key)
                    .arg(now as i64)
                    .arg(loading_session_timeout_seconds as i64)
                    .invoke_async(&mut redis)
                    .await;

                let mut script_result = match result {
                    Ok(val) if !val.is_empty() => val,
                    Ok(_) => {
                        // 스크립트가 빈 값을 반환하면, 세션이 타임아웃되지 않았거나 이미 처리되었음을 의미합니다.
                        if let Err(e) = lock.release(&mut redis).await {
                            error!(
                                "Failed to release lock for stale check on key {}: {}",
                                key, e
                            );
                        }
                        continue;
                    }
                    Err(e) => {
                        error!(
                            "Failed to run stale session cleanup script for key {}: {}",
                            key, e
                        );
                        if let Err(e) = lock.release(&mut redis).await {
                            error!(
                                "Failed to release lock for stale check on key {}: {}",
                                key, e
                            );
                        }
                        continue;
                    }
                };

                // 스크립트는 [game_mode, player1_id, player2_id, ...] 형식으로 반환합니다.
                let game_mode = script_result.remove(0);
                let players_to_requeue = script_result;

                if !players_to_requeue.is_empty() {
                    warn!(
                        "Found stale loading session {}. Scheduling re-queuing for players {:?} for game mode '{}'.",
                        key, players_to_requeue, game_mode
                    );

                    let message = ServerMessage::Error {
                        message:
                            "Matchmaking timed out. You will be returned to the queue shortly."
                                .to_string(),
                    };
                    for player_id_str in &players_to_requeue {
                        if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                            publish_message(&mut redis, player_id, message.clone()).await;
                        }
                    }

                    // 지연된 재큐잉 메시지 전송
                    matchmaker_addr.do_send(DelayedRequeuePlayers {
                        player_ids: players_to_requeue,
                        game_mode: game_mode,
                        delay: Duration::from_secs(5), // 5초 지연
                    });
                }

                // --- 작업 완료 후 락 해제 ---
                if let Err(e) = lock.release(&mut redis).await {
                    error!(
                        "Failed to release lock for stale check on key {}: {}",
                        key, e
                    );
                }
            }
        })
    }
}

impl Handler<DelayedRequeuePlayers> for Matchmaker {
    type Result = ResponseFuture<()>;

    fn handle(&mut self, msg: DelayedRequeuePlayers, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            info!(
                "Re-queuing players {:?} for game mode {} after delay.",
                msg.player_ids, msg.game_mode
            );
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);
            requeue_players(&mut redis, &queue_key, &msg.player_ids).await;
        })
    }
}
</file>

<file path="src/matchmaker/lock.rs">
use redis::aio::ConnectionManager;
// `cmd`를 추가하고, `Commands`와 `Script`를 임포트합니다. `SetOptions`는 사용하지 않으므로 삭제합니다.
use redis::{cmd, RedisResult, Script};
use uuid::Uuid;

const RELEASE_SCRIPT: &str = r#"
    if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
    else
        return 0
    end
"#;

/// Redis를 이용한 분산락 구조체.
/// Drop 트레이트를 구현하지 않았으므로, 사용 후 반드시 `release`를 명시적으로 호출해야 합니다.
pub struct DistributedLock {
    key: String,
    value: String,
}

impl DistributedLock {
    /// 분산락을 획득합니다.
    ///
    /// # Arguments
    /// * `redis` - Redis 커넥션 매니저
    /// * `key` - 락을 걸 대상 키
    /// * `duration_ms` - 락의 만료 시간 (밀리초)
    ///
    /// # Returns
    /// * `Ok(Some(lock))` - 락 획득 성공
    /// * `Ok(None)` - 다른 프로세스가 락을 이미 소유하고 있음
    /// * `Err(e)` - Redis 오류 발생
    pub async fn acquire(
        redis: &mut ConnectionManager,
        key: &str,
        duration_ms: usize,
    ) -> RedisResult<Option<Self>> {
        let value = Uuid::new_v4().to_string();

        // `set_options` 대신 `cmd`를 사용하여 'SET key value NX PX ms' 명령을 직접 구성합니다.
        let result: Option<String> = cmd("SET")
            .arg(key)
            .arg(&value)
            .arg("NX") // Set only if the key does not already exist.
            .arg("PX") // Set the specified expire time, in milliseconds.
            .arg(duration_ms)
            .query_async(redis) // ConnectionManager에서 비동기적으로 실행
            .await?;

        // 'SET NX'는 성공 시 "OK"를 반환하고, 키가 이미 존재하면 nil을 반환합니다.
        // `redis-rs`는 "OK"를 `Some("OK".to_string())`으로, nil을 `None`으로 변환합니다.
        if result.is_some() {
            Ok(Some(Self {
                key: key.to_string(),
                value,
            }))
        } else {
            Ok(None)
        }
    }

    /// 획득했던 분산락을 해제합니다.
    pub async fn release(&self, redis: &mut ConnectionManager) -> RedisResult<()> {
        let script = Script::new(RELEASE_SCRIPT);
        script
            .key(&self.key)
            .arg(&self.value)
            .invoke_async::<_, ()>(redis)
            .await?;
        Ok(())
    }
}
</file>

<file path="src/matchmaker/messages.rs">
use crate::env::GameModeSettings;
use actix::Message;
use uuid::Uuid;
use std::time::Duration;

#[derive(Message)]
#[rtype(result = "()")]
pub struct EnqueuePlayer {
    pub player_id: Uuid,
    pub game_mode: String,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct DequeuePlayer {
    pub player_id: Uuid,
    pub game_mode: String,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct HandleLoadingComplete {
    pub player_id: Uuid,
    pub loading_session_id: Uuid,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct CancelLoadingSession {
    pub player_id: Uuid,
    pub loading_session_id: Uuid,
}

#[derive(Message, Clone)]
#[rtype(result = "()")]
pub(super) struct TryMatch {
    pub(super) game_mode: GameModeSettings,
}

/// 오래된 로딩 세션을 정리하기 위한 내부 메시지입니다.
#[derive(Message)]
#[rtype(result = "()")]
pub(super) struct CheckStaleLoadingSessions;

#[derive(Message)]
#[rtype(result = "()")]
pub struct DelayedRequeuePlayers {
    pub player_ids: Vec<String>,
    pub game_mode: String,
    pub delay: Duration,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct RetryRequeuePlayers {
    pub player_ids: Vec<String>,
    pub game_mode: String,
    pub retry_count: u32,
}
</file>

<file path="src/matchmaker/mod.rs">
pub mod actor;
mod handlers;
mod lock;
pub mod messages;
mod scripts;
pub use actor::Matchmaker;
</file>

<file path="src/matchmaker/scripts.rs">
pub(super) const ATOMIC_MATCH_SCRIPT: &str = r#"
    local queue_key = KEYS[1]
    local required_players = tonumber(ARGV[1])
    local loading_session_id = ARGV[2]
    local current_timestamp = ARGV[3]
    local loading_session_timeout_seconds = tonumber(ARGV[4])

    -- Extract game_mode from queue_key (e.g., "queue:game_mode_id" -> "game_mode_id")
    local game_mode_start_index = string.find(queue_key, ":")
    local game_mode = string.sub(queue_key, game_mode_start_index + 1)

    if redis.call('SCARD', queue_key) >= required_players then
        local player_ids = redis.call('SPOP', queue_key, required_players)
        if #player_ids == required_players then
            local loading_key = "loading:" .. loading_session_id
            local hset_args = {loading_key, "game_mode", game_mode, "created_at", current_timestamp, "status", "loading"}
            for i=1, #player_ids do
                table.insert(hset_args, player_ids[i])
                table.insert(hset_args, "loading")
            end
            redis.call('HMSET', unpack(hset_args))
            redis.call('EXPIRE', loading_key, loading_session_timeout_seconds)

            -- Return game_mode, loading_session_id, and player_ids
            local result = {game_mode, loading_session_id}
            for i=1, #player_ids do
                table.insert(result, player_ids[i])
            end
            return result
        else
            -- Not enough players popped, re-add them to the queue (this should ideally not happen if SCARD check is accurate)
            if #player_ids > 0 then
                redis.call('SADD', queue_key, unpack(player_ids))
            end
            return {}
        end
    else
        return {}
    end
"#;

pub(super) const ATOMIC_LOADING_COMPLETE_SCRIPT: &str = r#"
    local loading_key = KEYS[1]
    local player_id = ARGV[1]

    -- Stop if session does not exist or is already handled
    if redis.call('EXISTS', loading_key) == 0 then
        return {}
    end
    -- 추가: 세션 상태가 'loading'이 아니면 (예: 'cancelled') 중단
    local status = redis.call('HGET', loading_key, 'status')
    if status and status ~= 'loading' then
        return {}
    end

    redis.call('HSET', loading_key, player_id, 'ready')

    local players = redis.call('HGETALL', loading_key)
    local all_ready = true
    local player_ids = {}
    local game_mode = ''
    for i=1, #players, 2 do
        if players[i] == 'game_mode' then
            game_mode = players[i+1]
        elseif players[i] ~= 'created_at' and players[i] ~= 'status' then
            if players[i+1] ~= 'ready' then
                all_ready = false
                break
            end
            table.insert(player_ids, players[i])
        end
    end

    if all_ready and #player_ids > 0 then
        -- 성공 시 키 삭제
        redis.call('DEL', loading_key)
        -- game_mode를 맨 앞에 추가하여 반환
        table.insert(player_ids, 1, game_mode)
        return player_ids
    else
        return {}
    end
"#;

pub(super) const ATOMIC_CANCEL_SESSION_SCRIPT: &str = r#"
    local loading_key = KEYS[1]
    local disconnected_player_id = ARGV[1]

    -- 세션이 존재하는지 확인
    if redis.call('EXISTS', loading_key) == 0 then
        return {} -- 이미 처리되었거나 존재하지 않음
    end

    -- 세션 정보를 가져옴
    local all_players_status = redis.call('HGETALL', loading_key)

    -- 세션 키를 삭제하여 다른 프로세스의 개입을 막음
    redis.call('DEL', loading_key)

    local game_mode = ''
    local players_to_requeue = {}

    -- 플레이어 목록을 순회하며 재입장시킬 플레이어를 찾음
    for i=1, #all_players_status, 2 do
        local key = all_players_status[i]
        local value = all_players_status[i+1]

        if key == 'game_mode' then
            game_mode = value
        elseif key ~= 'created_at' and key ~= 'status' and key ~= disconnected_player_id then
            table.insert(players_to_requeue, key)
        end
    end

    -- game_mode를 맨 앞에 추가하여 반환
    table.insert(players_to_requeue, 1, game_mode)
    return players_to_requeue
"#;

pub(super) const CLEANUP_STALE_SESSION_SCRIPT: &str = r#"
    local loading_key = KEYS[1]
    local current_time = tonumber(ARGV[1])
    local timeout_seconds = tonumber(ARGV[2])

    -- 'created_at' 필드를 가져옴
    local created_at = redis.call('HGET', loading_key, 'created_at')

    -- 키가 존재하지 않거나 'created_at' 필드가 없으면 아무것도 하지 않음
    if not created_at then
        return {}
    end

    -- 타임아웃이 지났는지 확인
    if current_time > tonumber(created_at) + timeout_seconds then
        -- 타임아웃된 세션의 모든 정보를 가져옴
        local all_players_status = redis.call('HGETALL', loading_key)
        -- 세션 키를 삭제
        redis.call('DEL', loading_key)

        local game_mode = ''
        local players_to_requeue = {}

        -- 플레이어 목록을 순회하며 재입장시킬 플레이어를 찾음
        for i=1, #all_players_status, 2 do
            local key = all_players_status[i]
            local value = all_players_status[i+1]

            if key == 'game_mode' then
                game_mode = value
            elseif key ~= 'created_at' and key ~= 'status' then
                table.insert(players_to_requeue, key)
            end
        end
        
        -- game_mode를 맨 앞에 추가하여 반환
        table.insert(players_to_requeue, 1, game_mode)
        return players_to_requeue
    end

    -- 타임아웃되지 않았으면 빈 테이블 반환
    return {}
"#;
</file>

<file path="src/protocol.rs">
use actix::prelude::*;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

// --- Client to Server Messages ---

#[derive(Deserialize, Message)]
#[rtype(result = "()")]
#[serde(tag = "type")]
pub enum ClientMessage {
    /// 플레이어가 매칭 대기열에 들어가기를 요청합니다.
    #[serde(rename = "enqueue")]
    Enqueue { player_id: Uuid, game_mode: String },
    /// 클라이언트가 에셋 로딩을 완료했음을 서버에 알립니다.
    #[serde(rename = "loading_complete")]
    LoadingComplete { loading_session_id: Uuid },
}

// --- Server to Client Messages ---

#[derive(Serialize, Deserialize, Message, Clone)]
#[rtype(result = "()")]
#[serde(tag = "type")]
pub enum ServerMessage {
    /// 대기열에 성공적으로 등록되었음을 알립니다.
    #[serde(rename = "queued")]
    Queued,

    /// 클라이언트에게 에셋 로딩을 시작하라고 지시합니다.
    #[serde(rename = "start_loading")]
    StartLoading { loading_session_id: Uuid },

    /// 최종적으로 매칭이 성사되었고, 게임 서버 접속 정보를 전달합니다.
    #[serde(rename = "match_found")]
    MatchFound {
        session_id: Uuid, // dedicated_server의 게임 세션 ID
        server_address: String,
    },

    /// 에러가 발생했음을 알립니다.
    #[serde(rename = "error")]
    Error { message: String },
}
</file>

<file path="src/provider.rs">
use actix::{Actor, Context, Handler, Message, ResponseFuture};
use futures_util::stream::StreamExt;
use redis::{aio::ConnectionManager, AsyncCommands};
use serde::Deserialize;
use tracing::{info, warn};

// --- Actor Definition ---

/// 사용 가능한 Dedicated Server를 찾아 제공하는 책임을 가진 액터입니다.
pub struct DedicatedServerProvider {
    redis: ConnectionManager,
}

impl DedicatedServerProvider {
    pub fn new(redis: ConnectionManager) -> Self {
        Self { redis }
    }
}

impl Actor for DedicatedServerProvider {
    type Context = Context<Self>;
}

// --- Message Definition ---

/// 사용 가능한 서버를 찾아달라는 메시지입니다.
#[derive(Message)]
#[rtype(result = "Result<ServerInfo, anyhow::Error>")]
pub struct FindAvailableServer;

/// 찾아낸 서버의 정보를 담는 구조체입니다.
#[derive(Deserialize, Debug, Clone)]
pub struct ServerInfo {
    pub address: String,
    pub status: String,
}

// --- Message Handler ---

impl Handler<FindAvailableServer> for DedicatedServerProvider {
    type Result = ResponseFuture<Result<ServerInfo, anyhow::Error>>;

    /// `FindAvailableServer` 메시지를 처리합니다.
    fn handle(&mut self, _msg: FindAvailableServer, _ctx: &mut Context<Self>) -> Self::Result {
        let mut redis = self.redis.clone();

        Box::pin(async move {
            info!("Finding an available dedicated server from Redis...");

            // SCAN 사용으로 Redis 블로킹 방지
            let mut keys: Vec<String> = Vec::new();
            match redis.scan_match::<_, String>("dedicated_server:*").await {
                Ok(mut iter) => {
                    while let Some(key) = iter.next().await {
                        keys.push(key);
                    }
                }
                Err(e) => {
                    warn!("Failed to scan dedicated servers: {}", e);
                    return Err(anyhow::anyhow!("Failed to scan dedicated servers: {}", e));
                }
            };

            // 각 서버의 상태를 확인하여 "idle"인 서버를 찾습니다.
            for key in keys {
                let server_info_json: String = match redis.get(&key).await {
                    Ok(info) => info,
                    Err(e) => {
                        warn!(
                            "Failed to get server info for key {}: {}. Skipping.",
                            key, e
                        );
                        continue; // 다음 키로 넘어감
                    }
                };

                let server_info: ServerInfo = match serde_json::from_str(&server_info_json) {
                    Ok(info) => info,
                    Err(e) => {
                        warn!(
                            "Failed to parse server info for key {}: {}. Skipping.",
                            key, e
                        );
                        continue; // 다음 키로 넘어감
                    }
                };

                // "idle" 상태인 서버를 찾으면 즉시 반환합니다.
                if server_info.status == "idle" {
                    info!("Found idle server: {:?}", server_info);
                    return Ok(server_info);
                }
            }

            // 모든 서버를 확인했지만 "idle" 상태인 서버가 없는 경우
            warn!("All dedicated servers are currently busy.");
            Err(anyhow::anyhow!("All dedicated servers are busy."))
        })
    }
}
</file>

<file path="src/pubsub.rs">
use actix::{
    Actor, Addr, AsyncContext, Context, ContextFutureSpawner, Handler, Message, WrapFuture,
};
use futures_util::stream::StreamExt;
use redis::Client as RedisClient;
use simulator_metrics::{APPLICATION_RESTARTS_TOTAL, REDIS_CONNECTION_FAILURES_TOTAL};
use std::collections::HashMap;
use std::time::Duration;
use tokio::sync::mpsc;
use tracing::{error, info, warn};
use uuid::Uuid;

use crate::protocol::ServerMessage;
use crate::ws_session::MatchmakingSession;

// --- Messages for this module ---
#[derive(Message)]
#[rtype(result = "()")]
struct Connect;

// --- SubscriptionManager Actor ---

/// Manages the mapping between player_id and their WebSocket session actor address.
pub struct SubscriptionManager {
    sessions: HashMap<Uuid, Addr<MatchmakingSession>>,
}

impl SubscriptionManager {
    pub fn new() -> Self {
        Self {
            sessions: HashMap::new(),
        }
    }
}

impl Actor for SubscriptionManager {
    type Context = Context<Self>;
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct Register {
    pub player_id: Uuid,
    pub addr: Addr<MatchmakingSession>,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct Deregister {
    pub player_id: Uuid,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct ForwardMessage {
    pub player_id: Uuid,
    pub message: ServerMessage,
}

impl Handler<Register> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: Register, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Player {} registered for notifications.", msg.player_id);
        self.sessions.insert(msg.player_id, msg.addr);
    }
}

impl Handler<Deregister> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: Deregister, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Player {} deregistered.", msg.player_id);
        self.sessions.remove(&msg.player_id);
    }
}

impl Handler<ForwardMessage> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: ForwardMessage, _ctx: &mut Context<Self>) -> Self::Result {
        if let Some(recipient_addr) = self.sessions.get(&msg.player_id) {
            recipient_addr.do_send(msg.message);
        } else {
            warn!(
                "Could not find session for player {} to forward message.",
                msg.player_id
            );
        }
    }
}

// --- RedisSubscriber Actor ---

pub struct RedisSubscriber {
    redis_client: RedisClient,
    manager_addr: Addr<SubscriptionManager>,
    reconnect_attempts: u32,
    max_reconnect_attempts: u32,
    initial_reconnect_delay_ms: u64,
    max_reconnect_delay_ms: u64,
    shutdown_tx: mpsc::Sender<()>, // Shutdown channel sender
}

impl RedisSubscriber {
    pub fn new(
        redis_client: RedisClient,
        manager_addr: Addr<SubscriptionManager>,
        max_reconnect_attempts: u32,
        initial_reconnect_delay_ms: u64,
        max_reconnect_delay_ms: u64,
        shutdown_tx: mpsc::Sender<()>,
    ) -> Self {
        Self {
            redis_client,
            manager_addr,
            reconnect_attempts: 0,
            max_reconnect_attempts,
            initial_reconnect_delay_ms,
            max_reconnect_delay_ms,
            shutdown_tx,
        }
    }

    fn connect_and_subscribe(&mut self, ctx: &mut Context<Self>) {
        info!("Attempting to connect and subscribe to Redis...");
        let client = self.redis_client.clone();
        let manager = self.manager_addr.clone();
        let self_addr = ctx.address();
        let current_reconnect_attempts = self.reconnect_attempts;
        let max_reconnect_attempts = self.max_reconnect_attempts;
        let shutdown_tx = self.shutdown_tx.clone();

        async move {
            if current_reconnect_attempts >= max_reconnect_attempts {
                REDIS_CONNECTION_FAILURES_TOTAL
                    .with_label_values(&["pubsub"])
                    .inc();
                APPLICATION_RESTARTS_TOTAL.inc();
                error!(
                    "Max Redis reconnect attempts ({}) reached. Sending shutdown signal.",
                    max_reconnect_attempts
                );
                if shutdown_tx.send(()).await.is_err() {
                    error!("Failed to send shutdown signal. Forcing exit.");
                    std::process::exit(1); // Fallback
                }
                return;
            }

            let conn = match client.get_async_connection().await {
                Ok(c) => {
                    info!("Successfully connected to Redis.");
                    self_addr.do_send(ResetReconnectAttempts); // Reset on success
                    c
                }
                Err(e) => {
                    REDIS_CONNECTION_FAILURES_TOTAL
                        .with_label_values(&["pubsub"])
                        .inc();
                    error!("RedisSubscriber failed to get connection: {}", e);
                    self_addr.do_send(Connect); // Trigger reconnect
                    return;
                }
            };
            let mut pubsub = conn.into_pubsub();
            if let Err(e) = pubsub.psubscribe("notifications:*").await {
                REDIS_CONNECTION_FAILURES_TOTAL
                    .with_label_values(&["pubsub"])
                    .inc();
                error!("RedisSubscriber failed to psubscribe: {}", e);
                self_addr.do_send(Connect); // Trigger reconnect
                return;
            }
            info!("Successfully subscribed to 'notifications:*'");

            let mut stream = pubsub.on_message();
            while let Some(msg) = stream.next().await {
                let channel: String = msg.get_channel_name().to_string();
                let payload: String = match msg.get_payload() {
                    Ok(p) => p,
                    Err(_) => continue,
                };

                if let Some(player_id_str) = channel.strip_prefix("notifications:") {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        if let Ok(server_msg) = serde_json::from_str::<ServerMessage>(&payload) {
                            manager.do_send(ForwardMessage {
                                player_id,
                                message: server_msg,
                            });
                        }
                    }
                }
            }
            warn!("Redis Pub/Sub stream ended. Attempting to reconnect...");
            self_addr.do_send(Connect); // Trigger reconnect
        }
        .into_actor(self)
        .wait(ctx);
    }
}

impl Actor for RedisSubscriber {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Context<Self>) {
        info!("RedisSubscriber actor started.");
        self.connect_and_subscribe(ctx);
    }
}

#[derive(Message)]
#[rtype(result = "()")]
struct ResetReconnectAttempts;

impl Handler<ResetReconnectAttempts> for RedisSubscriber {
    type Result = ();
    fn handle(&mut self, _msg: ResetReconnectAttempts, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Redis connection successful. Resetting reconnect attempts.");
        self.reconnect_attempts = 0;
    }
}

impl Handler<Connect> for RedisSubscriber {
    type Result = ();

    fn handle(&mut self, _msg: Connect, ctx: &mut Context<Self>) -> Self::Result {
        self.reconnect_attempts += 1;
        let delay = Duration::from_millis(std::cmp::min(
            self.max_reconnect_delay_ms,
            self.initial_reconnect_delay_ms * (2u64.pow(self.reconnect_attempts - 1)),
        ));
        info!("Reconnect message received. Attempt: {}. Waiting for a delay of {:?} before next attempt.", self.reconnect_attempts, delay);
        ctx.run_later(delay, |act, ctx| {
            act.connect_and_subscribe(ctx);
        });
    }
}
</file>

<file path="src/util/mod.rs">

</file>

<file path="src/ws_session.rs">
use crate::{
    matchmaker::messages::{CancelLoadingSession, DequeuePlayer, EnqueuePlayer},
    protocol::{ClientMessage, ServerMessage},
    pubsub::{Deregister, Register},
    Matchmaker, SubscriptionManager,
};
use actix::{
    fut, Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Handler, Running, StreamHandler,
};
use actix_web_actors::ws;
use std::time::{Duration, Instant};
use tracing::{info, warn};
use uuid::Uuid;

pub struct MatchmakingSession {
    player_id: Option<Uuid>,
    game_mode: Option<String>,
    loading_session_id: Option<Uuid>,
    is_stopping: bool,
    hb: Instant,
    matchmaker_addr: Addr<Matchmaker>,
    sub_manager_addr: Addr<SubscriptionManager>,
    heartbeat_interval: Duration,
    client_timeout: Duration,
}

impl MatchmakingSession {
    pub fn new(
        matchmaker_addr: Addr<Matchmaker>,
        sub_manager_addr: Addr<SubscriptionManager>,
        heartbeat_interval: Duration,
        client_timeout: Duration,
    ) -> Self {
        Self {
            player_id: None,
            game_mode: None,
            loading_session_id: None,
            is_stopping: false,
            hb: Instant::now(),
            matchmaker_addr,
            sub_manager_addr,
            heartbeat_interval,
            client_timeout,
        }
    }

    fn hb(&self, ctx: &mut ws::WebsocketContext<Self>) {
        ctx.run_interval(self.heartbeat_interval, |act, ctx| {
            if Instant::now().duration_since(act.hb) > act.client_timeout {
                info!("Websocket Client heartbeat failed, disconnecting!");
                ctx.stop();
                return;
            }
            ctx.ping(b"");
        });
    }
}

impl Actor for MatchmakingSession {
    type Context = ws::WebsocketContext<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        info!("MatchmakingSession started.");
        self.hb(ctx);
    }

    fn stopping(&mut self, ctx: &mut Self::Context) -> Running {
        // 이미 종료 절차가 시작되었다면, 추가 작업을 하지 않고 즉시 중단
        if self.is_stopping {
            return Running::Stop;
        }

        // 종료 절차 시작
        self.is_stopping = true;

        if let Some(player_id) = self.player_id {
            info!(
                "Player {:?} disconnected. Starting graceful shutdown...",
                self.player_id
            );

            let sub_manager_addr_inner = self.sub_manager_addr.clone();
            let matchmaker_addr_inner = self.matchmaker_addr.clone();
            let loading_session_id = self.loading_session_id;
            let game_mode = self.game_mode.clone();

            // 1. 정리 작업을 나타내는 비동기 Future를 생성합니다.
            let cleanup_future = async move {
                let deregister_fut = sub_manager_addr_inner.send(Deregister { player_id });

                match (loading_session_id, game_mode.clone()) {
                    (Some(loading_session_id), _) => {
                        let cancel_fut = matchmaker_addr_inner.send(CancelLoadingSession {
                            player_id,
                            loading_session_id,
                        });
                        // 두 작업이 모두 완료될 때까지 기다립니다.
                        _ = tokio::join!(deregister_fut, cancel_fut);
                    }
                    (None, Some(game_mode)) => {
                        let dequeue_fut = matchmaker_addr_inner.send(DequeuePlayer {
                            player_id,
                            game_mode,
                        });
                        // 두 작업이 모두 완료될 때까지 기다립니다.
                        _ = tokio::join!(deregister_fut, dequeue_fut);
                    }
                    _ => {
                        // Deregister만 처리
                        _ = deregister_fut.await;
                    }
                }
            };

            // 2. Future를 액터의 컨텍스트에서 실행하고, 완료된 후 액터를 중지시킵니다.
            let graceful_shutdown =
                fut::wrap_future::<_, Self>(cleanup_future).then(|_result, _actor, ctx| {
                    info!(
                        "Cleanup finished for player {:?}. Stopping actor now.",
                        _actor.player_id
                    );
                    ctx.stop(); // <-- 모든 작업이 끝난 후 액터를 진짜로 중지
                    fut::ready(())
                });

            ctx.wait(graceful_shutdown);

            // 3. 액터가 정리 작업을 완료할 때까지 살아있도록 Continue를 반환
            Running::Continue
        } else {
            // 정리할 플레이어 정보가 없으면 즉시 중지
            info!("Anonymous session is stopping.");
            Running::Stop
        }
    }
}

// This handler now receives messages forwarded from the SubscriptionManager
impl Handler<ServerMessage> for MatchmakingSession {
    type Result = ();

    fn handle(&mut self, msg: ServerMessage, ctx: &mut Self::Context) {
        if let ServerMessage::StartLoading { loading_session_id } = &msg {
            self.loading_session_id = Some(*loading_session_id);
        }
        match serde_json::to_string(&msg) {
            Ok(text) => ctx.text(text),
            Err(e) => warn!("Failed to serialize ServerMessage for client: {}", e),
        }
    }
}

impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for MatchmakingSession {
    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
        match msg {
            Ok(ws::Message::Ping(msg)) => {
                self.hb = Instant::now();
                ctx.pong(&msg);
            }
            Ok(ws::Message::Pong(_)) => {
                self.hb = Instant::now();
            }
            Ok(ws::Message::Text(text)) => {
                match serde_json::from_str::<ClientMessage>(&text) {
                    Ok(ClientMessage::Enqueue {
                        player_id,
                        game_mode,
                    }) => {
                        // Check if player is already enqueued to prevent duplicate requests
                        if self.player_id.is_some() {
                            warn!(
                                "Player {:?} tried to enqueue more than once.",
                                self.player_id
                            );
                            return;
                        }

                        info!(
                            "Player {} requests queue for {}. Updating session state.",
                            player_id, game_mode
                        );

                        // *** Critical fix: Update actor's state immediately ***
                        self.player_id = Some(player_id);
                        self.game_mode = Some(game_mode.clone());

                        // Register with the SubscriptionManager
                        self.sub_manager_addr.do_send(Register {
                            player_id,
                            addr: ctx.address(),
                        });

                        // Send enqueue message to the matchmaker
                        self.matchmaker_addr.do_send(EnqueuePlayer {
                            player_id,
                            game_mode,
                        });
                    }
                    Ok(ClientMessage::LoadingComplete { loading_session_id }) => {
                        if let Some(player_id) = self.player_id {
                            info!(
                                "Player {} finished loading for session {}",
                                player_id, loading_session_id
                            );
                            self.matchmaker_addr.do_send(
                                crate::matchmaker::messages::HandleLoadingComplete {
                                    player_id,
                                    loading_session_id,
                                },
                            );
                        } else {
                            warn!("Received LoadingComplete from a session with no player_id.");
                        }
                    }
                    Err(e) => {
                        warn!("Failed to parse client message: {}", e);
                        match serde_json::to_string(&ServerMessage::Error {
                            message: "Invalid message format".to_string(),
                        }) {
                            Ok(text) => ctx.text(text),
                            Err(e) => warn!("Failed to serialize error message for client: {}", e),
                        }
                    }
                }
            }
            Ok(ws::Message::Close(reason)) => {
                ctx.close(reason);
                ctx.stop();
            }
            _ => ctx.stop(),
        }
    }
}
</file>

</files>
