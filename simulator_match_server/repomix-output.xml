This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/main.yml
.gitignore
Cargo.toml
config/development.toml
config/production.toml
review.md
scripts/ATOMIC_CANCEL_SESSION_SCRIPT.lua
scripts/ATOMIC_LOADING_COMPLETE_SCRIPT.lua
scripts/ATOMIC_MATCH_SCRIPT.lua
scripts/CLEANUP_STALE_SESSION_SCRIPT.lua
src/auth.rs
src/debug.rs
src/env.rs
src/events.rs
src/lib.rs
src/main.rs
src/matchmaker/actor.rs
src/matchmaker/handlers.rs
src/matchmaker/lock.rs
src/matchmaker/messages.rs
src/matchmaker/mod.rs
src/matchmaker/scripts.rs
src/protocol.rs
src/provider/mod.rs
src/pubsub.rs
src/util/mod.rs
src/ws_session.rs
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/main.yml">
name: Code Coverage

on:
  pull_request:
    branches: [ main, master ]

jobs:
  codecov:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
          components: llvm-tools-preview
      
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      
      - name: Generate code coverage
        run: cargo llvm-cov --all-features --workspace --lcov --output-path lcov.info
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: lcov.info
</file>

<file path=".gitignore">
/target
</file>

<file path="Cargo.toml">
[package]
name = "match_server"
version = "0.1.0"
edition = "2021"
description = "card backend"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html



[dependencies]
backoff = "0.4"
tokio = { version = "1.15", features = ["full", "tracing"] }
tracing = "0.1.41"
tracing-appender = "0.2.3"
tracing-subscriber = { version = "0.3.19", features = ["env-filter"] }
actix-web = "4.9.0"
actix = "=0.13.5"
actix-ws = "0.2.5"
actix-web-actors = "4.3.0"
async-tungstenite = {version="0.28.2", features = ["tokio-runtime"]}
reqwest = { version = "0.12.12", features = ["json"] }
url = "2.5.4"
uuid = { version = "1.14.0", features = ["v4", "serde"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
rand_core = "0.6"
rand = "0.8.5"
futures-util = {version = "0.3", features = ["sink"]}
futures = "0.3.31"
ctor = "0.4.1"
redis = { version = "0.22.3", features = ["tokio-comp", "connection-manager"] }
thiserror-core = "1.0.50"
dotenv = "0.15.0"
thiserror = "2.0.12"
config = {version = "0.15.11", features = ["toml"]}
jsonwebtoken = "9.3.1"
anyhow = "1.0"
actix-web-prom = "0.10.0"
simulator_metrics = { path = "../simulator_metrics" }
chrono = { version = "0.4.41", features = ["serde"] }
hex = "0.4.3"
</file>

<file path="config/development.toml">
# 서버 설정
[server]
bind_address = "0.0.0.0"
port = 8080
log_level = "info" # 로그 레벨

# 로깅 설정
[logging]
directory = "logs"
filename = "app.log"

# Redis 설정
[redis]
url = "redis://127.0.0.1:6379"
max_reconnect_attempts = 10
initial_reconnect_delay_ms = 1000
max_reconnect_delay_ms = 60000
dedicated_server_key_pattern = "dedicated_server:*"
notification_channel_pattern = "notifications:*"

[jwt]
secret = "your-super-secret-and-long-key-that-no-one-knows"

# 매치메이킹 설정
[matchmaking]
tick_interval_seconds = 5
queue_key_prefix = "queue"
heartbeat_interval_seconds = 30
client_timeout_seconds = 120
loading_session_timeout_seconds = 60

# 게임 서버 상태 문자열
[server_status]
idle = "idle"

# 게임 모드 목록 (테이블 배열)
# 각 [[matchmaking.game_modes]] 항목이 하나의 게임 모드를 정의합니다.
[[matchmaking.game_modes]]
id = "Normal_1v1"          # 게임 모드의 고유 ID
required_players = 2       # 매칭에 필요한 인원
use_mmr_matching = false   # MMR 기반 매칭 사용 여부 (일반 모드)

# [[matchmaking.game_modes]]
# id = "Ranked_1v1"
# required_players = 2
# use_mmr_matching = true    # 랭크 모드에서는 이 값을 true로 설정
</file>

<file path="config/production.toml">
# 서버 설정
[server]
bind_address = "0.0.0.0"
port = 8080
log_level = "warn" # 로그 레벨

# 로깅 설정
[logging]
directory = "logs"
filename = "app.log"

# Redis 설정
[redis]
url = "redis://your-production-redis-endpoint:6379"
max_reconnect_attempts = 10
initial_reconnect_delay_ms = 1000
max_reconnect_delay_ms = 60000
dedicated_server_key_pattern = "dedicated_server:*"
notification_channel_pattern = "notifications:*"

[jwt]
secret = "a-very-secure-secret-that-should-be-injected-via-env-vars"

# 매치메이킹 설정
[matchmaking]
tick_interval_seconds = 5
queue_key_prefix = "queue"
heartbeat_interval_seconds = 30
client_timeout_seconds = 120
loading_session_timeout_seconds = 60

# 게임 서버 상태 문자열
[server_status]
idle = "idle"

# 게임 모드 목록 (테이블 배열)
[[matchmaking.game_modes]]
id = "Normal_1v1"
required_players = 2
use_mmr_matching = false

[[matchmaking.game_modes]]
id = "Ranked_1v1"
required_players = 2
use_mmr_matching = true
</file>

<file path="review.md">
네, 정확합니다. 원래 의도하신 "쿠버네티스가 비정상 종료를 감지하고 복구하도록 한다"는 전략은 클라우드 네이티브 환경에서 매우 올바른 방향입니다.

문제는 **어떻게** 비정상 종료를 하느냐에 있습니다. `std::process::exit(1)`는 가장 간단하지만 가장 무책임한 방법입니다. 마치 퇴근 시간이 되었다고 컴퓨터 전원을 그냥 뽑아버리는 것과 같습니다.

사용자께서 제안하신 **"Terminate 메시지를 만들어 리소스 정리 후 비정상 종료"** 하는 방식이 바로 이 문제를 해결하는 **가장 정석적이고 우아한 해법**입니다. 이를 "Graceful Shutdown Triggering" 패턴이라고 부를 수 있습니다.

### 왜 `std::process::exit(1)`가 위험한가?

`RedisSubscriber`가 `std::process::exit(1)`을 호출하는 순간, 다른 모든 액터들은 자신의 상태를 정리할 기회를 완전히 잃어버립니다.

- **`Matchmaker` 액터:** 만약 `TryMatch` 핸들러가 실행 중이었다면, 플레이어들을 큐에서 꺼냈지만(`SPOP`) 아직 로딩 세션을 생성하지 않은 상태에서 서버가 꺼질 수 있습니다. 이 플레이어들은 허공에 뜨게 됩니다.
- **`DedicatedServerProvider` 액터:** 진행 중인 작업이 있었다면 중단됩니다.
- **수많은 `MatchmakingSession` 액터들:** `stopping` 메서드가 호출될 기회조차 없이 사라집니다. 이는 수많은 플레이어가 큐나 로딩 세션에서 제거되지 않는 결과를 낳습니다.
- **로그 및 메트릭 유실:** 버퍼에 남아있던 마지막 로그나 메트릭 데이터가 파일이나 네트워크로 전송되지 않고 유실될 수 있습니다.

결론적으로, 서버가 재시작되더라도 이전 상태의 "쓰레기 데이터(stale data)"가 Redis에 그대로 남아있어 시작부터 문제를 안고 가게 됩니다.

---

### Graceful Shutdown Triggering (권장 해결책)

이 패턴은 시스템의 최상위 관리자(이 경우 `main` 함수 또는 감독(Supervisor) 액터)에게 "이제 우리 모두 정중하게 문을 닫아야 할 시간입니다"라고 알리는 방식입니다.

**구현 단계:**

#### 1. `main` 함수에서 시스템 종료를 위한 채널(Channel) 설정

`main` 함수는 모든 액터의 "부모"와 같으므로, 종료 명령을 내리기에 가장 적합한 위치입니다. `tokio::sync::mpsc` 같은 비동기 채널을 사용하여 종료 신호를 전달합니다.

```rust
// in src/main.rs
use tokio::sync::mpsc;

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    // ... 기존 설정 ...

    // 1. 시스템 종료를 위한 MPSC(Multi-Producer, Single-Consumer) 채널 생성
    // tx는 여러 곳에서 복제해서 사용할 수 있고, rx는 한 곳(main)에서만 받습니다.
    let (shutdown_tx, mut shutdown_rx) = mpsc::channel::<()>(1);

    // ... RedisSubscriber 생성 시 shutdown_tx의 복제본을 전달 ...
    RedisSubscriber::new(
        redis_client.clone(),
        sub_manager_addr.clone(),
        10,
        1000,
        60000,
        shutdown_tx.clone(), // <-- 종료 채널의 송신기를 전달
    ).start();

    // ... HttpServer 설정 ...
    let server = HttpServer::new(move || {
        App::new()
            // ...
    })
    .bind(&bind_address)?
    .run();

    let server_handle = server.handle();

    // 2. 종료 신호 대기 및 처리 로직
    tokio::select! {
        // 서버가 정상적으로 완료된 경우
        res = server => {
            info!("Actix-Web server has shut down.");
            res
        },
        // 어딘가에서 종료 신호를 보낸 경우
        _ = shutdown_rx.recv() => {
            error!("Shutdown signal received. Initiating graceful shutdown of Actix-Web server...");
            // 서버를 정중하게 중지시킴 (기존 연결 처리가 끝날 때까지 기다림)
            server_handle.stop(true).await;

            info!("Server stopped. Exiting with error code to trigger K8s restart.");
            // 모든 정리가 끝난 후, 쿠버네티스가 감지하도록 비정상 종료
            std::process::exit(1);
        }
    }
}
```

#### 2. `RedisSubscriber`가 종료 신호 보내기

`RedisSubscriber`는 이제 직접 프로세스를 죽이는 대신, `main` 함수에게 "더 이상 가망이 없으니 종료 절차를 시작해 주세요"라고 요청합니다.

```rust
// in src/pubsub.rs
use tokio::sync::mpsc;

pub struct RedisSubscriber {
    // ... 기존 필드
    shutdown_tx: mpsc::Sender<()>, // <-- 종료 채널 송신기
}

impl RedisSubscriber {
    pub fn new(
        // ... 기존 인자
        shutdown_tx: mpsc::Sender<()>,
    ) -> Self {
        Self {
            // ...
            shutdown_tx,
        }
    }

    fn connect_and_subscribe(&mut self, ctx: &mut Context<Self>) {
        // ...
        let shutdown_tx = self.shutdown_tx.clone(); // 복제해서 async 블록으로 이동

        async move {
            if current_reconnect_attempts >= max_reconnect_attempts {
                error!("Max Redis reconnect attempts reached. Sending shutdown signal.");
                // 직접 종료하는 대신, main 함수에 종료 요청 메시지를 보냄
                if shutdown_tx.send(()).await.is_err() {
                    error!("Failed to send shutdown signal. Forcing exit.");
                    std::process::exit(1); // 만약 채널마저 닫혔다면 최후의 수단 사용
                }
                return;
            }
            // ...
        }.into_actor(self).wait(ctx);
    }
}
```

#### 3. (선택적) 다른 액터들의 Graceful Shutdown

`actix-web`의 `server_handle.stop(true)`는 새로운 연결을 받지 않고 기존 HTTP 요청이 완료될 때까지 기다려주는 역할을 합니다. 하지만 이와 별개로 백그라운드에서 동작하는 `Matchmaker` 같은 액터들은 별도의 정리 로직이 필요할 수 있습니다. `main` 함수에서 `server_handle.stop()` 호출 전, 이들 액터에게도 `System::current().stop()` 등을 통해 종료 신호를 보내고 잠시 기다려주는 로직을 추가할 수 있습니다.

### 이 방식의 장점

1.  **질서 있는 종료:** `main` 함수가 중앙 통제소 역할을 하여 모든 시스템이 질서정연하게 종료될 기회를 갖습니다. `actix-web` 서버는 처리 중인 요청을 마저 처리하고, 다른 액터들도 `stopping` 라이프사이클 훅을 통해 자신의 상태를 정리할 수 있습니다.
2.  **데이터 정합성 유지:** "유령 플레이어"나 "허공에 뜬 매치" 같은 쓰레기 데이터가 남을 가능성을 최소화합니다.
3.  **의도 명확화:** "쿠버네티스에 의한 복구"라는 원래의 의도는 그대로 유지됩니다. 모든 정리 작업이 끝난 후 마지막에 `std::process::exit(1)`을 호출하여 쿠버네티스에게 "의도된 비정상 종료"임을 명확히 알립니다.
4.  **중앙화된 제어:** 종료 로직이 `main` 함수 한 곳에 모여있어 관리하기 쉽고, 나중에 다른 종료 조건(예: 특정 관리자 API 호출)을 추가하기도 용이합니다.

결론적으로, `std::process::exit(1)`을 직접 호출하는 대신 **메시지 패싱(Message Passing)**을 통해 중앙 관리자에게 종료를 위임하는 방식은 시스템의 안정성과 예측 가능성을 극적으로 향상시키는 매우 성숙한 엔지니어링 패턴입니다.
</file>

<file path="scripts/ATOMIC_CANCEL_SESSION_SCRIPT.lua">
local loading_key = KEYS[1]
local disconnected_player_id = ARGV[1]

-- 세션이 존재하는지 확인
if redis.call('EXISTS', loading_key) == 0 then
    return {} -- 이미 처리되었거나 존재하지 않음
end

-- 세션 정보를 가져옴
local all_players_status = redis.call('HGETALL', loading_key)

-- 세션 키를 삭제하여 다른 프로세스의 개입을 막음
redis.call('DEL', loading_key)

local game_mode = ''
local players_to_requeue = {}

-- 플레이어 목록을 순회하며 재입장시킬 플레이어를 찾음
for i=1, #all_players_status, 2 do
    local key = all_players_status[i]
    local value = all_players_status[i+1]

    if key == 'game_mode' then
        game_mode = value
    elseif key ~= 'created_at' and key ~= 'status' and key ~= disconnected_player_id then
        table.insert(players_to_requeue, key)
    end
end

-- game_mode를 맨 앞에 추가하여 반환
table.insert(players_to_requeue, 1, game_mode)
return players_to_requeue
</file>

<file path="scripts/ATOMIC_LOADING_COMPLETE_SCRIPT.lua">
local loading_key = KEYS[1]
    local player_id = ARGV[1]

    -- Stop if session does not exist or is already handled
    if redis.call('EXISTS', loading_key) == 0 then
        return {}
    end
    -- 추가: 세션 상태가 'loading'이 아니면 (예: 'cancelled') 중단
    local status = redis.call('HGET', loading_key, 'status')
    if status and status ~= 'loading' then
        return {}
    end

    redis.call('HSET', loading_key, player_id, 'ready')

    local players = redis.call('HGETALL', loading_key)
    local all_ready = true
    local player_ids = {}
    local game_mode = ''
    for i=1, #players, 2 do
        if players[i] == 'game_mode' then
            game_mode = players[i+1]
        elseif players[i] ~= 'created_at' and players[i] ~= 'status' then
            if players[i+1] ~= 'ready' then
                all_ready = false
                break
            end
            table.insert(player_ids, players[i])
        end
    end

    if all_ready and #player_ids > 0 then
        -- 성공 시 키 삭제
        redis.call('DEL', loading_key)
        -- game_mode를 맨 앞에 추가하여 반환
        table.insert(player_ids, 1, game_mode)
        return player_ids
    else
        return {}
    end
</file>

<file path="scripts/ATOMIC_MATCH_SCRIPT.lua">
local queue_key = KEYS[1]
local required_players = tonumber(ARGV[1])
local loading_session_id = ARGV[2]
local current_timestamp = ARGV[3]
local loading_session_timeout_seconds = tonumber(ARGV[4])

-- Extract game_mode from queue_key (e.g., "queue:game_mode_id" -> "game_mode_id")
local game_mode_start_index = string.find(queue_key, ":")
local game_mode = string.sub(queue_key, game_mode_start_index + 1)

if redis.call('SCARD', queue_key) >= required_players then
    local player_ids = redis.call('SPOP', queue_key, required_players)
    if #player_ids == required_players then
        local loading_key = "loading:" .. loading_session_id
        local hset_args = {loading_key, "game_mode", game_mode, "created_at", current_timestamp, "status", "loading"}
        for i=1, #player_ids do
            table.insert(hset_args, player_ids[i])
            table.insert(hset_args, "loading")
        end
        redis.call('HMSET', unpack(hset_args))
        redis.call('EXPIRE', loading_key, loading_session_timeout_seconds)

        -- Return game_mode, loading_session_id, and player_ids
        local result = {game_mode, loading_session_id}
        for i=1, #player_ids do
            table.insert(result, player_ids[i])
        end
        return result
    else
        -- Not enough players popped, re-add them to the queue (this should ideally not happen if SCARD check is accurate)
        if #player_ids > 0 then
            redis.call('SADD', queue_key, unpack(player_ids))
        end
        return {}
    end
else
    return {}
end
</file>

<file path="scripts/CLEANUP_STALE_SESSION_SCRIPT.lua">
local loading_key = KEYS[1]
local current_time = tonumber(ARGV[1])
local timeout_seconds = tonumber(ARGV[2])

-- 'created_at' 필드를 가져옴
local created_at = redis.call('HGET', loading_key, 'created_at')

-- 키가 존재하지 않거나 'created_at' 필드가 없으면 아무것도 하지 않음
if not created_at then
    return {}
end

-- 타임아웃이 지났는지 확인
if current_time > tonumber(created_at) + timeout_seconds then
    -- 타임아웃된 세션의 모든 정보를 가져옴
    local all_players_status = redis.call('HGETALL', loading_key)
    -- 세션 키를 삭제
    redis.call('DEL', loading_key)

    local game_mode = ''
    local players_to_requeue = {}

    -- 플레이어 목록을 순회하며 재입장시킬 플레이어를 찾음
    for i=1, #all_players_status, 2 do
        local key = all_players_status[i]
        local value = all_players_status[i+1]

        if key == 'game_mode' then
            game_mode = value
        elseif key ~= 'created_at' and key ~= 'status' then
            table.insert(players_to_requeue, key)
        end
    end
    
    -- game_mode를 맨 앞에 추가하여 반환
    table.insert(players_to_requeue, 1, game_mode)
    return players_to_requeue
end

-- 타임아웃되지 않았으면 빈 테이블 반환
return {}
</file>

<file path="src/auth.rs">
use actix_web::{dev::Payload, web, FromRequest, HttpRequest};
use jsonwebtoken::{decode, DecodingKey, Validation};
use serde::{Deserialize, Serialize};
use std::future::{ready, Ready};

use crate::AppState;

// JWT Claims 구조체 (auth_server의 것과 동일해야 함)
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Claims {
    pub sub: String, // Subject (user's steam_id)
    pub iat: usize,
    pub exp: usize,
}

// 핸들러에서 인증된 사용자 정보를 담을 구조체
#[derive(Debug)]
pub struct AuthenticatedUser {
    pub steam_id: i64,
}

// Actix-Web 추출기(Extractor) 구현
impl FromRequest for AuthenticatedUser {
    type Error = actix_web::Error;
    type Future = Ready<Result<Self, Self::Error>>;

    fn from_request(req: &HttpRequest, _: &mut Payload) -> Self::Future {
        let app_state = req.app_data::<web::Data<AppState>>().expect(
            "AppState not configured in Actix-Web application. This is a critical server configuration error.",
        );

        // 1. Authorization 헤더에서 토큰 추출
        let auth_header = match req.headers().get("Authorization") {
            Some(header) => match header.to_str() {
                Ok(s) => s,
                Err(_) => {
                    return ready(Err(actix_web::error::ErrorUnauthorized(
                        "Invalid Authorization header encoding",
                    )));
                }
            },
            None => {
                return ready(Err(actix_web::error::ErrorUnauthorized(
                    "Missing Authorization header",
                )));
            }
        };

        if !auth_header.starts_with("Bearer ") {
            return ready(Err(actix_web::error::ErrorUnauthorized(
                "Invalid token format",
            )));
        }

        let token = &auth_header["Bearer ".len()..];

        // 2. JWT 디코딩 및 검증
        let token_data = match decode::<Claims>(
            token,
            &DecodingKey::from_secret(app_state.settings.jwt.secret.as_ref()),
            &Validation::default(),
        ) {
            Ok(data) => data,
            Err(e) => {
                // 로그에 에러 기록
                tracing::warn!("JWT validation failed: {}", e);
                return ready(Err(actix_web::error::ErrorUnauthorized("Invalid token")));
            }
        };

        // 3. Claims에서 steam_id 파싱
        let steam_id = match token_data.claims.sub.parse::<i64>() {
            Ok(id) => id,
            Err(_) => {
                return ready(Err(actix_web::error::ErrorBadRequest(
                    "Invalid steam_id in token",
                )));
            }
        };

        // 4. 성공 시 AuthenticatedUser 반환
        ready(Ok(AuthenticatedUser { steam_id }))
    }
}
</file>

<file path="src/debug.rs">
use actix_web::{get, web, HttpResponse, Result};
use redis::aio::ConnectionLike;
use redis::AsyncCommands;
use serde_json::json;
use std::collections::HashMap;

use crate::matchmaker::messages::GetDebugInfo;
use crate::pubsub::GetActiveSessionsDebug;
use crate::AppState;

/// 큐 상태 조회 - 유령 플레이어 탐지의 핵심
#[get("/debug/queue")]
pub async fn debug_queue_status(state: web::Data<AppState>) -> Result<HttpResponse> {
    let mut redis = state.redis_conn_manager.clone();

    // 1. 큐에 있는 플레이어들 조회
    let queue_members: Vec<String> = redis.smembers("queue:Normal_1v1").await.unwrap_or_default();

    let queue_size: i64 = redis.scard("queue:Normal_1v1").await.unwrap_or(0);

    // 2. 다른 게임 모드들도 조회
    let queue_keys: Vec<String> = redis.keys("queue:*").await.unwrap_or_default();

    let mut all_queues = HashMap::new();
    for queue_key in queue_keys {
        let members: Vec<String> = redis.smembers(&queue_key).await.unwrap_or_default();
        all_queues.insert(queue_key, members);
    }

    Ok(HttpResponse::Ok().json(json!({
        "status": "success",
        "normal_1v1_queue": {
            "size": queue_size,
            "members": queue_members
        },
        "all_queues": all_queues,
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}

/// 활성 세션 상태 조회 - WebSocket 연결 추적
#[get("/debug/sessions")]
pub async fn debug_active_sessions(state: web::Data<AppState>) -> Result<HttpResponse> {
    // SubscriptionManager에서 활성 세션 정보 요청
    let session_info = state.sub_manager_addr.send(GetActiveSessionsDebug).await;

    match session_info {
        Ok(Ok(sessions)) => Ok(HttpResponse::Ok().json(json!({
            "status": "success",
            "active_sessions": sessions,
            "session_count": sessions.len(),
            "timestamp": chrono::Utc::now().to_rfc3339()
        }))),
        _ => Ok(HttpResponse::InternalServerError().json(json!({
            "status": "error",
            "message": "Failed to get session information"
        }))),
    }
}

/// 로딩 세션 상태 조회 - 로딩 중 유령 탐지
#[get("/debug/loading")]
pub async fn debug_loading_sessions(state: web::Data<AppState>) -> Result<HttpResponse> {
    let mut redis = state.redis_conn_manager.clone();

    // 1. 모든 로딩 세션 키 찾기
    let loading_keys: Vec<String> = redis.keys("loading:*").await.unwrap_or_default();

    let mut loading_sessions = HashMap::new();
    for loading_key in loading_keys {
        // 2. 각 로딩 세션의 상세 정보
        let session_data: HashMap<String, String> =
            redis.hgetall(&loading_key).await.unwrap_or_default();

        let ttl: i64 = redis.ttl(&loading_key).await.unwrap_or(-1);

        loading_sessions.insert(
            loading_key,
            json!({
                "players": session_data,
                "ttl_seconds": ttl
            }),
        );
    }

    Ok(HttpResponse::Ok().json(json!({
        "status": "success",
        "loading_sessions": loading_sessions,
        "total_sessions": loading_sessions.len(),
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}

/// Redis 연결 상태 및 헬스체크
#[get("/debug/redis")]
pub async fn debug_redis_health(state: web::Data<AppState>) -> Result<HttpResponse> {
    let mut redis = state.redis_conn_manager.clone();

    let start_time = std::time::Instant::now();

    // Redis ping 테스트
    let ping_result: Result<redis::Value, _> = redis.req_packed_command(&redis::cmd("PING")).await;
    let ping_duration = start_time.elapsed();
    
    let ping_success = ping_result.is_ok();
    let ping_string = match ping_result {
        Ok(redis::Value::Status(s)) => s,
        Ok(other) => format!("{:?}", other),
        Err(e) => format!("Error: {}", e),
    };

    // Redis 메모리 사용량
    let info_result: Result<redis::Value, _> = redis
        .req_packed_command(&redis::cmd("INFO").arg("memory"))
        .await;
    
    let info_string = match info_result {
        Ok(redis::Value::Data(data)) => String::from_utf8_lossy(&data).to_string(),
        Ok(other) => format!("{:?}", other),
        Err(e) => format!("Error: {}", e),
    };

    Ok(HttpResponse::Ok().json(json!({
        "status": "success",
        "ping": {
            "success": ping_success,
            "duration_ms": ping_duration.as_millis(),
            "response": ping_string
        },
        "memory_info": info_string,
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}

/// 종합 유령 플레이어 탐지
#[get("/debug/ghosts")]
pub async fn debug_ghost_detection(state: web::Data<AppState>) -> Result<HttpResponse> {
    let mut redis = state.redis_conn_manager.clone();

    // 1. Redis에서 큐에 있는 플레이어들
    let queue_players: Vec<String> = redis.smembers("queue:Normal_1v1").await.unwrap_or_default();

    // 2. 로딩 중인 플레이어들
    let loading_keys: Vec<String> = redis.keys("loading:*").await.unwrap_or_default();
    let mut loading_players = Vec::new();
    for key in loading_keys {
        let players: HashMap<String, String> = redis.hgetall(&key).await.unwrap_or_default();
        loading_players.extend(players.keys().cloned());
    }

    // 3. 실제 WebSocket 연결된 플레이어들 (SubscriptionManager에서)
    let session_info = state.sub_manager_addr.send(GetActiveSessionsDebug).await;

    let active_players: Vec<String> = match session_info {
        Ok(Ok(sessions)) => sessions.into_iter().map(|s| s.player_id).collect(),
        _ => Vec::new(),
    };

    // 4. 유령 탐지 로직
    let queue_ghosts: Vec<String> = queue_players
        .iter()
        .filter(|player| !active_players.contains(player))
        .cloned()
        .collect();

    let loading_ghosts: Vec<String> = loading_players
        .iter()
        .filter(|player| !active_players.contains(player))
        .cloned()
        .collect();

    Ok(HttpResponse::Ok().json(json!({
        "status": "success",
        "summary": {
            "total_queue_players": queue_players.len(),
            "total_loading_players": loading_players.len(),
            "total_active_connections": active_players.len(),
            "queue_ghosts": queue_ghosts.len(),
            "loading_ghosts": loading_ghosts.len()
        },
        "details": {
            "queue_players": queue_players,
            "loading_players": loading_players,
            "active_connections": active_players,
            "queue_ghosts": queue_ghosts,
            "loading_ghosts": loading_ghosts
        },
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}

/// 매칭메이커 내부 상태 조회
#[get("/debug/matchmaker")]
pub async fn debug_matchmaker_state(state: web::Data<AppState>) -> Result<HttpResponse> {
    let debug_info = state.matchmaker_addr.send(GetDebugInfo).await;

    match debug_info {
        Ok(info_string) => {
            // JSON 문자열을 다시 파싱해서 응답
            match serde_json::from_str::<serde_json::Value>(&info_string) {
                Ok(info_json) => Ok(HttpResponse::Ok().json(json!({
                    "status": "success",
                    "matchmaker_info": info_json,
                    "timestamp": chrono::Utc::now().to_rfc3339()
                }))),
                Err(_) => Ok(HttpResponse::InternalServerError().json(json!({
                    "status": "error",
                    "message": "Failed to parse matchmaker debug info"
                })))
            }
        }
        Err(e) => Ok(HttpResponse::InternalServerError().json(json!({
            "status": "error",
            "message": format!("Failed to get matchmaker info: {}", e)
        }))),
    }
}

/// 전체 서버 상태 대시보드용 종합 정보
#[get("/debug/dashboard")]
pub async fn debug_dashboard(state: web::Data<AppState>) -> Result<HttpResponse> {
    let mut redis = state.redis_conn_manager.clone();

    // 모든 정보를 병렬로 수집
    let queue_size: i64 = redis.scard("queue:Normal_1v1").await.unwrap_or(0);
    let loading_sessions: Vec<String> = redis.keys("loading:*").await.unwrap_or_default();

    let session_info = state.sub_manager_addr.send(GetActiveSessionsDebug).await;

    let active_session_count = match session_info {
        Ok(Ok(sessions)) => sessions.len(),
        _ => 0,
    };

    Ok(HttpResponse::Ok().json(json!({
        "status": "success",
        "server_health": {
            "queue_size": queue_size,
            "loading_sessions": loading_sessions.len(),
            "active_connections": active_session_count,
            "uptime": "TODO", // 서버 시작 시간부터 계산
        },
        "quick_stats": {
            "healthy": queue_size >= 0, // 기본 헬스체크
            "redis_connected": true,    // Redis 응답했으므로 연결됨
        },
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}
</file>

<file path="src/env.rs">
use config::{Config, ConfigError, File};
use serde::Deserialize;

#[derive(Debug, Deserialize, Clone)]
pub struct ServerSettings {
    pub bind_address: String,
    pub port: u16,
    pub log_level: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct LoggingSettings {
    pub directory: String,
    pub filename: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct RedisSettings {
    pub url: String,
    pub max_reconnect_attempts: u32,
    pub initial_reconnect_delay_ms: u64,
    pub max_reconnect_delay_ms: u64,
    pub dedicated_server_key_pattern: String,
    pub notification_channel_pattern: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct JwtSettings {
    pub secret: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct ServerStatusSettings {
    pub idle: String,
}

/// TOML 설정 파일의 [[matchmaking.game_modes]] 테이블에 대응하는 구조체입니다.
#[derive(Debug, Deserialize, Clone)]
pub struct GameModeSettings {
    pub id: String,
    pub required_players: u32,
    pub use_mmr_matching: bool,
}

#[derive(Debug, Deserialize, Clone)]
pub struct MatchmakingSettings {
    pub tick_interval_seconds: u64,
    pub queue_key_prefix: String,
    pub game_modes: Vec<GameModeSettings>,
    pub heartbeat_interval_seconds: u64,
    pub client_timeout_seconds: u64,
    pub loading_session_timeout_seconds: u64,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Settings {
    pub server: ServerSettings,
    pub logging: LoggingSettings,
    pub redis: RedisSettings,
    pub jwt: JwtSettings,
    pub matchmaking: MatchmakingSettings,
    pub server_status: ServerStatusSettings,
}

impl Settings {
    pub fn new() -> Result<Self, ConfigError> {
        let run_mode = std::env::var("RUN_MODE").unwrap_or_else(|_| "development".into());

        let s = Config::builder()
            .add_source(File::with_name(&format!("config/{}", run_mode)).required(true))
            .build()?;

        s.try_deserialize()
    }
}
</file>

<file path="src/events.rs">
use actix::{Actor, ActorContext, AsyncContext, Handler, Message, StreamHandler, WrapFuture};
use actix_web::{get, web, HttpRequest, HttpResponse, Result};
use actix_web_actors::ws;
use chrono::{DateTime, Utc};
use futures::stream::StreamExt;
use redis::aio::ConnectionManager;
use serde::{Deserialize, Serialize};
use tracing::{error, info, warn};
use uuid::Uuid;

use crate::AppState;

#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct EventStreamMessage {
    pub event_type: String,
    pub player_id: Option<Uuid>,
    pub timestamp: DateTime<Utc>,
    pub data: serde_json::Value,
}

#[derive(Deserialize, Debug, Clone)]
pub struct EventStreamQuery {
    pub player_id: Option<Uuid>,
    pub event_type: Option<String>,
}

// EventStream WebSocket Actor
pub struct EventStreamSession {
    pub filter: EventStreamQuery,
    pub redis_conn: ConnectionManager,
}

impl EventStreamSession {
    pub fn new(filter: EventStreamQuery, redis_conn: ConnectionManager) -> Self {
        Self { filter, redis_conn }
    }
}

impl Actor for EventStreamSession {
    type Context = ws::WebsocketContext<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        info!("EventStreamSession started with filter: {:?}", self.filter);

        // Start Redis subscription
        let addr = ctx.address();
        let mut _redis_conn = self.redis_conn.clone();
        let filter = self.filter.clone();

        ctx.spawn(
            async move {
                // Create a new connection for pubsub
                let client = redis::Client::open("redis://127.0.0.1:6379").unwrap();
                let mut pubsub = client.get_async_connection().await.unwrap().into_pubsub();

                // Subscribe to all notification channels
                if let Err(e) = pubsub.psubscribe("notifications:*").await {
                    error!("Failed to subscribe to Redis: {}", e);
                    return;
                }

                let mut stream = pubsub.on_message();

                while let Some(msg) = stream.next().await {
                    let channel: String = msg.get_channel_name().to_string();
                    let payload: String = msg.get_payload().unwrap_or_default();

                    // Extract player_id from channel name
                    let player_id = channel
                        .strip_prefix("notifications:")
                        .and_then(|id_str| Uuid::parse_str(id_str).ok());

                    // Apply filter
                    if let Some(filter_player_id) = filter.player_id {
                        if player_id != Some(filter_player_id) {
                            continue;
                        }
                    }

                    // Parse message payload
                    let event_data: serde_json::Value = serde_json::from_str(&payload)
                        .unwrap_or_else(|_| serde_json::json!({"raw": payload}));

                    let event_message = EventStreamMessage {
                        event_type: "server_message".to_string(),
                        player_id,
                        timestamp: Utc::now(),
                        data: event_data,
                    };

                    addr.do_send(ForwardEvent(event_message));
                }
            }
            .into_actor(self),
        );
    }
}

#[derive(Message)]
#[rtype(result = "()")]
struct ForwardEvent(EventStreamMessage);

impl Handler<ForwardEvent> for EventStreamSession {
    type Result = ();

    fn handle(&mut self, msg: ForwardEvent, ctx: &mut Self::Context) -> Self::Result {
        let event_json = serde_json::to_string(&msg.0).unwrap_or_else(|e| {
            error!("Failed to serialize event: {}", e);
            "{}".to_string()
        });

        ctx.text(event_json);
    }
}

impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for EventStreamSession {
    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
        match msg {
            Ok(ws::Message::Ping(msg)) => ctx.pong(&msg),
            Ok(ws::Message::Pong(_)) => {}
            Ok(ws::Message::Text(_text)) => {
                // Echo back for heartbeat
                ctx.text("pong");
            }
            Ok(ws::Message::Binary(_)) => {
                warn!("Received binary message in event stream");
            }
            Ok(ws::Message::Close(_)) => {
                info!("EventStreamSession closing");
                ctx.stop();
            }
            Ok(ws::Message::Continuation(_)) => {
                // Handle continuation frames
            }
            Ok(ws::Message::Nop) => {
                // Handle nop frames
            }
            Err(e) => {
                error!("EventStreamSession error: {}", e);
                ctx.stop();
            }
        }
    }
}

#[get("/events/stream")]
pub async fn event_stream_ws(
    req: HttpRequest,
    stream: web::Payload,
    query: web::Query<EventStreamQuery>,
    state: web::Data<AppState>,
) -> Result<HttpResponse> {
    let session = EventStreamSession::new(query.into_inner(), state.redis_conn_manager.clone());

    ws::start(session, &req, stream)
}
</file>

<file path="src/lib.rs">
use crate::{env::Settings, matchmaker::Matchmaker, pubsub::SubscriptionManager};
use actix::Addr;
use redis::aio::ConnectionManager;
use std::{io, sync::Arc};
use tracing_appender::rolling::{RollingFileAppender, Rotation};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt, EnvFilter};

pub mod auth;
pub mod debug;
pub mod env;
pub mod events;
pub mod matchmaker;
pub mod protocol;
pub mod provider;
pub mod pubsub;
pub mod util;
pub mod ws_session;

// RAII 패턴을 사용한 로거 매니저
pub struct LoggerManager {
    _guard: tracing_appender::non_blocking::WorkerGuard,
}

impl LoggerManager {
    pub fn setup(settings: &Settings) -> Self {
        // 1. 파일 로거 설정
        let file_appender = RollingFileAppender::new(
            Rotation::DAILY,
            &settings.logging.directory,
            &settings.logging.filename,
        );
        let (non_blocking_file_writer, guard) = tracing_appender::non_blocking(file_appender);

        // 2. 로그 레벨 필터 설정 (환경 변수 또는 설정 파일 값)
        let filter = EnvFilter::try_from_default_env()
            .unwrap_or_else(|_| EnvFilter::new(&settings.server.log_level));

        // 3. 콘솔 출력 레이어 설정
        let console_layer = fmt::layer()
            .with_writer(io::stdout) // 표준 출력으로 설정
            .with_ansi(true) // ANSI 색상 코드 사용 (터미널 지원 시)
            .with_thread_ids(true) // 스레드 ID 포함
            .with_thread_names(true) // 스레드 이름 포함
            .with_file(true) // 파일 경로 포함
            .with_line_number(true) // 라인 번호 포함
            .with_target(false) // target 정보 제외 (선택 사항)
            .pretty(); // 사람이 읽기 좋은 포맷

        // 4. 파일 출력 레이어 설정
        let file_layer = fmt::layer()
            .with_writer(non_blocking_file_writer) // Non-blocking 파일 로거 사용
            .with_ansi(false) // 파일에는 ANSI 코드 제외
            .with_thread_ids(true)
            .with_thread_names(true)
            .with_file(true)
            .with_line_number(true)
            .with_target(false)
            .pretty();

        // 5. 레지스트리(Registry)에 필터와 레이어 결합
        tracing_subscriber::registry()
            .with(filter) // 필터를 먼저 적용
            .with(console_layer) // 콘솔 레이어 추가
            .with(file_layer) // 파일 레이어 추가
            .init(); // 전역 Subscriber로 설정

        tracing::info!(
            "로거 초기화 완료: 콘솔 및 파일({}/{}) 출력 활성화.",
            settings.logging.directory,
            settings.logging.filename
        );

        Self { _guard: guard }
    }
}

// 서버 전체에서 공유될 상태
#[derive(Clone)]
pub struct AppState {
    pub settings: Settings,
    pub matchmaker_addr: Addr<Matchmaker>,
    pub sub_manager_addr: Addr<SubscriptionManager>,
    pub redis_conn_manager: ConnectionManager,
    pub _logger_manager: Arc<LoggerManager>, // RAII 패턴으로 메모리 관리
}
</file>

<file path="src/main.rs">
use actix::{Actor, System};
use actix_web::{get, web, App, Error, HttpRequest, HttpResponse, HttpServer};
use actix_web_actors::ws;
use actix_web_prom::PrometheusMetricsBuilder;
use match_server::{
    env::Settings,
    matchmaker::Matchmaker,
    provider::DedicatedServerProvider,
    pubsub::{RedisSubscriber, SubscriptionManager},
    ws_session::MatchmakingSession,
    AppState, LoggerManager,
};
use simulator_metrics::register_custom_metrics;
use std::{sync::Arc, time::Duration};
use tokio::sync::mpsc;
use tracing::{error, info};

#[get("/ws/")]
async fn matchmaking_ws_route(
    req: HttpRequest,
    stream: web::Payload,
    state: web::Data<AppState>,
) -> Result<HttpResponse, Error> {
    let session = MatchmakingSession::new(
        state.matchmaker_addr.clone(),
        state.sub_manager_addr.clone(),
        Duration::from_secs(state.settings.matchmaking.heartbeat_interval_seconds),
        Duration::from_secs(state.settings.matchmaking.client_timeout_seconds),
    );
    ws::start(session, &req, stream)
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    dotenv::dotenv().ok();
    let settings = Settings::new().expect("Failed to load settings.");
    
    // RAII 패턴으로 로거 매니저 생성
    let logger_manager = Arc::new(LoggerManager::setup(&settings));

    let prometheus = PrometheusMetricsBuilder::new("match_server")
        .endpoint("/metrics")
        .build()
        .expect("Failed to build Prometheus metrics.");

    register_custom_metrics(&prometheus.registry).expect("Failed to register custom metrics");

    let redis_client =
        redis::Client::open(settings.redis.url.clone()).expect("Failed to create Redis client");
    let redis_conn_manager = redis::aio::ConnectionManager::new(redis_client.clone())
        .await
        .expect("Failed to create Redis connection manager");
    info!("Redis connection manager created.");

    // --- Graceful Shutdown Channel ---
    let (shutdown_tx, mut shutdown_rx) = mpsc::channel::<()>(1);

    // --- Start New Pub/Sub Actors ---
    let sub_manager_addr = SubscriptionManager::new().start();
    info!("SubscriptionManager actor started.");

    RedisSubscriber::new(
        redis_client.clone(),
        sub_manager_addr.clone(),
        settings.clone(),
        shutdown_tx.clone(),
    )
    .start();
    info!("RedisSubscriber actor started.");
    // --- End of Start New Actors ---

    let provider_addr =
        DedicatedServerProvider::new(redis_conn_manager.clone(), settings.clone()).start();
    info!("DedicatedServerProvider actor started.");

    let matchmaker_addr = Matchmaker::new(
        redis_conn_manager.clone(),
        settings.matchmaking.clone(),
        provider_addr.clone(),
    )
    .start();
    info!("Matchmaker actor started.");

    let app_state = AppState {
        settings: settings.clone(),
        matchmaker_addr: matchmaker_addr.clone(),
        sub_manager_addr,
        redis_conn_manager: redis_conn_manager.clone(),
        _logger_manager: logger_manager, // RAII 패턴으로 메모리 관리
    };

    let bind_address = format!("{}:{}", settings.server.bind_address, settings.server.port);
    info!("Starting Actix-Web server on {}", bind_address);

    // --- 서버 시작 준비 ---
    let mut server = HttpServer::new(move || {
        App::new()
            .wrap(prometheus.clone())
            .app_data(web::Data::new(app_state.clone()))
            .service(matchmaking_ws_route)
            .service(match_server::events::event_stream_ws)
            // 디버그 엔드포인트들 추가
            .service(match_server::debug::debug_queue_status)
            .service(match_server::debug::debug_active_sessions)
            .service(match_server::debug::debug_loading_sessions)
            .service(match_server::debug::debug_redis_health)
            .service(match_server::debug::debug_ghost_detection)
            .service(match_server::debug::debug_matchmaker_state)
            .service(match_server::debug::debug_dashboard)
    })
    .bind(&bind_address)?
    .run();

    // 종료가 비정상적인지 여부를 추적하는 플래그
    let mut is_error_shutdown = false;

    // --- 종료 신호 대기 ---
    tokio::select! {
        // 서버가 자체적으로 종료된 경우 (예: 바인딩 실패 후 등 드문 경우)
        res = &mut server => {
            error!("Server exited unexpectedly on its own.");
            return res;
        },

        // RedisSubscriber로부터 비정상 종료 신호를 받은 경우
        _ = shutdown_rx.recv() => {
            error!("Critical error signal received. Initiating graceful shutdown...");
            is_error_shutdown = true; // 비정상 종료임을 표시
            // Actix 시스템 전체에 종료 신호를 보냄
            System::current().stop();
        },

        // 사용자가 Ctrl+C를 누른 경우
        _ = tokio::signal::ctrl_c() => {
            info!("Ctrl+C received. Initiating graceful shutdown...");
            // Actix 시스템 전체에 종료 신호를 보냄
            System::current().stop();
        }
    }

    // --- 실제 종료 대기 ---
    // `tokio::select!` 블록이 끝난 후, 즉 종료 신호가 감지된 후에
    // 서버(및 Actix 시스템)가 완전히 멈출 때까지 여기서 기다립니다.
    // 이 `await`는 모든 액터의 `stopping` 메서드가 완료된 후에 반환됩니다.
    info!("Waiting for all actors and connections to close...");
    server.await?;
    info!("System has shut down gracefully.");

    // 비정상 종료 신호로 시작된 경우, 모든 정리가 끝난 이 시점에서 프로세스를 종료합니다.
    if is_error_shutdown {
        info!("Exiting with error code to trigger K8s restart.");
        std::process::exit(1);
    }

    Ok(())
}
</file>

<file path="src/matchmaker/actor.rs">
use crate::provider::DedicatedServerProvider;
use actix::{Actor, Addr, AsyncContext, Context, Handler};
use redis::aio::ConnectionManager;
use serde_json::json;
use std::time::Duration;
use tracing::info;

use super::messages::{CheckStaleLoadingSessions, GetDebugInfo, TryMatch};

pub struct Matchmaker {
    pub(super) redis: ConnectionManager,
    pub(super) http_client: reqwest::Client,
    pub(super) settings: crate::env::MatchmakingSettings,
    pub(super) provider_addr: Addr<DedicatedServerProvider>,
}

impl Matchmaker {
    pub fn new(
        redis: ConnectionManager,
        settings: crate::env::MatchmakingSettings,
        provider_addr: Addr<DedicatedServerProvider>,
    ) -> Self {
        Self {
            redis,
            http_client: reqwest::Client::new(),
            settings,
            provider_addr,
        }
    }
}

impl Actor for Matchmaker {
    type Context = Context<Self>;
    fn started(&mut self, ctx: &mut Self::Context) {
        info!("Matchmaker actor started.");
        // 매칭 시도 타이머
        ctx.run_interval(
            Duration::from_secs(self.settings.tick_interval_seconds),
            |act, ctx| {
                for mode_settings in act.settings.game_modes.clone() {
                    ctx.address().do_send(TryMatch {
                        game_mode: mode_settings,
                    });
                }
            },
        );
        // 오래된 로딩 세션 정리 타이머
        ctx.run_interval(
            Duration::from_secs(self.settings.loading_session_timeout_seconds),
            |_act, ctx| {
                ctx.address().do_send(CheckStaleLoadingSessions);
            },
        );
    }
}

impl Handler<GetDebugInfo> for Matchmaker {
    type Result = String;

    fn handle(&mut self, _msg: GetDebugInfo, _ctx: &mut Context<Self>) -> Self::Result {
        let debug_info = json!({
            "matchmaker_status": "active",
            "settings": {
                "tick_interval_seconds": self.settings.tick_interval_seconds,
                "loading_session_timeout_seconds": self.settings.loading_session_timeout_seconds,
                "game_modes": self.settings.game_modes.iter().map(|mode| {
                    json!({
                        "id": mode.id,
                        "required_players": mode.required_players,
                        "use_mmr_matching": mode.use_mmr_matching
                    })
                }).collect::<Vec<_>>()
            },
            "internal_state": {
                "redis_connected": true,
                "http_client_ready": true,
                "provider_addr_available": true
            }
        });
        debug_info.to_string()
    }
}
</file>

<file path="src/matchmaker/handlers.rs">
use crate::{protocol::ServerMessage, provider::FindAvailableServer};
use actix::{AsyncContext, Handler, ResponseFuture};
use futures_util::stream::StreamExt;
use redis::aio::ConnectionManager;
use redis::{AsyncCommands, Script};
use serde::{Deserialize, Serialize};
use simulator_metrics::{
    HTTP_TIMEOUT_ERRORS_TOTAL, MATCHES_CREATED_TOTAL, MATCHMAKING_ERRORS_TOTAL, PLAYERS_IN_QUEUE,
    SYSTEM_TIME_ERRORS_TOTAL,
};
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use tracing::{error, info, warn};
use uuid::Uuid;

// test_client 작성해서 시나리오 테스트 해야함.

use super::{
    actor::Matchmaker,
    lock::DistributedLock, // DistributedLock 임포트
    messages::*,
    scripts::{
        get_atomic_cancel_session_script, get_atomic_loading_complete_script,
        get_atomic_match_script, get_cleanup_stale_session_script,
    },
};

const LOCK_DURATION_MS: usize = 30_000; // 30초

// --- Helper Functions ---
async fn publish_message(redis: &mut ConnectionManager, player_id: Uuid, message: ServerMessage) {
    let channel = format!("notifications:{}", player_id);
    let payload = match serde_json::to_string(&message) {
        Ok(p) => p,
        Err(e) => {
            warn!(
                "Failed to serialize ServerMessage for player {}: {}",
                player_id, e
            );
            return;
        }
    };
    if let Err(e) = redis.publish::<_, _, ()>(&channel, &payload).await {
        warn!("Failed to publish message to channel {}: {}", channel, e);
    }
}

async fn requeue_players(redis: &mut ConnectionManager, queue_key: &str, player_ids: &[String]) {
    warn!("Re-queuing players due to an error: {:?}", player_ids);
    if player_ids.is_empty() {
        return;
    }
    PLAYERS_IN_QUEUE.add(player_ids.len() as i64);
    let result: Result<i32, _> = redis.sadd(queue_key, player_ids).await;
    if let Err(e) = result {
        error!(
            "CRITICAL: Failed to re-queue players {:?} into {}: {}",
            player_ids, queue_key, e
        );
    }
}

// --- Message Handlers ---

/// EnqueuePlayer: 플레이어를 큐에 추가하는 메시지
impl Handler<EnqueuePlayer> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: EnqueuePlayer, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let settings = self.settings.clone();

        Box::pin(async move {
            // 게임 모드 유효성 검사
            let is_valid_game_mode = settings.game_modes.iter().any(|m| m.id == msg.game_mode);
            if !is_valid_game_mode {
                warn!(
                    "Player {} tried to enqueue for invalid game mode: {}",
                    msg.player_id, msg.game_mode
                );
                publish_message(
                    &mut redis,
                    msg.player_id,
                    ServerMessage::Error {
                        message: format!("Invalid game mode: {}", msg.game_mode),
                    },
                )
                .await;
                return;
            }

            let player_id_str = msg.player_id.to_string();
            let queue_key = format!("{}:{}", settings.queue_key_prefix, msg.game_mode);

            // Redis SADD는 원자적이므로 락 불필요
            let result: Result<i32, _> = redis.sadd(&queue_key, &player_id_str).await;
            match result {
                Ok(count) if count > 0 => {
                    info!("Player {} added to queue {}", player_id_str, queue_key);
                    PLAYERS_IN_QUEUE.inc();
                    publish_message(&mut redis, msg.player_id, ServerMessage::EnQueued).await;
                }
                Ok(_) => {
                    warn!("Player {} already in queue {}", player_id_str, queue_key);
                    publish_message(
                        &mut redis,
                        msg.player_id,
                        ServerMessage::Error {
                            message: "Already in queue".to_string(),
                        },
                    )
                    .await;
                }
                Err(e) => {
                    error!("Failed to add player to queue: {}", e);
                    publish_message(
                        &mut redis,
                        msg.player_id,
                        ServerMessage::Error {
                            message: "Internal server error".to_string(),
                        },
                    )
                    .await;
                }
            }
        })
    }
}

impl Handler<DequeuePlayer> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: DequeuePlayer, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();
        Box::pin(async move {
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);
            let player_id_str = msg.player_id.to_string();

            // Redis SREM은 원자적이므로 락 불필요
            let result: Result<i32, _> = redis.srem(&queue_key, &player_id_str).await;
            match result {
                Ok(count) if count > 0 => {
                    info!(
                        "Player {} (disconnected) removed from queue {}",
                        player_id_str, queue_key
                    );
                    PLAYERS_IN_QUEUE.dec();
                }
                Ok(_) => {
                    tracing::debug!(
                        "Player {} was not in queue {}, likely already matched.",
                        player_id_str,
                        queue_key
                    );
                }
                Err(e) => {
                    error!(
                        "Failed to remove player {} from queue {}: {}",
                        player_id_str, queue_key, e
                    );
                }
            }
        })
    }
}

impl Handler<TryMatch> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: TryMatch, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let game_mode_settings = msg.game_mode.clone();
        let settings = self.settings.clone();

        Box::pin(async move {
            let queue_key = format!("{}:{}", settings.queue_key_prefix, game_mode_settings.id);
            let required_players = game_mode_settings.required_players;
            let lock_key = format!("lock:match:{}", game_mode_settings.id);

            if game_mode_settings.use_mmr_matching {
                warn!(
                    "MMR-based matching for '{}' is not yet implemented. Falling back to simple matching.",
                    game_mode_settings.id
                );
            }

            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => return,
                Err(e) => {
                    error!(
                        "Failed to acquire lock for matching in {}: {}",
                        game_mode_settings.id, e
                    );
                    return;
                }
            };

            let current_timestamp = match SystemTime::now().duration_since(UNIX_EPOCH) {
                Ok(d) => d.as_secs().to_string(),
                Err(e) => {
                    SYSTEM_TIME_ERRORS_TOTAL.inc();
                    error!(
                        "System time is before UNIX EPOCH, cannot get timestamp: {}",
                        e
                    );
                    if let Err(e) = lock.release(&mut redis).await {
                        error!("Failed to release lock: {}", e);
                    }
                    return;
                }
            };

            let loading_session_id = Uuid::new_v4();
            let script = Script::new(get_atomic_match_script());
            let script_result: Vec<String> = match script
                .key(&queue_key)
                .arg(required_players)
                .arg(loading_session_id.to_string())
                .arg(current_timestamp)
                .arg(settings.loading_session_timeout_seconds)
                .invoke_async(&mut redis)
                .await
            {
                Ok(p) => p,
                Err(e) => {
                    error!("Matchmaking script failed for queue {}: {}", queue_key, e);
                    if let Err(e) = lock.release(&mut redis).await {
                        error!("Failed to release lock: {}", e);
                    }
                    return;
                }
            };

            if script_result.len() as u32 >= (required_players + 2) {
                let game_mode = script_result.get(0).cloned().unwrap_or_default();
                let returned_loading_session_id = script_result
                    .get(1)
                    .and_then(|s| Uuid::parse_str(s).ok())
                    .unwrap_or_else(|| {
                        error!("Could not parse loading session ID from script result.");
                        loading_session_id // Fallback to the one we generated
                    });
                let player_ids: Vec<String> = script_result[2..].to_vec();

                PLAYERS_IN_QUEUE.sub(required_players as i64);
                info!(
                    "[{}] Found a potential match with players: {:?} for session {}",
                    game_mode, player_ids, returned_loading_session_id
                );

                let message = ServerMessage::StartLoading {
                    loading_session_id: returned_loading_session_id,
                };
                for player_id_str in &player_ids {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        publish_message(&mut redis, player_id, message.clone()).await;
                    } else {
                        warn!(
                            "Could not parse player UUID from script result: {}",
                            player_id_str
                        );
                    }
                }
            }

            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for matching in {}: {}",
                    game_mode_settings.id, e
                );
            }
        })
    }
}

impl Handler<HandleLoadingComplete> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: HandleLoadingComplete, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let http_client = self.http_client.clone();
        let provider_addr = self.provider_addr.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let loading_key = format!("loading:{}", msg.loading_session_id);
            let player_id_str = msg.player_id.to_string();
            let lock_key = format!("lock:{}", loading_key);

            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => {
                    info!(
                        "Could not acquire lock for session {}, another process is handling it.",
                        msg.loading_session_id
                    );
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to acquire lock for session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            let script = Script::new(get_atomic_loading_complete_script());
            let result: Result<Vec<String>, _> = script
                .key(&loading_key)
                .arg(&player_id_str)
                .invoke_async(&mut redis)
                .await;

            let mut script_result: Vec<String> = match result {
                Ok(ids) if !ids.is_empty() => ids,
                Ok(_) => {
                    info!(
                        "Player {} is ready, but waiting for others in session {}.",
                        player_id_str, msg.loading_session_id
                    );
                    if let Err(e) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for session {}: {}",
                            msg.loading_session_id, e
                        );
                    }
                    return;
                }
                Err(e) => {
                    error!(
                        "Atomic loading script failed for session {}: {}",
                        msg.loading_session_id, e
                    );
                    if let Err(e) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for session {}: {}",
                            msg.loading_session_id, e
                        );
                    }
                    return;
                }
            };

            let game_mode = if !script_result.is_empty() {
                script_result.remove(0)
            } else {
                error!("Script result for loading complete is empty, cannot proceed.");
                if let Err(e) = lock.release(&mut redis).await {
                    error!("Failed to release lock: {}", e);
                }
                return;
            };
            let player_ids = script_result;

            info!(
                "All players {:?} are ready for session {}. Finding a dedicated server...",
                player_ids, msg.loading_session_id
            );

            let find_server_result = provider_addr.send(FindAvailableServer).await;

            match find_server_result {
                Ok(Ok(server_info)) => {
                    let create_session_url =
                        format!("http://{}/session/create", server_info.address);
                    #[derive(Serialize)]
                    struct CreateSessionReq {
                        players: Vec<Uuid>,
                    }
                    let req_body = CreateSessionReq {
                        players: player_ids
                            .iter()
                            .filter_map(|id| Uuid::parse_str(id).ok())
                            .collect(),
                    };

                    match http_client
                        .post(&create_session_url)
                        .json(&req_body)
                        .timeout(Duration::from_secs(5))
                        .send()
                        .await
                    {
                        Ok(resp) if resp.status().is_success() => {
                            #[derive(Deserialize, Debug)]
                            struct CreateSessionResp {
                                server_address: String,
                                session_id: Uuid,
                            }

                            match resp.json::<CreateSessionResp>().await {
                                Ok(session_info) => {
                                    info!(
                                        "[{}] Successfully created session: {:?}",
                                        game_mode, session_info
                                    );
                                    MATCHES_CREATED_TOTAL.inc();
                                    let message = ServerMessage::MatchFound {
                                        session_id: session_info.session_id,
                                        server_address: session_info.server_address.clone(),
                                    };
                                    for player_id_str in &player_ids {
                                        if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                                            publish_message(&mut redis, player_id, message.clone())
                                                .await;
                                        }
                                    }
                                }
                                Err(e) => {
                                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                                    MATCHMAKING_ERRORS_TOTAL
                                        .with_label_values(&["session_response_parse_failed"])
                                        .inc();
                                    error!("[{}] Failed to parse session creation response: {}. Re-queuing players.", game_mode, e);
                                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                                }
                            }
                        }
                        Ok(resp) => {
                            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                            MATCHMAKING_ERRORS_TOTAL
                                .with_label_values(&["dedicated_server_error_response"])
                                .inc();
                            error!(
                                "[{}] Dedicated server returned error: {}. Re-queuing players.",
                                game_mode,
                                resp.status()
                            );
                            requeue_players(&mut redis, &queue_key, &player_ids).await;
                        }
                        Err(e) => {
                            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                            if e.is_timeout() {
                                HTTP_TIMEOUT_ERRORS_TOTAL
                                    .with_label_values(&["dedicated_server"])
                                    .inc();
                            }
                            MATCHMAKING_ERRORS_TOTAL
                                .with_label_values(&["dedicated_server_request_failed"])
                                .inc();
                            error!(
                                "[{}] Failed to contact dedicated server (timeout or network error): {}. Re-queuing players.",
                                game_mode, e
                            );
                            requeue_players(&mut redis, &queue_key, &player_ids).await;
                        }
                    }
                }
                Ok(Err(e)) => {
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                    MATCHMAKING_ERRORS_TOTAL
                        .with_label_values(&["server_provider_failed"])
                        .inc();
                    error!(
                        "[{}] Failed to find available server: {}. Re-queuing players.",
                        game_mode, e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                }
                Err(e) => {
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                    MATCHMAKING_ERRORS_TOTAL
                        .with_label_values(&["server_provider_mailbox_error"])
                        .inc();
                    error!(
                        "[{}] Mailbox error when contacting provider: {}. Re-queuing players.",
                        game_mode, e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                }
            }

            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for session {}: {}",
                    msg.loading_session_id, e
                );
            }
        })
    }
}

impl Handler<CancelLoadingSession> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: CancelLoadingSession, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let loading_key = format!("loading:{}", msg.loading_session_id);
            let disconnected_player_id_str = msg.player_id.to_string();

            info!(
                "Attempting to cancel loading session {} due to player {} disconnection.",
                msg.loading_session_id, msg.player_id
            );

            let script = Script::new(get_atomic_cancel_session_script());
            let result: Result<Vec<String>, _> = script
                .key(&loading_key)
                .arg(&disconnected_player_id_str)
                .invoke_async(&mut redis)
                .await;

            let mut script_result = match result {
                Ok(val) if !val.is_empty() => val,
                Ok(_) => {
                    warn!(
                        "Loading session {} already handled or cleaned up before cancellation.",
                        msg.loading_session_id
                    );
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to run cancellation script for session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            let game_mode = if !script_result.is_empty() {
                script_result.remove(0)
            } else {
                error!("Script result for cancellation is empty, cannot proceed.");
                return;
            };
            let players_to_requeue = script_result;

            if !players_to_requeue.is_empty() {
                info!(
                    "Notifying remaining players {:?} and re-queuing them for game mode '{}'.",
                    players_to_requeue, game_mode
                );
                let queue_key = format!("{}:{}", queue_key_prefix, game_mode);

                let message = ServerMessage::Error {
                    message:
                        "A player disconnected during loading. You have been returned to the queue."
                            .to_string(),
                };

                for player_id_str in &players_to_requeue {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        publish_message(&mut redis, player_id, message.clone()).await;
                    }
                }
                requeue_players(&mut redis, &queue_key, &players_to_requeue).await;
            }
        })
    }
}
impl Handler<CheckStaleLoadingSessions> for Matchmaker {
    type Result = ResponseFuture<()>;

    fn handle(
        &mut self,
        _msg: CheckStaleLoadingSessions,
        _ctx: &mut Self::Context,
    ) -> Self::Result {
        let mut redis = self.redis.clone();
        let matchmaker_addr = _ctx.address();
        let settings = self.settings.clone();

        Box::pin(async move {
            info!("Checking for stale loading sessions...");

            let mut keys: Vec<String> = Vec::new();
            match redis.scan_match::<_, String>("loading:*").await {
                Ok(mut iter) => {
                    while let Some(key) = iter.next().await {
                        keys.push(key);
                    }
                }
                Err(e) => {
                    error!("Failed to scan loading sessions: {}", e);
                    return;
                }
            };

            for key in keys {
                let lock_key = format!("lock:{}", key);

                let lock =
                    match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await {
                        Ok(Some(lock)) => lock,
                        _ => continue,
                    };

                let now = match SystemTime::now().duration_since(UNIX_EPOCH) {
                    Ok(d) => d.as_secs(),
                    Err(e) => {
                        SYSTEM_TIME_ERRORS_TOTAL.inc();
                        error!(
                            "System time is before UNIX EPOCH, cannot check stale sessions: {}",
                            e
                        );
                        if let Err(e) = lock.release(&mut redis).await {
                            error!("Failed to release lock: {}", e);
                        }
                        continue;
                    }
                };

                let script = Script::new(get_cleanup_stale_session_script());
                let result: Result<Vec<String>, _> = script
                    .key(&key)
                    .arg(now as i64)
                    .arg(settings.loading_session_timeout_seconds as i64)
                    .invoke_async(&mut redis)
                    .await;

                let mut script_result = match result {
                    Ok(val) if !val.is_empty() => val,
                    Ok(_) => {
                        if let Err(e) = lock.release(&mut redis).await {
                            error!(
                                "Failed to release lock for stale check on key {}: {}",
                                key, e
                            );
                        }
                        continue;
                    }
                    Err(e) => {
                        error!(
                            "Failed to run stale session cleanup script for key {}: {}",
                            key, e
                        );
                        if let Err(e) = lock.release(&mut redis).await {
                            error!(
                                "Failed to release lock for stale check on key {}: {}",
                                key, e
                            );
                        }
                        continue;
                    }
                };

                let game_mode = if !script_result.is_empty() {
                    script_result.remove(0)
                } else {
                    error!("Script result for stale check is empty, cannot proceed.");
                    if let Err(e) = lock.release(&mut redis).await {
                        error!("Failed to release lock: {}", e);
                    }
                    continue;
                };
                let players_to_requeue = script_result;

                if !players_to_requeue.is_empty() {
                    warn!(
                        "Found stale loading session {}. Scheduling re-queuing for players {:?} for game mode '{}'.",
                        key, players_to_requeue, game_mode
                    );

                    let message = ServerMessage::Error {
                        message:
                            "Matchmaking timed out. You will be returned to the queue shortly."
                                .to_string(),
                    };
                    for player_id_str in &players_to_requeue {
                        if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                            publish_message(&mut redis, player_id, message.clone()).await;
                        }
                    }

                    matchmaker_addr.do_send(DelayedRequeuePlayers {
                        player_ids: players_to_requeue,
                        game_mode: game_mode,
                        delay: Duration::from_secs(5),
                    });
                }

                if let Err(e) = lock.release(&mut redis).await {
                    error!(
                        "Failed to release lock for stale check on key {}: {}",
                        key, e
                    );
                }
            }
        })
    }
}

impl Handler<DelayedRequeuePlayers> for Matchmaker {
    type Result = ResponseFuture<()>;

    fn handle(&mut self, msg: DelayedRequeuePlayers, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            info!(
                "Re-queuing players {:?} for game mode {} after delay.",
                msg.player_ids, msg.game_mode
            );
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);
            requeue_players(&mut redis, &queue_key, &msg.player_ids).await;
        })
    }
}
</file>

<file path="src/matchmaker/lock.rs">
use redis::aio::ConnectionManager;
use redis::{cmd, RedisResult, Script};
use uuid::Uuid;

const RELEASE_SCRIPT: &str = r#"
    if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
    else
        return 0
    end
"#;

/// Redis를 이용한 분산락 구조체.
/// Drop 트레이트를 구현하지 않았으므로, 사용 후 반드시 `release`를 명시적으로 호출해야 합니다.
pub struct DistributedLock {
    key: String,
    value: String,
}

impl DistributedLock {
    /// 분산락을 획득합니다.
    ///
    /// # Arguments
    /// * `redis` - Redis 커넥션 매니저
    /// * `key` - 락을 걸 대상 키
    /// * `duration_ms` - 락의 만료 시간 (밀리초)
    ///
    /// # Returns
    /// * `Ok(Some(lock))` - 락 획득 성공
    /// * `Ok(None)` - 다른 프로세스가 락을 이미 소유하고 있음
    /// * `Err(e)` - Redis 오류 발생
    pub async fn acquire(
        redis: &mut ConnectionManager,
        key: &str,
        duration_ms: usize,
    ) -> RedisResult<Option<Self>> {
        let value = Uuid::new_v4().to_string();

        // 'SET key value NX PX ms' 명령을 직접 구성합니다.
        let result: Option<String> = cmd("SET")
            .arg(key)
            .arg(&value)
            .arg("NX")
            .arg("PX")
            .arg(duration_ms)
            .query_async(redis) // ConnectionManager에서 비동기적으로 실행
            .await?;

        // 'SET NX'는 성공 시 "OK"를 반환하고, 키가 이미 존재하면 nil을 반환합니다.
        // `redis-rs`는 "OK"를 `Some("OK".to_string())`으로, nil을 `None`으로 변환합니다.
        if result.is_some() {
            Ok(Some(Self {
                key: key.to_string(),
                value,
            }))
        } else {
            Ok(None)
        }
    }

    /// 획득했던 분산락을 해제합니다.
    pub async fn release(&self, redis: &mut ConnectionManager) -> RedisResult<()> {
        let script = Script::new(RELEASE_SCRIPT);
        script
            .key(&self.key)
            .arg(&self.value)
            .invoke_async::<_, ()>(redis)
            .await?;
        Ok(())
    }
}
</file>

<file path="src/matchmaker/messages.rs">
use crate::env::GameModeSettings;
use actix::Message;
use std::time::Duration;
use uuid::Uuid;

#[derive(Message)]
#[rtype(result = "()")]
pub struct EnqueuePlayer {
    pub player_id: Uuid,
    pub game_mode: String,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct DequeuePlayer {
    pub player_id: Uuid,
    pub game_mode: String,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct HandleLoadingComplete {
    pub player_id: Uuid,
    pub loading_session_id: Uuid,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct CancelLoadingSession {
    pub player_id: Uuid,
    pub loading_session_id: Uuid,
}

#[derive(Message, Clone)]
#[rtype(result = "()")]
pub(super) struct TryMatch {
    pub(super) game_mode: GameModeSettings,
}

/// 오래된 로딩 세션을 정리하기 위한 내부 메시지입니다.
#[derive(Message)]
#[rtype(result = "()")]
pub(super) struct CheckStaleLoadingSessions;

#[derive(Message)]
#[rtype(result = "()")]
pub struct DelayedRequeuePlayers {
    pub player_ids: Vec<String>,
    pub game_mode: String,
    pub delay: Duration,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct RetryRequeuePlayers {
    pub player_ids: Vec<String>,
    pub game_mode: String,
    pub retry_count: u32,
}

#[derive(Message)]
#[rtype(result = "String")]
pub struct GetDebugInfo;
</file>

<file path="src/matchmaker/mod.rs">
pub mod actor;
mod handlers;
mod lock;
pub mod messages;
mod scripts;
pub use actor::Matchmaker;
</file>

<file path="src/matchmaker/scripts.rs">
use std::path::Path;
use std::fs;
use std::sync::OnceLock;

static ATOMIC_MATCH_SCRIPT: OnceLock<String> = OnceLock::new();
static ATOMIC_LOADING_COMPLETE_SCRIPT: OnceLock<String> = OnceLock::new();
static ATOMIC_CANCEL_SESSION_SCRIPT: OnceLock<String> = OnceLock::new();
static CLEANUP_STALE_SESSION_SCRIPT: OnceLock<String> = OnceLock::new();

fn load_script(filename: &str) -> String {
    let script_path = Path::new("scripts").join(filename);
    fs::read_to_string(&script_path)
        .unwrap_or_else(|e| {
            eprintln!("Failed to load script {}: {}", script_path.display(), e);
            String::new()
        })
}

pub(super) fn get_atomic_match_script() -> &'static str {
    ATOMIC_MATCH_SCRIPT.get_or_init(|| load_script("ATOMIC_MATCH_SCRIPT.lua"))
}

pub(super) fn get_atomic_loading_complete_script() -> &'static str {
    ATOMIC_LOADING_COMPLETE_SCRIPT.get_or_init(|| load_script("ATOMIC_LOADING_COMPLETE_SCRIPT.lua"))
}

pub(super) fn get_atomic_cancel_session_script() -> &'static str {
    ATOMIC_CANCEL_SESSION_SCRIPT.get_or_init(|| load_script("ATOMIC_CANCEL_SESSION_SCRIPT.lua"))
}

pub(super) fn get_cleanup_stale_session_script() -> &'static str {
    CLEANUP_STALE_SESSION_SCRIPT.get_or_init(|| load_script("CLEANUP_STALE_SESSION_SCRIPT.lua"))
}
</file>

<file path="src/protocol.rs">
use actix::prelude::*;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

// --- Client to Server Messages ---

#[derive(Deserialize, Message)]
#[rtype(result = "()")]
#[serde(tag = "type")]
pub enum ClientMessage {
    /// 플레이어가 매칭 대기열에 들어가기를 요청합니다.
    #[serde(rename = "enqueue")]
    Enqueue { player_id: Uuid, game_mode: String },
    /// 클라이언트가 에셋 로딩을 완료했음을 서버에 알립니다.
    #[serde(rename = "loading_complete")]
    LoadingComplete { loading_session_id: Uuid },
}

// --- Server to Client Messages ---

#[derive(Serialize, Deserialize, Message, Clone)]
#[rtype(result = "()")]
#[serde(tag = "type")]
pub enum ServerMessage {
    /// 대기열에 성공적으로 등록되었음을 알립니다.
    #[serde(rename = "enqueued")]
    EnQueued,

    /// 클라이언트에게 에셋 로딩을 시작하라고 지시합니다.
    #[serde(rename = "start_loading")]
    StartLoading { loading_session_id: Uuid },

    /// 최종적으로 매칭이 성사되었고, 게임 서버 접속 정보를 전달합니다.
    #[serde(rename = "match_found")]
    MatchFound {
        session_id: Uuid, // dedicated_server의 게임 세션 ID
        server_address: String,
    },

    /// 에러가 발생했음을 알립니다.
    #[serde(rename = "error")]
    Error { message: String },
}
</file>

<file path="src/provider/mod.rs">
use crate::env::Settings;
use actix::{Actor, Context, Handler, Message, ResponseFuture};
use futures_util::stream::StreamExt;
use redis::{aio::ConnectionManager, AsyncCommands};
use serde::Deserialize;
use serde_json::json;
use tracing::{info, warn};

// --- Actor Definition ---

/// 사용 가능한 Dedicated Server를 찾아 제공하는 책임을 가진 액터입니다.
pub struct DedicatedServerProvider {
    redis: ConnectionManager,
    settings: Settings,
}

impl DedicatedServerProvider {
    pub fn new(redis: ConnectionManager, settings: Settings) -> Self {
        Self { redis, settings }
    }
}

impl Actor for DedicatedServerProvider {
    type Context = Context<Self>;
}

// --- Message Definition ---

/// 사용 가능한 서버를 찾아달라는 메시지입니다.
#[derive(Message)]
#[rtype(result = "Result<ServerInfo, anyhow::Error>")]
pub struct FindAvailableServer;

/// 찾아낸 서버의 정보를 담는 구조체입니다.
#[derive(Deserialize, Debug, Clone)]
pub struct ServerInfo {
    pub address: String,
    pub status: String,
}

/// 서버 상태를 조회하는 메시지입니다.
#[derive(Message)]
#[rtype(result = "String")]
pub struct GetServerStatus;

// --- Message Handler ---

impl Handler<FindAvailableServer> for DedicatedServerProvider {
    type Result = ResponseFuture<Result<ServerInfo, anyhow::Error>>;

    /// `FindAvailableServer` 메시지를 처리합니다.
    fn handle(&mut self, _msg: FindAvailableServer, _ctx: &mut Context<Self>) -> Self::Result {
        let mut redis = self.redis.clone();
        let settings = self.settings.clone();

        Box::pin(async move {
            info!("Finding an available dedicated server from Redis...");

            // SCAN 사용으로 Redis 블로킹 방지
            let mut keys: Vec<String> = Vec::new();
            match redis
                .scan_match::<_, String>(&settings.redis.dedicated_server_key_pattern)
                .await
            {
                Ok(mut iter) => {
                    while let Some(key) = iter.next().await {
                        keys.push(key);
                    }
                }
                Err(e) => {
                    warn!("Failed to scan dedicated servers: {}", e);
                    return Err(anyhow::anyhow!("Failed to scan dedicated servers: {}", e));
                }
            };

            // 각 서버의 상태를 확인하여 "idle"인 서버를 찾습니다.
            for key in keys {
                let server_info_json: String = match redis.get(&key).await {
                    Ok(info) => info,
                    Err(e) => {
                        warn!(
                            "Failed to get server info for key {}: {}. Skipping.",
                            key, e
                        );
                        continue; // 다음 키로 넘어감
                    }
                };

                let server_info: ServerInfo = match serde_json::from_str(&server_info_json) {
                    Ok(info) => info,
                    Err(e) => {
                        warn!(
                            "Failed to parse server info for key {}: {}. Skipping.",
                            key, e
                        );
                        continue; // 다음 키로 넘어감
                    }
                };

                // "idle" 상태인 서버를 찾으면 즉시 반환합니다.
                if server_info.status == settings.server_status.idle {
                    info!("Found idle server: {:?}", server_info);
                    return Ok(server_info);
                }
            }

            // 모든 서버를 확인했지만 "idle" 상태인 서버가 없는 경우
            warn!("All dedicated servers are currently busy.");
            Err(anyhow::anyhow!("All dedicated servers are busy."))
        })
    }
}

impl Handler<GetServerStatus> for DedicatedServerProvider {
    type Result = String;

    fn handle(&mut self, _msg: GetServerStatus, _ctx: &mut Context<Self>) -> Self::Result {
        let status_info = json!({
            "provider_status": "active",
            "settings": {
                "dedicated_server_key_pattern": self.settings.redis.dedicated_server_key_pattern,
                "server_status": {
                    "idle": self.settings.server_status.idle
                }
            },
            "health": {
                "redis_connected": true,
                "can_scan_servers": true
            }
        });
        status_info.to_string()
    }
}
</file>

<file path="src/pubsub.rs">
use crate::{env::Settings, protocol::ServerMessage, ws_session::MatchmakingSession};
use actix::{
    Actor, Addr, AsyncContext, Context, ContextFutureSpawner, Handler, Message, WrapFuture,
};
use futures_util::stream::StreamExt;
use redis::Client as RedisClient;
use simulator_metrics::{APPLICATION_RESTARTS_TOTAL, REDIS_CONNECTION_FAILURES_TOTAL};
use std::collections::HashMap;
use std::time::Duration;
use tokio::sync::mpsc;
use tracing::{error, info, warn};
use uuid::Uuid;
use serde::Serialize;

// --- Messages for this module ---
#[derive(Message)]
#[rtype(result = "()")]
struct Connect;

// --- SubscriptionManager Actor ---

/// Manages the mapping between player_id and their WebSocket session actor address.
pub struct SubscriptionManager {
    sessions: HashMap<Uuid, Addr<MatchmakingSession>>,
}

impl SubscriptionManager {
    pub fn new() -> Self {
        Self {
            sessions: HashMap::new(),
        }
    }
}

impl Actor for SubscriptionManager {
    type Context = Context<Self>;
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct Register {
    pub player_id: Uuid,
    pub addr: Addr<MatchmakingSession>,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct Deregister {
    pub player_id: Uuid,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct ForwardMessage {
    pub player_id: Uuid,
    pub message: ServerMessage,
}

#[derive(Message)]
#[rtype(result = "Result<Vec<SessionInfo>, anyhow::Error>")]
pub struct GetActiveSessionsDebug;

#[derive(Serialize, Debug, Clone)]
pub struct SessionInfo {
    pub player_id: String,
    pub connected_at: String,
}

impl Handler<Register> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: Register, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Player {} registered for notifications.", msg.player_id);
        self.sessions.insert(msg.player_id, msg.addr);
    }
}

impl Handler<Deregister> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: Deregister, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Player {} deregistered.", msg.player_id);
        self.sessions.remove(&msg.player_id);
    }
}

impl Handler<ForwardMessage> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: ForwardMessage, _ctx: &mut Context<Self>) -> Self::Result {
        if let Some(recipient_addr) = self.sessions.get(&msg.player_id) {
            recipient_addr.do_send(msg.message);
        } else {
            warn!(
                "Could not find session for player {} to forward message.",
                msg.player_id
            );
        }
    }
}

impl Handler<GetActiveSessionsDebug> for SubscriptionManager {
    type Result = Result<Vec<SessionInfo>, anyhow::Error>;
    
    fn handle(&mut self, _msg: GetActiveSessionsDebug, _ctx: &mut Context<Self>) -> Self::Result {
        let sessions: Vec<SessionInfo> = self
            .sessions
            .keys()
            .map(|player_id| SessionInfo {
                player_id: player_id.to_string(),
                connected_at: chrono::Utc::now().to_rfc3339(), // 실제로는 연결 시간을 저장해야 함
            })
            .collect();
        
        Ok(sessions)
    }
}

// --- RedisSubscriber Actor ---

pub struct RedisSubscriber {
    redis_client: RedisClient,
    manager_addr: Addr<SubscriptionManager>,
    reconnect_attempts: u32,
    settings: Settings,
    shutdown_tx: mpsc::Sender<()>, // Shutdown channel sender
}

impl RedisSubscriber {
    pub fn new(
        redis_client: RedisClient,
        manager_addr: Addr<SubscriptionManager>,
        settings: Settings,
        shutdown_tx: mpsc::Sender<()>,
    ) -> Self {
        Self {
            redis_client,
            manager_addr,
            reconnect_attempts: 0,
            settings,
            shutdown_tx,
        }
    }

    fn connect_and_subscribe(&mut self, ctx: &mut Context<Self>) {
        info!("Attempting to connect and subscribe to Redis...");
        let client = self.redis_client.clone();
        let manager = self.manager_addr.clone();
        let self_addr = ctx.address();
        let current_reconnect_attempts = self.reconnect_attempts;
        let settings = self.settings.clone();
        let shutdown_tx = self.shutdown_tx.clone();

        async move {
            if current_reconnect_attempts >= settings.redis.max_reconnect_attempts {
                REDIS_CONNECTION_FAILURES_TOTAL
                    .with_label_values(&["pubsub"])
                    .inc();
                APPLICATION_RESTARTS_TOTAL.inc();
                error!(
                    "Max Redis reconnect attempts ({}) reached. Sending shutdown signal.",
                    settings.redis.max_reconnect_attempts
                );
                if shutdown_tx.send(()).await.is_err() {
                    error!("Failed to send shutdown signal. Forcing exit.");
                    std::process::exit(1); // Fallback
                }
                return;
            }

            let conn = match client.get_async_connection().await {
                Ok(c) => {
                    info!("Successfully connected to Redis.");
                    self_addr.do_send(ResetReconnectAttempts); // Reset on success
                    c
                }
                Err(e) => {
                    REDIS_CONNECTION_FAILURES_TOTAL
                        .with_label_values(&["pubsub"])
                        .inc();
                    error!("RedisSubscriber failed to get connection: {}", e);
                    self_addr.do_send(Connect); // Trigger reconnect
                    return;
                }
            };
            let mut pubsub = conn.into_pubsub();
            let channel_pattern = &settings.redis.notification_channel_pattern;
            if let Err(e) = pubsub.psubscribe(channel_pattern).await {
                REDIS_CONNECTION_FAILURES_TOTAL
                    .with_label_values(&["pubsub"])
                    .inc();
                error!("RedisSubscriber failed to psubscribe: {}", e);
                self_addr.do_send(Connect); // Trigger reconnect
                return;
            }
            info!("Successfully subscribed to '{}'", channel_pattern);

            let mut stream = pubsub.on_message();
            let prefix_to_strip = channel_pattern.trim_end_matches('*');

            while let Some(msg) = stream.next().await {
                let channel: String = msg.get_channel_name().to_string();
                let payload: String = match msg.get_payload() {
                    Ok(p) => p,
                    Err(_) => continue,
                };

                if let Some(player_id_str) = channel.strip_prefix(prefix_to_strip) {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        if let Ok(server_msg) = serde_json::from_str::<ServerMessage>(&payload) {
                            manager.do_send(ForwardMessage {
                                player_id,
                                message: server_msg,
                            });
                        }
                    }
                }
            }
            warn!("Redis Pub/Sub stream ended. Attempting to reconnect...");
            self_addr.do_send(Connect); // Trigger reconnect
        }
        .into_actor(self)
        .wait(ctx);
    }
}

impl Actor for RedisSubscriber {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Context<Self>) {
        info!("RedisSubscriber actor started.");
        self.connect_and_subscribe(ctx);
    }
}

#[derive(Message)]
#[rtype(result = "()")]
struct ResetReconnectAttempts;

impl Handler<ResetReconnectAttempts> for RedisSubscriber {
    type Result = ();
    fn handle(&mut self, _msg: ResetReconnectAttempts, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Redis connection successful. Resetting reconnect attempts.");
        self.reconnect_attempts = 0;
    }
}

impl Handler<Connect> for RedisSubscriber {
    type Result = ();

    fn handle(&mut self, _msg: Connect, ctx: &mut Context<Self>) -> Self::Result {
        self.reconnect_attempts += 1;
        let delay = Duration::from_millis(std::cmp::min(
            self.settings.redis.max_reconnect_delay_ms,
            self.settings.redis.initial_reconnect_delay_ms * (2u64.pow(self.reconnect_attempts - 1)),
        ));
        info!("Reconnect message received. Attempt: {}. Waiting for a delay of {:?} before next attempt.", self.reconnect_attempts, delay);
        ctx.run_later(delay, |act, ctx| {
            act.connect_and_subscribe(ctx);
        });
    }
}
</file>

<file path="src/util/mod.rs">

</file>

<file path="src/ws_session.rs">
use crate::{
    matchmaker::messages::{CancelLoadingSession, DequeuePlayer, EnqueuePlayer},
    protocol::{ClientMessage, ServerMessage},
    pubsub::{Deregister, Register},
    Matchmaker, SubscriptionManager,
};
use actix::{
    fut, Actor, ActorContext, ActorFutureExt, Addr, AsyncContext, Handler, Running, StreamHandler,
};
use actix_web_actors::ws;
use std::time::{Duration, Instant};
use tracing::{info, warn};
use uuid::Uuid;

/// Represents the state of the matchmaking session.
#[derive(Clone, Debug, PartialEq)]
enum SessionState {
    Idle,          // Initial state, no activity.
    Enqueuing,     // Enqueue request received, processing.
    InQueue,       // Successfully enqueued, waiting for a match.
    InLoading,     // Match found, loading assets.
    Disconnecting, // Session is gracefully shutting down.
}

pub struct MatchmakingSession {
    player_id: Option<Uuid>,
    game_mode: Option<String>,
    loading_session_id: Option<Uuid>,
    state: SessionState,
    hb: Instant,
    matchmaker_addr: Addr<Matchmaker>,
    sub_manager_addr: Addr<SubscriptionManager>,
    heartbeat_interval: Duration,
    client_timeout: Duration,
}

impl MatchmakingSession {
    pub fn new(
        matchmaker_addr: Addr<Matchmaker>,
        sub_manager_addr: Addr<SubscriptionManager>,
        heartbeat_interval: Duration,
        client_timeout: Duration,
    ) -> Self {
        Self {
            player_id: None,
            game_mode: None,
            loading_session_id: None,
            state: SessionState::Idle,
            hb: Instant::now(),
            matchmaker_addr,
            sub_manager_addr,
            heartbeat_interval,
            client_timeout,
        }
    }

    fn hb(&self, ctx: &mut ws::WebsocketContext<Self>) {
        ctx.run_interval(self.heartbeat_interval, |act, ctx| {
            if Instant::now().duration_since(act.hb) > act.client_timeout {
                info!("Websocket Client heartbeat failed, disconnecting!");
                ctx.stop();
                return;
            }
            ctx.ping(b"");
        });
    }
}

impl Actor for MatchmakingSession {
    type Context = ws::WebsocketContext<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        info!("MatchmakingSession started.");
        self.hb(ctx);
    }

    fn stopping(&mut self, ctx: &mut Self::Context) -> Running {
        if self.state == SessionState::Disconnecting {
            return Running::Stop;
        }
        self.state = SessionState::Disconnecting;

        if let Some(player_id) = self.player_id {
            info!(
                "Player {:?} disconnected. Starting graceful shutdown...",
                self.player_id
            );

            let sub_manager_addr_inner = self.sub_manager_addr.clone();
            let matchmaker_addr_inner = self.matchmaker_addr.clone();
            let loading_session_id = self.loading_session_id;
            let game_mode = self.game_mode.clone();

            let cleanup_future = async move {
                let deregister_fut = sub_manager_addr_inner.send(Deregister { player_id });

                match (loading_session_id, game_mode.clone()) {
                    (Some(loading_session_id), _) => {
                        let cancel_fut = matchmaker_addr_inner.send(CancelLoadingSession {
                            player_id,
                            loading_session_id,
                        });
                        _ = tokio::join!(deregister_fut, cancel_fut);
                    }
                    (None, Some(game_mode)) => {
                        let dequeue_fut = matchmaker_addr_inner.send(DequeuePlayer {
                            player_id,
                            game_mode,
                        });
                        _ = tokio::join!(deregister_fut, dequeue_fut);
                    }
                    _ => {
                        _ = deregister_fut.await;
                    }
                }
            };

            let graceful_shutdown =
                fut::wrap_future::<_, Self>(cleanup_future).then(|_result, _actor, ctx| {
                    info!(
                        "Cleanup finished for player {:?}. Stopping actor now.",
                        _actor.player_id
                    );
                    ctx.stop();
                    fut::ready(())
                });

            ctx.wait(graceful_shutdown);
            Running::Continue
        } else {
            info!("Anonymous session is stopping.");
            Running::Stop
        }
    }
}

impl Handler<ServerMessage> for MatchmakingSession {
    type Result = ();

    fn handle(&mut self, msg: ServerMessage, ctx: &mut Self::Context) {
        match msg {
            ServerMessage::EnQueued => {
                self.state = SessionState::InQueue;
            }
            ServerMessage::StartLoading { loading_session_id } => {
                self.state = SessionState::InLoading;
                self.loading_session_id = Some(loading_session_id);
            }
            _ => {}
        }

        match serde_json::to_string(&msg) {
            Ok(text) => ctx.text(text),
            Err(e) => warn!("Failed to serialize ServerMessage for client: {}", e),
        }
    }
}

impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for MatchmakingSession {
    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
        match msg {
            Ok(ws::Message::Ping(msg)) => {
                self.hb = Instant::now();
                ctx.pong(&msg);
            }
            Ok(ws::Message::Pong(_)) => {
                self.hb = Instant::now();
            }
            Ok(ws::Message::Text(text)) => match serde_json::from_str::<ClientMessage>(&text) {
                Ok(ClientMessage::Enqueue {
                    player_id,
                    game_mode,
                }) => {
                    if self.state != SessionState::Idle {
                        warn!(
                            "Player {:?} sent Enqueue request in non-idle state: {:?}. Ignoring.",
                            self.player_id.or(Some(player_id)),
                            self.state
                        );
                        return;
                    }

                    info!(
                        "Player {} requests queue for {}. Updating session state.",
                        player_id, game_mode
                    );

                    self.state = SessionState::Enqueuing;
                    self.player_id = Some(player_id);
                    self.game_mode = Some(game_mode.clone());

                    self.sub_manager_addr.do_send(Register {
                        player_id,
                        addr: ctx.address(),
                    });

                    self.matchmaker_addr.do_send(EnqueuePlayer {
                        player_id,
                        game_mode,
                    });
                }
                Ok(ClientMessage::LoadingComplete { loading_session_id }) => {
                    if self.state != SessionState::InLoading {
                        warn!(
                                "Received LoadingComplete from player {:?} not in loading state. Ignoring.",
                                self.player_id
                            );
                        return;
                    }
                    if let Some(player_id) = self.player_id {
                        info!(
                            "Player {} finished loading for session {}",
                            player_id, loading_session_id
                        );
                        self.matchmaker_addr.do_send(
                            crate::matchmaker::messages::HandleLoadingComplete {
                                player_id,
                                loading_session_id,
                            },
                        );
                    } else {
                        warn!("Received LoadingComplete from a session with no player_id.");
                    }
                }
                Err(e) => {
                    warn!("Failed to parse client message: {}", e);
                    match serde_json::to_string(&ServerMessage::Error {
                        message: "Invalid message format".to_string(),
                    }) {
                        Ok(text) => ctx.text(text),
                        Err(e) => warn!("Failed to serialize error message for client: {}", e),
                    }
                }
            },
            Ok(ws::Message::Close(reason)) => {
                ctx.close(reason);
                ctx.stop();
            }
            _ => ctx.stop(),
        }
    }
}
</file>

</files>
