This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/main.yml
.gitignore
Cargo.toml
config/development.toml
config/production.toml
review.md
src/auth.rs
src/env.rs
src/lib.rs
src/main.rs
src/matchmaker/actor.rs
src/matchmaker/handlers.rs
src/matchmaker/lock.rs
src/matchmaker/messages.rs
src/matchmaker/mod.rs
src/matchmaker/scripts.rs
src/protocol.rs
src/provider.rs
src/pubsub.rs
src/util/mod.rs
src/ws_session.rs
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/main.yml">
name: Code Coverage

on:
  pull_request:
    branches: [ main, master ]

jobs:
  codecov:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
          components: llvm-tools-preview
      
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov
      
      - name: Generate code coverage
        run: cargo llvm-cov --all-features --workspace --lcov --output-path lcov.info
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: lcov.info
</file>

<file path=".gitignore">
/target
</file>

<file path="Cargo.toml">
[package]
name = "match_server"
version = "0.1.0"
edition = "2021"
description = "card backend"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html



[dependencies]
backoff = "0.4"
tokio = { version = "1.15", features = ["full", "tracing"] }
tracing = "0.1.41"
tracing-appender = "0.2.3"
tracing-subscriber = { version = "0.3.19", features = ["env-filter"] }
actix-web = "4.9.0"
actix = "=0.13.5"
actix-ws = "0.2.5"
actix-web-actors = "4.3.0"
async-tungstenite = {version="0.28.2", features = ["tokio-runtime"]}
reqwest = { version = "0.12.12", features = ["json"] }
url = "2.5.4"
uuid = { version = "1.14.0", features = ["v4", "serde"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
rand_core = "0.6"
rand = "0.8.5"
futures-util = {version = "0.3", features = ["sink"]}
futures = "0.3.31"
ctor = "0.4.1"
redis = { version = "0.22.3", features = ["tokio-comp", "connection-manager"] }
thiserror-core = "1.0.50"
dotenv = "0.15.0"
thiserror = "2.0.12"
config = {version = "0.15.11", features = ["toml"]}
jsonwebtoken = "9.3.1"
anyhow = "1.0"
actix-web-prom = "0.10.0"
simulator_metrics = { path = "../simulator_metrics" }
chrono = { version = "0.4.41", features = ["serde"] }
hex = "0.4.3"
</file>

<file path="config/development.toml">
# 서버 설정
[server]
bind_address = "0.0.0.0"
port = 8080
log_level = "INFO"

# Redis 설정
[redis]
url = "redis://127.0.0.1:6379"

[jwt]
secret = "your-super-secret-and-long-key-that-no-one-knows"

# 매치메이킹 설정
[matchmaking]
tick_interval_seconds = 5
queue_key_prefix = "queue"

# 게임 모드 목록 (테이블 배열)
# 각 [[matchmaking.game_modes]] 항목이 하나의 게임 모드를 정의합니다.
[[matchmaking.game_modes]]
id = "Normal_1v1"          # 게임 모드의 고유 ID
required_players = 2       # 매칭에 필요한 인원
use_mmr_matching = false   # MMR 기반 매칭 사용 여부 (일반 모드)

# [[matchmaking.game_modes]]
# id = "Ranked_1v1"
# required_players = 2
# use_mmr_matching = true    # 랭크 모드에서는 이 값을 true로 설정
</file>

<file path="config/production.toml">
redis_url = "redis://your-production-redis-endpoint:6379"
log_level = "WARN"
password = ""
</file>

<file path="review.md">
네, 알겠습니다. 분산락을 적용한 후의 `matchmaker/handlers.rs` 코드를 중심으로 전체적인 핸들러 로직을 꼼꼼하게 검토하고 평가해 드리겠습니다.

### 종합 평가 (Overall Assessment)

**아주 큰 진전입니다.** 가장 치명적이었던 **"유령 플레이어 발생 (Race Condition)" 문제가 `HandleLoadingComplete`와 `CancelLoadingSession`에 분산락을 적용함으로써 성공적으로 해결되었습니다.** 이는 시스템 안정성 측면에서 매우 중요한 성과입니다. `TryMatch`와 `CheckStaleLoadingSessions`에 락을 적용한 것 또한 여러 서버 인스턴스가 동시에 실행될 때의 안정성을 높이는 좋은 결정입니다.

다만, 일부 핸들러에서는 분산락이 과도하게 적용되거나 불필요한 경우가 발견되었습니다. 이는 코드를 복잡하게 만들고 약간의 성능 저하를 일으킬 수 있습니다.

| 핸들러 (Handler)            | 목적                              | 락 적용 평가 (Lock Correctness)    | 권장 사항                                          |
| :-------------------------- | :-------------------------------- | :--------------------------------- | :------------------------------------------------- |
| `HandleLoadingComplete`     | 로딩 완료 처리, 매치 생성 시도    | ✅ **매우 좋음 (Excellent)**       | 현행 유지                                          |
| `CancelLoadingSession`      | 연결 끊김으로 인한 로딩 세션 취소 | ✅ **매우 좋음 (Excellent)**       | 현행 유지 (이전 대화에서 정리한 코드가 올바릅니다) |
| `TryMatch`                  | 주기적인 매칭 시도                | ✅ **좋음 (Good)**                 | 현행 유지                                          |
| `CheckStaleLoadingSessions` | 오래된 로딩 세션 정리             | ✅ **좋음 (Good)**                 | `KEYS`를 `SCAN`으로 시급히 교체해야 합니다.        |
| `EnqueuePlayer`             | 플레이어를 매칭 큐에 추가         | ⚠️ **불필요 (Unnecessary)**        | **락 제거 권장**                                   |
| `DequeuePlayer`             | 플레이어를 매칭 큐에서 제거       | ❌ **잘못된 사용 (Incorrect Use)** | **락 제거 권장**                                   |

---

### 상세 분석

#### 1. 잘 적용된 부분 (Excellent Use Cases)

**`Handler<HandleLoadingComplete>` 및 `Handler<CancelLoadingSession>`**

- **평가**: **완벽합니다.** 두 핸들러는 동일한 `loading:<session_id>` 리소스를 두고 경쟁하므로, `lock:loading:<session_id>` 라는 동일한 락 키를 사용하여 상호 배제(mutual exclusion)를 보장하는 것이 이 문제의 정석적인 해결책입니다. 어느 한쪽이 락을 획득하면 다른 쪽은 작업을 수행하지 않게 되어, 데이터 정합성이 100% 보장됩니다.
- **결과**: `new_issues.md`에서 지적된 **가장 치명적인 버그 #1이 해결되었습니다.**

**`Handler<TryMatch>`**

- **평가**: **좋은 사용 사례입니다.** 여러 서버 인스턴스가 매치메이킹을 수행할 때, 각 게임 모드(`lock:match:<game_mode>`)에 대해 하나의 인스턴스만 주기적인 매칭을 시도하도록 보장합니다. 이는 불필요한 Redis 부하를 줄이고, 여러 인스턴스가 동일한 플레이어 그룹을 동시에 매칭시키려는 미세한 경쟁 상태를 방지합니다.
- **결과**: 다중 서버 환경에서의 안정성과 효율성이 향상되었습니다.

**`Handler<CheckStaleLoadingSessions>`**

- **평가**: **논리적으로 올바릅니다.** 정리 작업이 현재 활발하게 처리 중인(`HandleLoadingComplete` 또는 `CancelLoadingSession`이 락을 획득한) 세션을 건드리지 않도록 보호하는 역할을 훌륭하게 수행합니다.
- **결과**: 시스템의 안정성이 더욱 향상되었습니다.

#### 2. 과도하거나 불필요한 부분 (Over-application and Unnecessary Use)

**`Handler<EnqueuePlayer>` 및 `Handler<DequeuePlayer>`**

- **문제점**: 이 두 핸들러에 적용된 락은 불필요하며 오히려 코드를 복잡하게 만듭니다.
- **이유**:

  1.  **Redis 명령어의 원자성(Atomicity)**: `Enqueue`에서 사용하는 `SADD` (Set Add)와 `Dequeue`에서 사용하는 `SREM` (Set Remove)은 Redis에서 **원자적으로 실행되는 명령어**입니다. 즉, 이 명령어 하나가 실행되는 동안 다른 명령어가 끼어들 수 없습니다. 여러 서버에서 동시에 `SADD`를 호출해도 데이터가 깨지지 않으며, 각 요청은 순차적으로 안전하게 처리됩니다.
  2.  **불필요한 복잡성**: 락을 획득하고 해제하는 코드가 추가되어 가독성을 해치고, 락 획득 실패 시의 분기 처리가 로직을 불필요하게 복잡하게 만듭니다.
  3.  **잘못된 사용 (`DequeuePlayer`)**: `DequeuePlayer`에서는 락 획득에 실패하면 경고를 로그로 남기고 "락 없이 그냥 진행"합니다. 이는 락의 목적 자체를 무의미하게 만듭니다. 만약 락이 정말 필요했다면 획득 실패 시 작업을 중단해야 하고, 필요 없다면 애초에 시도조차 하지 말아야 합니다.

- **권장 사항**: 이 두 핸들러에서는 **분산락 관련 코드를 모두 제거**하여 코드를 단순하고 명확하게 만드는 것이 좋습니다. Redis의 원자성만으로도 충분히 안전합니다.

---

### 남아있는 주요 문제점 (Remaining Critical Issues)

분산락이 잘 적용되었음에도 불구하고, `new_issues.md`에서 지적된 또 다른 심각한 문제가 아직 남아있습니다.

**`KEYS` 명령어 사용 문제 (버그 #2)**

- `CheckStaleLoadingSessions`와 `provider.rs`의 `FindAvailableServer` 핸들러에서 여전히 `KEYS` 명령어를 사용하고 있습니다.
- **위험성**: Redis의 키 개수가 많아지면 `KEYS "pattern:*"` 명령어는 **Redis 서버 전체를 몇 초간 멈추게 할 수 있는 매우 위험한 작업**입니다. 이는 운영 환경에서 심각한 장애로 이어질 수 있습니다.
- **해결책**: 반드시 커서(cursor) 기반의 반복자인 **`SCAN` 명령어로 교체**해야 합니다. `SCAN`은 전체 키를 한 번에 스캔하지 않고, 일부 키와 다음 반복을 위한 커서 값을 반환하므로 Redis 서버를 블로킹하지 않습니다.

### 수정 권장 사항

1.  **`EnqueuePlayer`와 `DequeuePlayer`에서 락 제거**

    ```rust
    // in src/matchmaker/handlers.rs

    // --- EnqueuePlayer 수정 ---
    impl Handler<EnqueuePlayer> for Matchmaker {
        type Result = ResponseFuture<()>;
        fn handle(&mut self, msg: EnqueuePlayer, _ctx: &mut Self::Context) -> Self::Result {
            let mut redis = self.redis.clone();
            let queue_key_prefix = self.settings.queue_key_prefix.clone();
            // ... (is_valid_game_mode 체크는 그대로 둡니다) ...

            Box::pin(async move {
                let player_id_str = msg.player_id.to_string();
                let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);

                // 락 없이 바로 SADD 실행
                let result: Result<i32, _> = redis.sadd(&queue_key, &player_id_str).await;
                match result {
                    Ok(count) if count > 0 => {
                        info!("Player {} added to queue {}", player_id_str, queue_key);
                        PLAYERS_IN_QUEUE.inc();
                        publish_message(&mut redis, msg.player_id, ServerMessage::Queued).await;
                    }
                    // ... (이하 에러 처리 로직은 동일) ...
                }
            })
        }
    }

    // --- DequeuePlayer 수정 ---
    impl Handler<DequeuePlayer> for Matchmaker {
        type Result = ResponseFuture<()>;
        fn handle(&mut self, msg: DequeuePlayer, _ctx: &mut Self::Context) -> Self::Result {
            let mut redis = self.redis.clone();
            let queue_key_prefix = self.settings.queue_key_prefix.clone();
            Box::pin(async move {
                let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);
                let player_id_str = msg.player_id.to_string();

                // 락 없이 바로 SREM 실행
                let result: Result<i32, _> = redis.srem(&queue_key, &player_id_str).await;
                match result {
                    Ok(count) if count > 0 => {
                        info!(
                            "Player {} (disconnected) removed from queue {}",
                            player_id_str, queue_key
                        );
                        PLAYERS_IN_QUEUE.dec();
                    }
                     // ... (이하 로직은 동일) ...
                }
            })
        }
    }
    ```

2.  **`KEYS`를 `SCAN`으로 교체 (필수)**

    예를 들어 `CheckStaleLoadingSessions`에서는 `redis.keys("loading:*")` 대신 다음과 같은 `SCAN` 루프를 사용해야 합니다.

    ```rust
    // 개념적 예시
    use redis::AsyncIter;
    // ...
    let mut iter: AsyncIter<String> = redis.scan_match("loading:*").await?;
    while let Some(key) = iter.next_item().await {
        // ... 각 key에 대한 락 획득 및 처리 로직 ...
    }
    ```

### 최종 결론

분산락을 성공적으로 도입하여 **가장 시급했던 경쟁 상태 문제를 해결하셨습니다.** 이제 다음 단계로, 불필요한 락을 제거하여 코드를 정리하고, **남아있는 치명적인 `KEYS` 명령어 문제를 `SCAN`으로 교체**하는 작업에 집중하시면 훨씬 더 안정적이고 효율적인 시스템을 완성하실 수 있을 것입니다. 수고 많으셨습니다
</file>

<file path="src/auth.rs">
use actix_web::{dev::Payload, web, FromRequest, HttpRequest};
use jsonwebtoken::{decode, DecodingKey, Validation};
use serde::{Deserialize, Serialize};
use std::future::{ready, Ready};

use crate::AppState;

// JWT Claims 구조체 (auth_server의 것과 동일해야 함)
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Claims {
    pub sub: String, // Subject (user's steam_id)
    pub iat: usize,
    pub exp: usize,
}

// 핸들러에서 인증된 사용자 정보를 담을 구조체
#[derive(Debug)]
pub struct AuthenticatedUser {
    pub steam_id: i64,
}

// Actix-Web 추출기(Extractor) 구현
impl FromRequest for AuthenticatedUser {
    type Error = actix_web::Error;
    type Future = Ready<Result<Self, Self::Error>>;

    fn from_request(req: &HttpRequest, _: &mut Payload) -> Self::Future {
        let app_state = req.app_data::<web::Data<AppState>>().unwrap();

        // 1. Authorization 헤더에서 토큰 추출
        let auth_header = match req.headers().get("Authorization") {
            Some(header) => header.to_str().unwrap_or(""),
            None => {
                return ready(Err(actix_web::error::ErrorUnauthorized(
                    "Missing Authorization header",
                )))
            }
        };

        if !auth_header.starts_with("Bearer ") {
            return ready(Err(actix_web::error::ErrorUnauthorized(
                "Invalid token format",
            )));
        }

        let token = &auth_header["Bearer ".len()..];

        // 2. JWT 디코딩 및 검증
        let token_data = match decode::<Claims>(
            token,
            &DecodingKey::from_secret(app_state.jwt_secret.as_ref()),
            &Validation::default(),
        ) {
            Ok(data) => data,
            Err(e) => {
                // 로그에 에러 기록
                tracing::warn!("JWT validation failed: {}", e);
                return ready(Err(actix_web::error::ErrorUnauthorized("Invalid token")));
            }
        };

        // 3. Claims에서 steam_id 파싱
        let steam_id = match token_data.claims.sub.parse::<i64>() {
            Ok(id) => id,
            Err(_) => {
                return ready(Err(actix_web::error::ErrorBadRequest(
                    "Invalid steam_id in token",
                )))
            }
        };

        // 4. 성공 시 AuthenticatedUser 반환
        ready(Ok(AuthenticatedUser { steam_id }))
    }
}
</file>

<file path="src/env.rs">
use config::{Config, ConfigError, File};
use serde::Deserialize;

#[derive(Debug, Deserialize, Clone)]
pub struct ServerSettings {
    pub bind_address: String,
    pub port: u16,
    pub log_level: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct RedisSettings {
    pub url: String,
}

#[derive(Debug, Deserialize, Clone)]
pub struct JwtSettings {
    pub secret: String,
}

/// TOML 설정 파일의 [[matchmaking.game_modes]] 테이블에 대응하는 구조체입니다.
#[derive(Debug, Deserialize, Clone)]
pub struct GameModeSettings {
    pub id: String,
    pub required_players: u32,
    pub use_mmr_matching: bool,
}

#[derive(Debug, Deserialize, Clone)]
pub struct MatchmakingSettings {
    pub tick_interval_seconds: u64,
    pub queue_key_prefix: String,
    pub game_modes: Vec<GameModeSettings>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Settings {
    pub server: ServerSettings,
    pub redis: RedisSettings,
    pub jwt: JwtSettings,
    pub matchmaking: MatchmakingSettings,
}

impl Settings {
    pub fn new() -> Result<Self, ConfigError> {
        let run_mode = std::env::var("RUN_MODE").unwrap_or_else(|_| "development".into());

        let s = Config::builder()
            .add_source(File::with_name(&format!("config/{}", run_mode)).required(true))
            .build()?;

        s.try_deserialize()
    }
}
</file>

<file path="src/lib.rs">
use crate::{matchmaker::Matchmaker, pubsub::SubscriptionManager};
use actix::Addr;

pub mod auth;
pub mod env;
pub mod matchmaker;
pub mod protocol;
pub mod provider;
pub mod pubsub;
pub mod util;
pub mod ws_session;

// 서버 전체에서 공유될 상태
#[derive(Clone)]
pub struct AppState {
    pub jwt_secret: String,
    // redis_client는 이제 pubsub 액터에서만 사용되므로 AppState에서 제거
    // pub redis_client: RedisClient,
    pub matchmaker_addr: Addr<Matchmaker>,
    // provider_addr는 Matchmaker가 내부적으로 소유하므로 AppState에서 제거
    // pub provider_addr: Addr<DedicatedServerProvider>,
    pub sub_manager_addr: Addr<SubscriptionManager>,
}

use std::{io, sync::Once};
use tracing_appender::rolling::{RollingFileAppender, Rotation};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt, EnvFilter};
static INIT: Once = Once::new();
static mut GUARD: Option<tracing_appender::non_blocking::WorkerGuard> = None;
pub fn setup_logger() {
    INIT.call_once(|| {
        // 1. 파일 로거 설정
        let file_appender = RollingFileAppender::new(Rotation::DAILY, "logs", "app.log");
        let (non_blocking_file_writer, _guard) = tracing_appender::non_blocking(file_appender);

        // 2. 로그 레벨 필터 설정 (환경 변수 또는 기본값 INFO)
        let filter = EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("info")); // 기본 INFO 레벨

        // 3. 콘솔 출력 레이어 설정
        let console_layer = fmt::layer()
            .with_writer(io::stdout) // 표준 출력으로 설정
            .with_ansi(true) // ANSI 색상 코드 사용 (터미널 지원 시)
            .with_thread_ids(true) // 스레드 ID 포함
            .with_thread_names(true) // 스레드 이름 포함
            .with_file(true) // 파일 경로 포함
            .with_line_number(true) // 라인 번호 포함
            .with_target(false) // target 정보 제외 (선택 사항)
            .pretty(); // 사람이 읽기 좋은 포맷

        // 4. 파일 출력 레이어 설정
        let file_layer = fmt::layer()
            .with_writer(non_blocking_file_writer) // Non-blocking 파일 로거 사용
            .with_ansi(false) // 파일에는 ANSI 코드 제외
            .with_thread_ids(true)
            .with_thread_names(true)
            .with_file(true)
            .with_line_number(true)
            .with_target(false)
            .pretty();

        // 5. 레지스트리(Registry)에 필터와 레이어 결합
        tracing_subscriber::registry()
            .with(filter) // 필터를 먼저 적용
            .with(console_layer) // 콘솔 레이어 추가
            .with(file_layer) // 파일 레이어 추가
            .init(); // 전역 Subscriber로 설정

        unsafe {
            GUARD = Some(_guard);
        }

        tracing::info!("로거 초기화 완료: 콘솔 및 파일(logs/app.log) 출력 활성화.");
    });
}
</file>

<file path="src/main.rs">
use actix::Actor;
use actix_web::{get, web, App, Error, HttpRequest, HttpResponse, HttpServer};
use actix_web_actors::ws;
use actix_web_prom::PrometheusMetricsBuilder;
use match_server::{
    env::Settings,
    matchmaker::Matchmaker,
    provider::DedicatedServerProvider,
    pubsub::{RedisSubscriber, SubscriptionManager},
    setup_logger,
    ws_session::MatchmakingSession,
    AppState,
};
use simulator_metrics::register_custom_metrics;
use tracing::info;

#[get("/ws/")]
async fn matchmaking_ws_route(
    req: HttpRequest,
    stream: web::Payload,
    state: web::Data<AppState>,
) -> Result<HttpResponse, Error> {
    let session =
        MatchmakingSession::new(state.matchmaker_addr.clone(), state.sub_manager_addr.clone());
    ws::start(session, &req, stream)
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    dotenv::dotenv().ok();
    let settings = Settings::new().expect("Failed to load settings.");
    setup_logger();

    let prometheus = PrometheusMetricsBuilder::new("match_server")
        .endpoint("/metrics")
        .build()
        .unwrap();

    register_custom_metrics(&prometheus.registry).expect("Failed to register custom metrics");

    let redis_client =
        redis::Client::open(settings.redis.url.clone()).expect("Failed to create Redis client");
    let redis_conn_manager = redis::aio::ConnectionManager::new(redis_client.clone())
        .await
        .expect("Failed to create Redis connection manager");
    info!("Redis connection manager created.");

    // --- Start New Pub/Sub Actors ---
    let sub_manager_addr = SubscriptionManager::new().start();
    info!("SubscriptionManager actor started.");

    RedisSubscriber::new(redis_client.clone(), sub_manager_addr.clone()).start();
    info!("RedisSubscriber actor started.");
    // --- End of Start New Actors ---

    let provider_addr = DedicatedServerProvider::new(redis_conn_manager.clone()).start();
    info!("DedicatedServerProvider actor started.");

    let matchmaker_addr = Matchmaker::new(
        redis_conn_manager,
        settings.matchmaking.clone(),
        provider_addr.clone(),
    )
    .start();
    info!("Matchmaker actor started.");

    let app_state = AppState {
        jwt_secret: settings.jwt.secret.clone(),
        matchmaker_addr: matchmaker_addr.clone(),
        sub_manager_addr,
    };

    let bind_address = format!("{}:{}", settings.server.bind_address, settings.server.port);
    info!("Starting Actix-Web server on {}", bind_address);

    HttpServer::new(move || {
        App::new()
            .wrap(prometheus.clone())
            .app_data(web::Data::new(app_state.clone()))
            .service(matchmaking_ws_route)
    })
    .bind(&bind_address)?
    .run()
    .await
}
</file>

<file path="src/matchmaker/actor.rs">
use crate::provider::DedicatedServerProvider;
use actix::{Actor, Addr, AsyncContext, Context};
use redis::aio::ConnectionManager;
use std::time::Duration;
use tracing::info;

use super::messages::{CheckStaleLoadingSessions, TryMatch};

pub(super) const LOADING_SESSION_TIMEOUT_SECONDS: u64 = 60;

// --- Actor Definition ---
pub struct Matchmaker {
    pub(super) redis: ConnectionManager,
    pub(super) http_client: reqwest::Client,
    pub(super) settings: crate::env::MatchmakingSettings,
    pub(super) provider_addr: Addr<DedicatedServerProvider>,
}

impl Matchmaker {
    pub fn new(
        redis: ConnectionManager,
        settings: crate::env::MatchmakingSettings,
        provider_addr: Addr<DedicatedServerProvider>,
    ) -> Self {
        Self {
            redis,
            http_client: reqwest::Client::new(),
            settings,
            provider_addr,
        }
    }
}

impl Actor for Matchmaker {
    type Context = Context<Self>;
    fn started(&mut self, ctx: &mut Self::Context) {
        info!("Matchmaker actor started.");
        // 매칭 시도 타이머
        ctx.run_interval(
            Duration::from_secs(self.settings.tick_interval_seconds),
            |act, ctx| {
                for mode_settings in act.settings.game_modes.clone() {
                    ctx.address().do_send(TryMatch {
                        game_mode: mode_settings,
                    });
                }
            },
        );
        // 오래된 로딩 세션 정리 타이머
        ctx.run_interval(
            Duration::from_secs(LOADING_SESSION_TIMEOUT_SECONDS),
            |_act, ctx| {
                ctx.address().do_send(CheckStaleLoadingSessions);
            },
        );
    }
}
</file>

<file path="src/matchmaker/handlers.rs">
use crate::{protocol::ServerMessage, provider::FindAvailableServer};
use actix::{Handler, ResponseFuture};
use futures_util::stream::StreamExt;
use redis::aio::ConnectionManager;
use redis::{AsyncCommands, RedisResult, Script};
use serde::{Deserialize, Serialize};
use simulator_metrics::{MATCHES_CREATED_TOTAL, PLAYERS_IN_QUEUE};
use std::collections::HashMap;
use std::time::{SystemTime, UNIX_EPOCH};
use tracing::{error, info, warn};
use uuid::Uuid;

/*
  1. 아키텍처 및 액터(Actor)의 역할
   * Matchmaker 액터의 가장 핵심적인 책임은 무엇이며, DedicatedServerProvider 액터와는 어떤
     관계를 맺고 있나요? 두 액터가 서로 통신하는 주된 이유는 무엇입니까?
answer:
    Matchmaker 액터는 started() 에서 run_interval 를 통해 주기적으로 TryMatch 메시지와 CheckStaleLoadingSessions 메시지를 처리합니다.
    이는 매칭 요청을 처리하고 오래된 로딩 세션을 정리하는 책임을 가집니다.
    매칭 성공 시, dedicated server를 찾아서 게임을 생성하는데, 이 때 dedicated server를 찾기 위해 DedicatedServerProvider 액터와 통신합니다.

  2. 매치메이킹 핵심 로직
   * 플레이어가 매칭을 요청(EnqueuePlayer)했을 때부터, 매치가 성사되어 로딩을 시작하라는
     메시지(StartLoading)를 받기까지의 과정을 단계별로 설명해주세요. 이 과정에서 어떤 액터와
     Redis 키(key)가 관련되나요?
answer:
    1. 먼저 게임 모드 유효성 검사를 실시합니다. 만약 유효하지 않다면 publish_message 함수를 통해 구독자에게 ServerMessage::Error 메시지를 발행합니다.
    2. redis.sadd(&queue_key, &player_id_str).await; 함수를 통해 redis 에 원자적으로 k:queue, v:player_id_str 데이터 쌍을 추가합니다.
    2-1. 만약 성공 한다면 ( Ok(count) )
        - 플레이어가 큐에 추가되었다는 로그를 남기고, PLAYERS_IN_QUEUE 메트릭을 증가시킵니다.
        - publish_message 함수를 통해 구독자에게 ServerMessage::Queued 메시지를 발행합니다.
    2-2. 만약 실패한다면 ( Ok(_) )
        - 플레이어가 이미 큐에 존재한다는 경고 로그를 남기고, publish_message 함수를 통해 구독자에게 ServerMessage::Error 메시지를 발행합니다.
    2-3. 만약 redis.sadd 함수가 실패한다면 ( Err(e) )
        - 에러 로그를 남기고, publish_message 함수를 통해 구독자에게 ServerMessage::Error 메시지를 발행합니다.
    3. MatchMaker Actor 에서 주기적으로 수행되는 TryMatch 메시지의 Handler 가 수행됩니다.
    4. lock 을 획득 후 ATOMIC_MATCH_SCRIPT 를 실행하여 redis 에서 매칭 대기중인 player_id 를 가져옵니다.
    5.

  3. 상태 관리와 Redis
   * 이 시스템에서 Redis는 여러 가지 중요한 상태를 관리합니다. '플레이어 대기열', '로딩 중인
     세션', '전용 서버 목록' 이 세 가지를 관리하기 위해 각각 어떤 Redis 자료구조(Data
     Structure)와 키(Key) 패턴이 사용되고 있나요?

  4. 분산 환경 및 동시성 제어
   * match_server가 여러 인스턴스로 실행될 수 있는 분산 환경을 가정해 보겠습니다. 두 개의 다른
     서버 인스턴스가 정확히 같은 시간에 동일한 게임 모드의 매칭을 시도할 때, 플레이어들이
     중복으로 매칭되는 문제를 방지하기 위해 어떤 장치가 마련되어 있나요?

  5. 오류 처리 및 복원력
   * 플레이어 그룹이 성공적으로 매칭되어 로딩 상태에 들어갔지만, 한 명의 플레이어가 로딩을
     완료하지 못하고 타임아웃이 발생했다고 가정해봅시다.
       * 시스템은 이 "오래된(stale)" 로딩 세션을 어떻게 감지하고 정리하나요?
       * 남아있는 다른 플레이어들은 어떻게 처리되나요?
*/

use super::{
    actor::{Matchmaker, LOADING_SESSION_TIMEOUT_SECONDS},
    lock::DistributedLock, // DistributedLock 임포트
    messages::*,
    scripts::{ATOMIC_LOADING_COMPLETE_SCRIPT, ATOMIC_MATCH_SCRIPT},
};

const LOCK_DURATION_MS: usize = 10_000; // 10초

// --- Helper Functions ---
async fn publish_message(redis: &mut ConnectionManager, player_id: Uuid, message: ServerMessage) {
    let channel = format!("notifications:{}", player_id);
    let payload = serde_json::to_string(&message).unwrap();
    if let Err(e) = redis.publish::<_, _, ()>(&channel, &payload).await {
        warn!("Failed to publish message to channel {}: {}", channel, e);
    }
}

async fn requeue_players(redis: &mut ConnectionManager, queue_key: &str, player_ids: &[String]) {
    warn!("Re-queuing players due to an error: {:?}", player_ids);
    if player_ids.is_empty() {
        return;
    }
    PLAYERS_IN_QUEUE.add(player_ids.len() as i64);
    let result: Result<i32, _> = redis.sadd(queue_key, player_ids).await;
    if let Err(e) = result {
        error!(
            "CRITICAL: Failed to re-queue players {:?} into {}: {}",
            player_ids, queue_key, e
        );
    }
}

// --- Message Handlers ---

/// EnqueuePlayer: 플레이어를 큐에 추가하는 메시지
impl Handler<EnqueuePlayer> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: EnqueuePlayer, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        // 게임 모드 유효성 검사
        let is_valid_game_mode = self
            .settings
            .game_modes
            .iter()
            .any(|m| m.id == msg.game_mode);
        if !is_valid_game_mode {
            let player_id = msg.player_id;
            return Box::pin(async move {
                warn!(
                    "Player {} tried to enqueue for invalid game mode: {}",
                    player_id, msg.game_mode
                );
                publish_message(
                    &mut redis,
                    player_id,
                    ServerMessage::Error {
                        message: format!("Invalid game mode: {}", msg.game_mode),
                    },
                )
                .await;
            });
        }

        Box::pin(async move {
            let player_id_str = msg.player_id.to_string();
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);

            // Redis SADD는 원자적이므로 락 불필요
            let result: Result<i32, _> = redis.sadd(&queue_key, &player_id_str).await;
            match result {
                Ok(count) if count > 0 => {
                    info!("Player {} added to queue {}", player_id_str, queue_key);
                    PLAYERS_IN_QUEUE.inc();
                    publish_message(&mut redis, msg.player_id, ServerMessage::Queued).await;
                }
                Ok(_) => {
                    warn!("Player {} already in queue {}", player_id_str, queue_key);
                    publish_message(
                        &mut redis,
                        msg.player_id,
                        ServerMessage::Error {
                            message: "Already in queue".to_string(),
                        },
                    )
                    .await;
                }
                Err(e) => {
                    error!("Failed to add player to queue: {}", e);
                    publish_message(
                        &mut redis,
                        msg.player_id,
                        ServerMessage::Error {
                            message: "Internal server error".to_string(),
                        },
                    )
                    .await;
                }
            }
        })
    }
}

impl Handler<DequeuePlayer> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: DequeuePlayer, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();
        Box::pin(async move {
            let queue_key = format!("{}:{}", queue_key_prefix, msg.game_mode);
            let player_id_str = msg.player_id.to_string();

            // Redis SREM은 원자적이므로 락 불필요
            let result: Result<i32, _> = redis.srem(&queue_key, &player_id_str).await;
            match result {
                Ok(count) if count > 0 => {
                    info!(
                        "Player {} (disconnected) removed from queue {}",
                        player_id_str, queue_key
                    );
                    PLAYERS_IN_QUEUE.dec();
                }
                Ok(_) => {
                    tracing::debug!(
                        "Player {} was not in queue {}, likely already matched.",
                        player_id_str,
                        queue_key
                    );
                }
                Err(e) => {
                    error!(
                        "Failed to remove player {} from queue {}: {}",
                        player_id_str, queue_key, e
                    );
                }
            }
        })
    }
}

impl Handler<TryMatch> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: TryMatch, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let game_mode_settings = msg.game_mode;
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let queue_key = format!("{}:{}", queue_key_prefix, game_mode_settings.id);
            // 게임에 필요한 플레이어 수 입니다.
            let required_players = game_mode_settings.required_players;
            let lock_key = format!("lock:match:{}", game_mode_settings.id);

            if game_mode_settings.use_mmr_matching {
                warn!(
                    "MMR-based matching for '{}' is not yet implemented. Falling back to simple matching.",
                    game_mode_settings.id
                );
            }

            // --- 분산락 획득 ---
            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => {
                    // 다른 서버가 이미 매칭 처리 중이므로 건너뛰기
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to acquire lock for matching in {}: {}",
                        game_mode_settings.id, e
                    );
                    return;
                }
            };

            // 필요한 플레이어 수 만큼 redis 에서 player id 를 가져옵니다.
            let script = Script::new(ATOMIC_MATCH_SCRIPT);
            let player_ids: Vec<String> = match script
                .key(&queue_key)
                .arg(required_players)
                .invoke_async(&mut redis)
                .await
            {
                Ok(p) => p,
                Err(e) => {
                    error!("Matchmaking script failed for queue {}: {}", queue_key, e);
                    // 락 해제
                    if let Err(lock_err) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for matching in {}: {}",
                            game_mode_settings.id, lock_err
                        );
                    }
                    return;
                }
            };

            if player_ids.len() as u32 == required_players {
                PLAYERS_IN_QUEUE.sub(required_players as i64);
                info!(
                    "[{}] Found a potential match with players: {:?}",
                    game_mode_settings.id, player_ids
                );

                let loading_session_id = Uuid::new_v4();
                let loading_key = format!("loading:{}", loading_session_id);

                let mut players_map = HashMap::new();
                for player_id in &player_ids {
                    players_map.insert(player_id.clone(), "loading".to_string());
                }
                players_map.insert("game_mode".to_string(), game_mode_settings.id.clone());
                let current_timestamp = SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .unwrap()
                    .as_secs()
                    .to_string();
                players_map.insert("created_at".to_string(), current_timestamp);
                players_map.insert("status".to_string(), "loading".to_string());

                let players_map_slice: Vec<_> = players_map.iter().collect();
                if let Err(e) = redis
                    .hset_multiple::<_, _, _, ()>(&loading_key, &players_map_slice)
                    .await
                {
                    error!(
                        "Failed to create loading session in Redis: {}. Re-queuing players.",
                        e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                    // 락 해제
                    if let Err(lock_err) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for matching in {}: {}",
                            game_mode_settings.id, lock_err
                        );
                    }
                    return;
                }

                info!(
                    "[{}] Notifying players to start loading for session {}",
                    game_mode_settings.id, loading_session_id
                );
                let message = ServerMessage::StartLoading { loading_session_id };
                for player_id_str in &player_ids {
                    let player_id = Uuid::parse_str(player_id_str).unwrap();
                    publish_message(&mut redis, player_id, message.clone()).await;
                }
            }

            // --- 분산락 해제 ---
            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for matching in {}: {}",
                    game_mode_settings.id, e
                );
            }
        })
    }
}

impl Handler<HandleLoadingComplete> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: HandleLoadingComplete, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let http_client = self.http_client.clone();
        let provider_addr = self.provider_addr.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let loading_key = format!("loading:{}", msg.loading_session_id);
            let player_id_str = msg.player_id.to_string();
            let lock_key = format!("lock:{}", loading_key);

            // --- 분산락 획득 ---
            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => {
                    info!(
                        "Could not acquire lock for session {}, another process is handling it.",
                        msg.loading_session_id
                    );
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to acquire lock for session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            let script = Script::new(ATOMIC_LOADING_COMPLETE_SCRIPT);
            let result: Result<Vec<String>, _> = script
                .key(&loading_key)
                .arg(&player_id_str)
                .invoke_async(&mut redis)
                .await;

            // --- 분산락 해제 ---
            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for session {}: {}",
                    msg.loading_session_id, e
                );
            }

            let mut script_result: Vec<String> = match result {
                Ok(ids) if !ids.is_empty() => ids,
                Ok(_) => {
                    info!(
                        "Player {} is ready, but waiting for others in session {}.",
                        player_id_str, msg.loading_session_id
                    );
                    return;
                }
                Err(e) => {
                    error!(
                        "Atomic loading script failed for session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            let game_mode = script_result.remove(0);
            let player_ids = script_result;

            info!(
                "All players {:?} are ready for session {}. Finding a dedicated server...",
                player_ids, msg.loading_session_id
            );

            match provider_addr.send(FindAvailableServer).await {
                Ok(Ok(server_info)) => {
                    let create_session_url =
                        format!("http://{}/session/create", server_info.address);
                    #[derive(Serialize)]
                    struct CreateSessionReq {
                        players: Vec<Uuid>,
                    }
                    let req_body = CreateSessionReq {
                        players: player_ids
                            .iter()
                            .map(|id| Uuid::parse_str(id).unwrap())
                            .collect(),
                    };

                    match http_client
                        .post(&create_session_url)
                        .json(&req_body)
                        .send()
                        .await
                    {
                        Ok(resp) if resp.status().is_success() => {
                            #[derive(Deserialize, Debug)]
                            struct CreateSessionResp {
                                server_address: String,
                                session_id: Uuid,
                            }

                            match resp.json::<CreateSessionResp>().await {
                                Ok(session_info) => {
                                    info!(
                                        "[{}] Successfully created session: {:?}",
                                        game_mode, session_info
                                    );
                                    MATCHES_CREATED_TOTAL.inc();
                                    let message = ServerMessage::MatchFound {
                                        session_id: session_info.session_id,
                                        server_address: session_info.server_address.clone(),
                                    };
                                    for player_id_str in &player_ids {
                                        let player_id = Uuid::parse_str(player_id_str).unwrap();
                                        publish_message(&mut redis, player_id, message.clone())
                                            .await;
                                    }
                                }
                                Err(e) => {
                                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                                    error!("[{}] Failed to parse session creation response: {}. Re-queuing players.", game_mode, e);
                                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                                }
                            }
                        }
                        Ok(resp) => {
                            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                            error!(
                                "[{}] Dedicated server returned error: {}. Re-queuing players.",
                                game_mode,
                                resp.status()
                            );
                            requeue_players(&mut redis, &queue_key, &player_ids).await;
                        }
                        Err(e) => {
                            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                            error!(
                                "[{}] Failed to contact dedicated server: {}. Re-queuing players.",
                                game_mode, e
                            );
                            requeue_players(&mut redis, &queue_key, &player_ids).await;
                        }
                    }
                }
                Ok(Err(e)) => {
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                    error!(
                        "[{}] Failed to find available server: {}. Re-queuing players.",
                        game_mode, e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                }
                Err(e) => {
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);
                    error!(
                        "[{}] Mailbox error when contacting provider: {}. Re-queuing players.",
                        game_mode, e
                    );
                    requeue_players(&mut redis, &queue_key, &player_ids).await;
                }
            }
        })
    }
}

impl Handler<CancelLoadingSession> for Matchmaker {
    type Result = ResponseFuture<()>;
    fn handle(&mut self, msg: CancelLoadingSession, _ctx: &mut Self::Context) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            let loading_key = format!("loading:{}", msg.loading_session_id);
            let lock_key = format!("lock:{}", loading_key);

            info!(
                "Attempting to cancel loading session {} due to player {} disconnection.",
                msg.loading_session_id, msg.player_id
            );

            // --- 1. 분산락 획득 ---
            let lock = match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await
            {
                Ok(Some(lock)) => lock,
                Ok(None) => {
                    info!("Could not acquire lock for cancellation on session {}, another process is handling it.", msg.loading_session_id);
                    return;
                }
                Err(e) => {
                    error!(
                        "Failed to acquire lock for cancellation on session {}: {}",
                        msg.loading_session_id, e
                    );
                    return;
                }
            };

            // --- 2. 락 내부에서 세션 데이터 확인 ---
            // 이 시점에 HandleLoadingComplete가 먼저 성공했다면, 키는 이미 존재하지 않을 것입니다.
            let all_players_status: HashMap<String, String> = match redis
                .hgetall::<_, HashMap<String, String>>(&loading_key)
                .await
            {
                Ok(statuses) if !statuses.is_empty() => statuses,
                _ => {
                    // 세션이 존재하지 않으면(이미 처리되었으면) 아무것도 할 필요가 없습니다.
                    warn!(
                        "Loading session {} already handled or cleaned up before cancellation.",
                        msg.loading_session_id
                    );
                    // 락을 해제하고 종료합니다.
                    if let Err(e) = lock.release(&mut redis).await {
                        error!(
                            "Failed to release lock for session {}: {}",
                            msg.loading_session_id, e
                        );
                    }
                    return;
                }
            };

            // --- 3. 세션 키 삭제 (취소 로직의 핵심) ---
            // 여기서 키를 삭제함으로써, 뒤늦게 도착할 수 있는 HandleLoadingComplete가 아무 작업도 못하게 만듭니다.
            info!(
                "Cancelling and deleting loading session {}.",
                msg.loading_session_id
            );
            if let Err(e) = redis.del::<_, ()>(&loading_key).await {
                // 키 삭제 실패는 크리티컬한 상황일 수 있으나, 로직은 계속 진행하여 플레이어 재입장을 시도합니다.
                error!(
                    "Failed to delete loading session key {}: {}",
                    loading_key, e
                );
            }

            // --- 4. 락 해제 ---
            // 재입장 로직은 다른 플레이어에게 알림을 보내는 등 시간이 걸릴 수 있으므로,
            // 임계 영역에 해당하는 키 삭제 후에는 가능한 한 빨리 락을 해제합니다.
            if let Err(e) = lock.release(&mut redis).await {
                error!(
                    "Failed to release lock for session {}: {}",
                    msg.loading_session_id, e
                );
            }

            // --- 5. 나머지 플레이어 처리 ---
            let game_mode = all_players_status
                .get("game_mode")
                .cloned()
                .unwrap_or_default();
            let queue_key = format!("{}:{}", queue_key_prefix, game_mode);

            let disconnected_player_id_str = msg.player_id.to_string();
            let players_to_requeue: Vec<String> = all_players_status
                .keys()
                .filter(|k| {
                    let k_str = k.as_str();
                    k_str != "game_mode"
                        && k_str != "created_at"
                        && k_str != "status"
                        && k_str != disconnected_player_id_str
                })
                .cloned()
                .collect();

            if !players_to_requeue.is_empty() {
                info!("Notifying remaining players and re-queuing them.");
                let message = ServerMessage::Error {
                    message:
                        "A player disconnected during loading. You have been returned to the queue."
                            .to_string(),
                };
                for player_id_str in &players_to_requeue {
                    let player_id = Uuid::parse_str(player_id_str).unwrap();
                    publish_message(&mut redis, player_id, message.clone()).await;
                }
                requeue_players(&mut redis, &queue_key, &players_to_requeue).await;
            }
        })
    }
}
impl Handler<CheckStaleLoadingSessions> for Matchmaker {
    type Result = ResponseFuture<()>;

    fn handle(
        &mut self,
        _msg: CheckStaleLoadingSessions,
        _ctx: &mut Self::Context,
    ) -> Self::Result {
        let mut redis = self.redis.clone();
        let queue_key_prefix = self.settings.queue_key_prefix.clone();

        Box::pin(async move {
            info!("Checking for stale loading sessions...");

            // SCAN 사용으로 Redis 블로킹 방지
            let mut keys: Vec<String> = Vec::new();
            match redis.scan_match::<_, String>("loading:*").await {
                Ok(mut iter) => {
                    while let Some(key) = iter.next().await {
                        keys.push(key);
                    }
                }
                Err(e) => {
                    error!("Failed to scan loading sessions: {}", e);
                    return;
                }
            };

            for key in keys {
                let lock_key = format!("lock:{}", key);

                // --- 각 세션에 대한 분산락 획득 시도 ---
                let lock =
                    match DistributedLock::acquire(&mut redis, &lock_key, LOCK_DURATION_MS).await {
                        Ok(Some(lock)) => lock,
                        _ => continue, // 락 획득 실패 시 (다른 프로세스가 처리 중이거나 에러), 다음 키로 넘어감
                    };

                let Ok(all_players_status): RedisResult<HashMap<String, String>> =
                    redis.hgetall(&key).await
                else {
                    // 락 해제
                    let _ = lock.release(&mut redis).await;
                    continue;
                };

                let Some(created_at_str) = all_players_status.get("created_at") else {
                    let _ = lock.release(&mut redis).await;
                    continue;
                };
                let Ok(created_at) = created_at_str.parse::<u64>() else {
                    let _ = lock.release(&mut redis).await;
                    continue;
                };

                let now = SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .unwrap()
                    .as_secs();

                if now > created_at + LOADING_SESSION_TIMEOUT_SECONDS {
                    warn!("Found stale loading session {}. Cleaning up.", key);

                    let _: RedisResult<()> = redis.del(&key).await;

                    let game_mode = all_players_status
                        .get("game_mode")
                        .cloned()
                        .unwrap_or_default();
                    let queue_key = format!("{}:{}", queue_key_prefix, game_mode);

                    let players_to_requeue: Vec<String> = all_players_status
                        .keys()
                        .filter(|k| *k != "game_mode" && *k != "created_at" && *k != "status")
                        .cloned()
                        .collect();

                    if !players_to_requeue.is_empty() {
                        let message = ServerMessage::Error {
                            message: "Matchmaking timed out. You have been returned to the queue."
                                .to_string(),
                        };
                        for player_id_str in &players_to_requeue {
                            let player_id = Uuid::parse_str(player_id_str).unwrap();
                            publish_message(&mut redis, player_id, message.clone()).await;
                        }
                        requeue_players(&mut redis, &queue_key, &players_to_requeue).await;
                    }
                }
                // --- 작업 완료 후 락 해제 ---
                if let Err(e) = lock.release(&mut redis).await {
                    error!(
                        "Failed to release lock for stale check on key {}: {}",
                        key, e
                    );
                }
            }
        })
    }
}
</file>

<file path="src/matchmaker/lock.rs">
use redis::aio::ConnectionManager;
// `cmd`를 추가하고, `Commands`와 `Script`를 임포트합니다. `SetOptions`는 사용하지 않으므로 삭제합니다.
use redis::{cmd, RedisResult, Script};
use uuid::Uuid;

const RELEASE_SCRIPT: &str = r#"
    if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
    else
        return 0
    end
"#;

/// Redis를 이용한 분산락 구조체.
/// Drop 트레이트를 구현하지 않았으므로, 사용 후 반드시 `release`를 명시적으로 호출해야 합니다.
pub struct DistributedLock {
    key: String,
    value: String,
}

impl DistributedLock {
    /// 분산락을 획득합니다.
    ///
    /// # Arguments
    /// * `redis` - Redis 커넥션 매니저
    /// * `key` - 락을 걸 대상 키
    /// * `duration_ms` - 락의 만료 시간 (밀리초)
    ///
    /// # Returns
    /// * `Ok(Some(lock))` - 락 획득 성공
    /// * `Ok(None)` - 다른 프로세스가 락을 이미 소유하고 있음
    /// * `Err(e)` - Redis 오류 발생
    pub async fn acquire(
        redis: &mut ConnectionManager,
        key: &str,
        duration_ms: usize,
    ) -> RedisResult<Option<Self>> {
        let value = Uuid::new_v4().to_string();

        // `set_options` 대신 `cmd`를 사용하여 'SET key value NX PX ms' 명령을 직접 구성합니다.
        let result: Option<String> = cmd("SET")
            .arg(key)
            .arg(&value)
            .arg("NX") // Set only if the key does not already exist.
            .arg("PX") // Set the specified expire time, in milliseconds.
            .arg(duration_ms)
            .query_async(redis) // ConnectionManager에서 비동기적으로 실행
            .await?;

        // 'SET NX'는 성공 시 "OK"를 반환하고, 키가 이미 존재하면 nil을 반환합니다.
        // `redis-rs`는 "OK"를 `Some("OK".to_string())`으로, nil을 `None`으로 변환합니다.
        if result.is_some() {
            Ok(Some(Self {
                key: key.to_string(),
                value,
            }))
        } else {
            Ok(None)
        }
    }

    /// 획득했던 분산락을 해제합니다.
    pub async fn release(&self, redis: &mut ConnectionManager) -> RedisResult<()> {
        let script = Script::new(RELEASE_SCRIPT);
        script
            .key(&self.key)
            .arg(&self.value)
            .invoke_async::<_, ()>(redis)
            .await?;
        Ok(())
    }
}
</file>

<file path="src/matchmaker/messages.rs">
use crate::env::GameModeSettings;
use actix::Message;
use uuid::Uuid;

#[derive(Message)]
#[rtype(result = "()")]
pub struct EnqueuePlayer {
    pub player_id: Uuid,
    pub game_mode: String,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct DequeuePlayer {
    pub player_id: Uuid,
    pub game_mode: String,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct HandleLoadingComplete {
    pub player_id: Uuid,
    pub loading_session_id: Uuid,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct CancelLoadingSession {
    pub player_id: Uuid,
    pub loading_session_id: Uuid,
}

#[derive(Message, Clone)]
#[rtype(result = "()")]
pub(super) struct TryMatch {
    pub(super) game_mode: GameModeSettings,
}

/// 오래된 로딩 세션을 정리하기 위한 내부 메시지입니다.
#[derive(Message)]
#[rtype(result = "()")]
pub(super) struct CheckStaleLoadingSessions;
</file>

<file path="src/matchmaker/mod.rs">
pub mod actor;
mod handlers;
mod lock;
pub mod messages;
mod scripts;
pub use actor::Matchmaker;
</file>

<file path="src/matchmaker/scripts.rs">
pub(super) const ATOMIC_MATCH_SCRIPT: &str = r#"
    local queue_key = KEYS[1]
    local required_players = tonumber(ARGV[1])
    if redis.call('SCARD', queue_key) >= required_players then
        return redis.call('SPOP', queue_key, required_players)
    else
        return {}
    end
"#;

pub(super) const ATOMIC_LOADING_COMPLETE_SCRIPT: &str = r#"
    local loading_key = KEYS[1]
    local player_id = ARGV[1]

    -- Stop if session does not exist or is already handled
    if redis.call('EXISTS', loading_key) == 0 then
        return {}
    end
    -- 추가: 세션 상태가 'loading'이 아니면 (예: 'cancelled') 중단
    local status = redis.call('HGET', loading_key, 'status')
    if status and status ~= 'loading' then
        return {}
    end

    redis.call('HSET', loading_key, player_id, 'ready')

    local players = redis.call('HGETALL', loading_key)
    local all_ready = true
    local player_ids = {}
    local game_mode = ''
    for i=1, #players, 2 do
        if players[i] == 'game_mode' then
            game_mode = players[i+1]
        elseif players[i] ~= 'created_at' and players[i] ~= 'status' then
            if players[i+1] ~= 'ready' then
                all_ready = false
                break
            end
            table.insert(player_ids, players[i])
        end
    end

    if all_ready and #player_ids > 0 then
        -- 성공 시 키 삭제
        redis.call('DEL', loading_key)
        -- game_mode를 맨 앞에 추가하여 반환
        table.insert(player_ids, 1, game_mode)
        return player_ids
    else
        return {}
    end
"#;
</file>

<file path="src/protocol.rs">
use actix::prelude::*;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

// --- Client to Server Messages ---

#[derive(Deserialize, Message)]
#[rtype(result = "()")]
#[serde(tag = "type")]
pub enum ClientMessage {
    /// 플레이어가 매칭 대기열에 들어가기를 요청합니다.
    #[serde(rename = "enqueue")]
    Enqueue {
        player_id: Uuid,
        game_mode: String,
    },
    /// 클라이언트가 에셋 로딩을 완료했음을 서버에 알립니다.
    #[serde(rename = "loading_complete")]
    LoadingComplete {
        loading_session_id: Uuid,
    },
}

// --- Server to Client Messages ---

#[derive(Serialize, Deserialize, Message, Clone)]
#[rtype(result = "()")]
#[serde(tag = "type")]
pub enum ServerMessage {
    /// 대기열에 성공적으로 등록되었음을 알립니다.
    #[serde(rename = "queued")]
    Queued,
    
    /// 클라이언트에게 에셋 로딩을 시작하라고 지시합니다.
    #[serde(rename = "start_loading")]
    StartLoading {
        loading_session_id: Uuid,
    },

    /// 최종적으로 매칭이 성사되었고, 게임 서버 접속 정보를 전달합니다.
    #[serde(rename = "match_found")]
    MatchFound {
        session_id: Uuid, // dedicated_server의 게임 세션 ID
        server_address: String,
    },

    /// 에러가 발생했음을 알립니다.
    #[serde(rename = "error")]
    Error {
        message: String,
    },
}
</file>

<file path="src/provider.rs">
use actix::{Actor, Context, Handler, Message, ResponseFuture};
use futures_util::stream::StreamExt;
use redis::{aio::ConnectionManager, AsyncCommands};
use serde::Deserialize;
use tracing::{info, warn};

// --- Actor Definition ---

/// 사용 가능한 Dedicated Server를 찾아 제공하는 책임을 가진 액터입니다.
pub struct DedicatedServerProvider {
    redis: ConnectionManager,
}

impl DedicatedServerProvider {
    pub fn new(redis: ConnectionManager) -> Self {
        Self { redis }
    }
}

impl Actor for DedicatedServerProvider {
    type Context = Context<Self>;
}

// --- Message Definition ---

/// 사용 가능한 서버를 찾아달라는 메시지입니다.
#[derive(Message)]
#[rtype(result = "Result<ServerInfo, anyhow::Error>")]
pub struct FindAvailableServer;

/// 찾아낸 서버의 정보를 담는 구조체입니다.
#[derive(Deserialize, Debug, Clone)]
pub struct ServerInfo {
    pub address: String,
    pub status: String,
}

// --- Message Handler ---

impl Handler<FindAvailableServer> for DedicatedServerProvider {
    type Result = ResponseFuture<Result<ServerInfo, anyhow::Error>>;

    /// `FindAvailableServer` 메시지를 처리합니다.
    fn handle(&mut self, _msg: FindAvailableServer, _ctx: &mut Context<Self>) -> Self::Result {
        let mut redis = self.redis.clone();

        Box::pin(async move {
            info!("Finding an available dedicated server from Redis...");

            // SCAN 사용으로 Redis 블로킹 방지
            let mut keys: Vec<String> = Vec::new();
            match redis.scan_match::<_, String>("dedicated_server:*").await {
                Ok(mut iter) => {
                    while let Some(key) = iter.next().await {
                        keys.push(key);
                    }
                }
                Err(e) => {
                    warn!("Failed to scan dedicated servers: {}", e);
                    return Err(anyhow::anyhow!("Failed to scan dedicated servers: {}", e));
                }
            };

            // 각 서버의 상태를 확인하여 "idle"인 서버를 찾습니다.
            for key in keys {
                let server_info_json: String = match redis.get(&key).await {
                    Ok(info) => info,
                    Err(e) => {
                        warn!(
                            "Failed to get server info for key {}: {}. Skipping.",
                            key, e
                        );
                        continue; // 다음 키로 넘어감
                    }
                };

                let server_info: ServerInfo = match serde_json::from_str(&server_info_json) {
                    Ok(info) => info,
                    Err(e) => {
                        warn!(
                            "Failed to parse server info for key {}: {}. Skipping.",
                            key, e
                        );
                        continue; // 다음 키로 넘어감
                    }
                };

                // "idle" 상태인 서버를 찾으면 즉시 반환합니다.
                if server_info.status == "idle" {
                    info!("Found idle server: {:?}", server_info);
                    return Ok(server_info);
                }
            }

            // 모든 서버를 확인했지만 "idle" 상태인 서버가 없는 경우
            warn!("All dedicated servers are currently busy.");
            Err(anyhow::anyhow!("All dedicated servers are busy."))
        })
    }
}
</file>

<file path="src/pubsub.rs">
use actix::{
    Actor, Addr, AsyncContext, Context, ContextFutureSpawner, Handler, Message, WrapFuture,
};
use futures_util::stream::StreamExt;
use redis::Client as RedisClient;
use std::collections::HashMap;
use std::time::Duration;
use tracing::{error, info, warn};
use uuid::Uuid;

use crate::protocol::ServerMessage;
use crate::ws_session::MatchmakingSession;

const RECONNECT_DELAY: Duration = Duration::from_secs(5);

// --- Messages for this module ---
#[derive(Message)]
#[rtype(result = "()")]
struct Connect;

// --- SubscriptionManager Actor ---

/// Manages the mapping between player_id and their WebSocket session actor address.
pub struct SubscriptionManager {
    sessions: HashMap<Uuid, Addr<MatchmakingSession>>,
}

impl SubscriptionManager {
    pub fn new() -> Self {
        Self {
            sessions: HashMap::new(),
        }
    }
}

impl Actor for SubscriptionManager {
    type Context = Context<Self>;
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct Register {
    pub player_id: Uuid,
    pub addr: Addr<MatchmakingSession>,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct Deregister {
    pub player_id: Uuid,
}

#[derive(Message)]
#[rtype(result = "()")]
pub struct ForwardMessage {
    pub player_id: Uuid,
    pub message: ServerMessage,
}

impl Handler<Register> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: Register, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Player {} registered for notifications.", msg.player_id);
        self.sessions.insert(msg.player_id, msg.addr);
    }
}

impl Handler<Deregister> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: Deregister, _ctx: &mut Context<Self>) -> Self::Result {
        info!("Player {} deregistered.", msg.player_id);
        self.sessions.remove(&msg.player_id);
    }
}

impl Handler<ForwardMessage> for SubscriptionManager {
    type Result = ();
    fn handle(&mut self, msg: ForwardMessage, _ctx: &mut Context<Self>) -> Self::Result {
        if let Some(recipient_addr) = self.sessions.get(&msg.player_id) {
            recipient_addr.do_send(msg.message);
        } else {
            warn!(
                "Could not find session for player {} to forward message.",
                msg.player_id
            );
        }
    }
}

// --- RedisSubscriber Actor ---

pub struct RedisSubscriber {
    redis_client: RedisClient,
    manager_addr: Addr<SubscriptionManager>,
}

impl RedisSubscriber {
    pub fn new(redis_client: RedisClient, manager_addr: Addr<SubscriptionManager>) -> Self {
        Self {
            redis_client,
            manager_addr,
        }
    }

    fn connect_and_subscribe(&self, ctx: &mut Context<Self>) {
        info!("Attempting to connect and subscribe to Redis...");
        let client = self.redis_client.clone();
        let manager = self.manager_addr.clone();
        let self_addr = ctx.address();

        async move {
            let conn = match client.get_async_connection().await {
                Ok(c) => {
                    info!("Successfully connected to Redis.");
                    c
                }
                Err(e) => {
                    error!("RedisSubscriber failed to get connection: {}", e);
                    self_addr.do_send(Connect); // Trigger reconnect
                    return;
                }
            };
            let mut pubsub = conn.into_pubsub();
            if let Err(e) = pubsub.psubscribe("notifications:*").await {
                error!("RedisSubscriber failed to psubscribe: {}", e);
                self_addr.do_send(Connect); // Trigger reconnect
                return;
            }
            info!("Successfully subscribed to 'notifications:*'");

            let mut stream = pubsub.on_message();
            while let Some(msg) = stream.next().await {
                let channel: String = msg.get_channel_name().to_string();
                let payload: String = match msg.get_payload() {
                    Ok(p) => p,
                    Err(_) => continue,
                };

                if let Some(player_id_str) = channel.strip_prefix("notifications:") {
                    if let Ok(player_id) = Uuid::parse_str(player_id_str) {
                        if let Ok(server_msg) = serde_json::from_str::<ServerMessage>(&payload) {
                            manager.do_send(ForwardMessage {
                                player_id,
                                message: server_msg,
                            });
                        }
                    }
                }
            }
            warn!("Redis Pub/Sub stream ended. Attempting to reconnect...");
            self_addr.do_send(Connect); // Trigger reconnect
        }
        .into_actor(self)
        .wait(ctx);
    }
}

impl Actor for RedisSubscriber {
    type Context = Context<Self>;

    fn started(&mut self, ctx: &mut Context<Self>) {
        info!("RedisSubscriber actor started.");
        self.connect_and_subscribe(ctx);
    }
}

impl Handler<Connect> for RedisSubscriber {
    type Result = ();

    fn handle(&mut self, _msg: Connect, ctx: &mut Context<Self>) -> Self::Result {
        info!("Reconnect message received. Waiting for a delay...");
        ctx.run_later(RECONNECT_DELAY, |act, ctx| {
            act.connect_and_subscribe(ctx);
        });
    }
}
</file>

<file path="src/util/mod.rs">

</file>

<file path="src/ws_session.rs">
use crate::{
    matchmaker::messages::{CancelLoadingSession, DequeuePlayer, EnqueuePlayer},
    protocol::{ClientMessage, ServerMessage},
    pubsub::{Deregister, Register},
    Matchmaker, SubscriptionManager,
};
use actix::{Actor, ActorContext, Addr, AsyncContext, Handler, Running, StreamHandler};
use actix_web_actors::ws;
use std::time::{Duration, Instant};
use tracing::{info, warn};
use uuid::Uuid;

const HEARTBEAT_INTERVAL: Duration = Duration::from_secs(5);
const CLIENT_TIMEOUT: Duration = Duration::from_secs(10);

pub struct MatchmakingSession {
    player_id: Option<Uuid>,
    game_mode: Option<String>,
    loading_session_id: Option<Uuid>,
    hb: Instant,
    matchmaker_addr: Addr<Matchmaker>,
    sub_manager_addr: Addr<SubscriptionManager>,
}

impl MatchmakingSession {
    pub fn new(
        matchmaker_addr: Addr<Matchmaker>,
        sub_manager_addr: Addr<SubscriptionManager>,
    ) -> Self {
        Self {
            player_id: None,
            game_mode: None,
            loading_session_id: None,
            hb: Instant::now(),
            matchmaker_addr,
            sub_manager_addr,
        }
    }

    fn hb(&self, ctx: &mut ws::WebsocketContext<Self>) {
        ctx.run_interval(HEARTBEAT_INTERVAL, |act, ctx| {
            if Instant::now().duration_since(act.hb) > CLIENT_TIMEOUT {
                info!("Websocket Client heartbeat failed, disconnecting!");
                ctx.stop();
                return;
            }
            ctx.ping(b"");
        });
    }
}

impl Actor for MatchmakingSession {
    type Context = ws::WebsocketContext<Self>;

    fn started(&mut self, ctx: &mut Self::Context) {
        info!("MatchmakingSession started.");
        self.hb(ctx);
    }

    fn stopping(&mut self, _: &mut Self::Context) -> Running {
        if let Some(player_id) = self.player_id {
            // Unregister from the subscription manager
            self.sub_manager_addr.do_send(Deregister { player_id });

            // Send appropriate cleanup messages to the matchmaker
            match (self.loading_session_id, self.game_mode.clone()) {
                (Some(loading_session_id), _) => {
                    self.matchmaker_addr.do_send(CancelLoadingSession {
                        player_id,
                        loading_session_id,
                    });
                }
                (None, Some(game_mode)) => {
                    self.matchmaker_addr.do_send(DequeuePlayer {
                        player_id,
                        game_mode,
                    });
                }
                _ => {}
            }
        }
        info!(
            "MatchmakingSession for player {:?} is stopping.",
            self.player_id
        );
        Running::Stop
    }
}

// This handler now receives messages forwarded from the SubscriptionManager
impl Handler<ServerMessage> for MatchmakingSession {
    type Result = ();

    fn handle(&mut self, msg: ServerMessage, ctx: &mut Self::Context) {
        if let ServerMessage::StartLoading { loading_session_id } = &msg {
            self.loading_session_id = Some(*loading_session_id);
        }
        ctx.text(serde_json::to_string(&msg).unwrap());
    }
}

impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for MatchmakingSession {
    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
        match msg {
            Ok(ws::Message::Ping(msg)) => {
                self.hb = Instant::now();
                ctx.pong(&msg);
            }
            Ok(ws::Message::Pong(_)) => {
                self.hb = Instant::now();
            }
            Ok(ws::Message::Text(text)) => {
                match serde_json::from_str::<ClientMessage>(&text) {
                    Ok(ClientMessage::Enqueue {
                        player_id,
                        game_mode,
                    }) => {
                        if self.player_id.is_some() {
                            warn!("Player {} tried to enqueue more than once.", player_id);
                            return;
                        }

                        info!("Player {} requests queue for {}.", player_id, game_mode);
                        self.player_id = Some(player_id);
                        self.game_mode = Some(game_mode.clone());

                        // Register with the SubscriptionManager
                        self.sub_manager_addr.do_send(Register {
                            player_id,
                            addr: ctx.address(),
                        });

                        // Send enqueue message to the matchmaker
                        self.matchmaker_addr.do_send(EnqueuePlayer {
                            player_id,
                            game_mode,
                        });
                    }
                    Ok(ClientMessage::LoadingComplete { loading_session_id }) => {
                        if let Some(player_id) = self.player_id {
                            info!(
                                "Player {} finished loading for session {}",
                                player_id, loading_session_id
                            );
                            self.matchmaker_addr.do_send(
                                crate::matchmaker::messages::HandleLoadingComplete {
                                    player_id,
                                    loading_session_id,
                                },
                            );
                        } else {
                            warn!("Received LoadingComplete from a session with no player_id.");
                        }
                    }
                    Err(e) => {
                        warn!("Failed to parse client message: {}", e);
                        ctx.text(
                            serde_json::to_string(&ServerMessage::Error {
                                message: "Invalid message format".to_string(),
                            })
                            .unwrap(),
                        );
                    }
                }
            }
            Ok(ws::Message::Close(reason)) => {
                ctx.close(reason);
                ctx.stop();
            }
            _ => ctx.stop(),
        }
    }
}
</file>

</files>
